{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model \n",
    "from keras.layers import Input, Dense \n",
    "from keras.utils import np_utils \n",
    "import numpy as np\n",
    "from tensorflow.python.ops.variables import trainable_variables\n",
    "from numpy import genfromtxt\n",
    "\n",
    "X_train_my = genfromtxt('../UserData/405/trainX_item_405.csv', delimiter=',')\n",
    "y_train_my = genfromtxt('../UserData/405/trainY_item_405.csv', delimiter=',')\n",
    "X_test_my = genfromtxt('../UserData/405/testX_item_405.csv', delimiter=',')\n",
    "y_test_my = genfromtxt('../UserData/405/testY_item_405.csv', delimiter=',')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(416, 19)\n"
     ]
    }
   ],
   "source": [
    "X_train_my = X_train_my.astype('float32') \n",
    "X_train_my = X_train_my.astype('float32')\n",
    "print(X_train_my.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Y_train_my = y_train_my\n",
    "\n",
    "Y_test_my = y_test_my\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"input_4:0\", shape=(?, 19), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "input_img_my = Input(shape=(19,))\n",
    "\n",
    "print(input_img_my)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"dense_25/Relu:0\", shape=(?, 19), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x_my = Dense(19, activation='relu')(input_img_my)\n",
    "print(x_my)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoded1_my = Dense(15, activation='relu')(x_my)\n",
    "encoded2_my = Dense(12, activation='relu')(encoded1_my)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_my = Dense(10, activation='relu')(encoded2_my)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoded2_my = Dense(12, activation='relu')(y_my)\n",
    "decoded1_my = Dense(15, activation='relu')(decoded2_my)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown activation function:rely",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-0563028b1097>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mz_my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m19\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rely'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoded1_my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mautoencoder_my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_img_my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/layers/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, units, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)\u001b[0m\n\u001b[1;32m    808\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    811\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_bias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_initializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitializers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel_initializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/activations.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(identifier)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0midentifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/activations.py\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(name, custom_objects)\u001b[0m\n\u001b[1;32m     85\u001b[0m                                     \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                                     \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m                                     printable_module_name='activation function')\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                 raise ValueError('Unknown ' + printable_module_name +\n\u001b[0;32m--> 159\u001b[0;31m                                  ':' + function_name)\n\u001b[0m\u001b[1;32m    160\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown activation function:rely"
     ]
    }
   ],
   "source": [
    "z_my = Dense(19, activation='rely')(decoded1_my)\n",
    "autoencoder_my = Model(input_img_my, z_my)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#encoder is the model of the autoencoder slice in the middle \n",
    "encoder_my = Model(input_img_my, y_my)\n",
    "\n",
    "print(encoder_my)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "autoencoder_my.compile(loss='mse', optimizer='rmsprop') # reporting the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test_my.shape)\n",
    "\n",
    "print(X_train_my.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "autoencoder_my.fit(X_train_my, X_train_my,\n",
    "      epochs=100,\n",
    "      batch_size=10,\n",
    "      shuffle=True,\n",
    "      validation_data=(X_test_my, X_test_my)\n",
    "      )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.71313119 -0.77127057 -0.36123037 ..., -0.93808168  0.69707203\n",
      "   0.75193751]\n",
      " [-0.86592168 -0.84984612 -0.50108516 ..., -0.91473311  0.74837154\n",
      "   0.72986317]\n",
      " [-0.6138441  -0.78318155 -0.90172005 ..., -0.84966886  0.51899493\n",
      "   0.90724009]\n",
      " ..., \n",
      " [-0.75067955 -0.67946416 -0.88503432 ..., -0.92706603  0.34412602\n",
      "   0.91349804]\n",
      " [-0.18168673 -0.72164792 -0.787094   ...,  0.03458322  0.38204607\n",
      "   0.1362883 ]\n",
      " [-0.70125008 -0.90745407 -0.82681561 ..., -0.86925775  0.68382478\n",
      "   0.29069638]]\n"
     ]
    }
   ],
   "source": [
    "# if you want an encoded flatten representation of every test MNIST\n",
    "reduced_representation_my =encoder_my.predict(X_test_my)\n",
    "print(reduced_representation_my)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 416 samples, validate on 175 samples\n",
      "Epoch 1/100\n",
      "416/416 [==============================] - 1s 1ms/step - loss: 0.3521 - val_loss: 0.3092\n",
      "Epoch 2/100\n",
      "416/416 [==============================] - 0s 484us/step - loss: 0.3038 - val_loss: 0.2960\n",
      "Epoch 3/100\n",
      "416/416 [==============================] - 0s 484us/step - loss: 0.2967 - val_loss: 0.2912\n",
      "Epoch 4/100\n",
      "416/416 [==============================] - 0s 510us/step - loss: 0.2940 - val_loss: 0.2889\n",
      "Epoch 5/100\n",
      "416/416 [==============================] - 0s 452us/step - loss: 0.2923 - val_loss: 0.2879\n",
      "Epoch 6/100\n",
      "416/416 [==============================] - 0s 449us/step - loss: 0.2913 - val_loss: 0.2869\n",
      "Epoch 7/100\n",
      "416/416 [==============================] - 0s 499us/step - loss: 0.2907 - val_loss: 0.2861\n",
      "Epoch 8/100\n",
      "416/416 [==============================] - 0s 469us/step - loss: 0.2901 - val_loss: 0.2858\n",
      "Epoch 9/100\n",
      "416/416 [==============================] - ETA: 0s - loss: 0.291 - 0s 475us/step - loss: 0.2899 - val_loss: 0.2860\n",
      "Epoch 10/100\n",
      "416/416 [==============================] - 0s 512us/step - loss: 0.2896 - val_loss: 0.2854\n",
      "Epoch 11/100\n",
      "416/416 [==============================] - 0s 446us/step - loss: 0.2894 - val_loss: 0.2851\n",
      "Epoch 12/100\n",
      "416/416 [==============================] - 0s 436us/step - loss: 0.2892 - val_loss: 0.2849\n",
      "Epoch 13/100\n",
      "416/416 [==============================] - 0s 451us/step - loss: 0.2890 - val_loss: 0.2850\n",
      "Epoch 14/100\n",
      "416/416 [==============================] - 0s 445us/step - loss: 0.2889 - val_loss: 0.2847\n",
      "Epoch 15/100\n",
      "416/416 [==============================] - 0s 431us/step - loss: 0.2887 - val_loss: 0.2844\n",
      "Epoch 16/100\n",
      "416/416 [==============================] - 0s 443us/step - loss: 0.2885 - val_loss: 0.2844\n",
      "Epoch 17/100\n",
      "416/416 [==============================] - 0s 446us/step - loss: 0.2884 - val_loss: 0.2847\n",
      "Epoch 18/100\n",
      "416/416 [==============================] - 0s 447us/step - loss: 0.2884 - val_loss: 0.2841\n",
      "Epoch 19/100\n",
      "416/416 [==============================] - 0s 417us/step - loss: 0.2881 - val_loss: 0.2842\n",
      "Epoch 20/100\n",
      "416/416 [==============================] - 0s 433us/step - loss: 0.2880 - val_loss: 0.2838\n",
      "Epoch 21/100\n",
      "416/416 [==============================] - 0s 445us/step - loss: 0.2880 - val_loss: 0.2838\n",
      "Epoch 22/100\n",
      "416/416 [==============================] - 0s 450us/step - loss: 0.2879 - val_loss: 0.2835\n",
      "Epoch 23/100\n",
      "416/416 [==============================] - 0s 442us/step - loss: 0.2877 - val_loss: 0.2833\n",
      "Epoch 24/100\n",
      "416/416 [==============================] - 0s 453us/step - loss: 0.2877 - val_loss: 0.2834\n",
      "Epoch 25/100\n",
      "416/416 [==============================] - 0s 410us/step - loss: 0.2876 - val_loss: 0.2836\n",
      "Epoch 26/100\n",
      "416/416 [==============================] - 0s 427us/step - loss: 0.2875 - val_loss: 0.2833\n",
      "Epoch 27/100\n",
      "416/416 [==============================] - 0s 447us/step - loss: 0.2874 - val_loss: 0.2833\n",
      "Epoch 28/100\n",
      "416/416 [==============================] - 0s 466us/step - loss: 0.2874 - val_loss: 0.2835\n",
      "Epoch 29/100\n",
      "416/416 [==============================] - 0s 451us/step - loss: 0.2873 - val_loss: 0.2830\n",
      "Epoch 30/100\n",
      "416/416 [==============================] - 0s 454us/step - loss: 0.2873 - val_loss: 0.2829\n",
      "Epoch 31/100\n",
      "416/416 [==============================] - 0s 452us/step - loss: 0.2872 - val_loss: 0.2829\n",
      "Epoch 32/100\n",
      "416/416 [==============================] - 0s 438us/step - loss: 0.2872 - val_loss: 0.2826\n",
      "Epoch 33/100\n",
      "416/416 [==============================] - 0s 439us/step - loss: 0.2871 - val_loss: 0.2827\n",
      "Epoch 34/100\n",
      "416/416 [==============================] - ETA: 0s - loss: 0.289 - 0s 475us/step - loss: 0.2870 - val_loss: 0.2829\n",
      "Epoch 35/100\n",
      "416/416 [==============================] - 0s 521us/step - loss: 0.2870 - val_loss: 0.2830\n",
      "Epoch 36/100\n",
      "416/416 [==============================] - 0s 489us/step - loss: 0.2869 - val_loss: 0.2830\n",
      "Epoch 37/100\n",
      "416/416 [==============================] - 0s 281us/step - loss: 0.2869 - val_loss: 0.2828\n",
      "Epoch 38/100\n",
      "416/416 [==============================] - 0s 370us/step - loss: 0.2868 - val_loss: 0.2831\n",
      "Epoch 39/100\n",
      "416/416 [==============================] - 0s 257us/step - loss: 0.2867 - val_loss: 0.2828\n",
      "Epoch 40/100\n",
      "416/416 [==============================] - 0s 459us/step - loss: 0.2867 - val_loss: 0.2826\n",
      "Epoch 41/100\n",
      "416/416 [==============================] - 0s 509us/step - loss: 0.2867 - val_loss: 0.2826\n",
      "Epoch 42/100\n",
      "416/416 [==============================] - 0s 489us/step - loss: 0.2866 - val_loss: 0.2824\n",
      "Epoch 43/100\n",
      "416/416 [==============================] - 0s 447us/step - loss: 0.2866 - val_loss: 0.2824\n",
      "Epoch 44/100\n",
      "416/416 [==============================] - 0s 480us/step - loss: 0.2865 - val_loss: 0.2824\n",
      "Epoch 45/100\n",
      "416/416 [==============================] - 0s 439us/step - loss: 0.2865 - val_loss: 0.2828\n",
      "Epoch 46/100\n",
      "416/416 [==============================] - 0s 438us/step - loss: 0.2864 - val_loss: 0.2824\n",
      "Epoch 47/100\n",
      "416/416 [==============================] - 0s 457us/step - loss: 0.2864 - val_loss: 0.2823\n",
      "Epoch 48/100\n",
      "416/416 [==============================] - 0s 439us/step - loss: 0.2863 - val_loss: 0.2826\n",
      "Epoch 49/100\n",
      "416/416 [==============================] - 0s 455us/step - loss: 0.2864 - val_loss: 0.2828\n",
      "Epoch 50/100\n",
      "416/416 [==============================] - 0s 426us/step - loss: 0.2863 - val_loss: 0.2824\n",
      "Epoch 51/100\n",
      "416/416 [==============================] - 0s 447us/step - loss: 0.2862 - val_loss: 0.2828\n",
      "Epoch 52/100\n",
      "416/416 [==============================] - 0s 447us/step - loss: 0.2862 - val_loss: 0.2826\n",
      "Epoch 53/100\n",
      "416/416 [==============================] - 0s 421us/step - loss: 0.2862 - val_loss: 0.2824\n",
      "Epoch 54/100\n",
      "416/416 [==============================] - 0s 440us/step - loss: 0.2861 - val_loss: 0.2827\n",
      "Epoch 55/100\n",
      "416/416 [==============================] - 0s 460us/step - loss: 0.2861 - val_loss: 0.2825\n",
      "Epoch 56/100\n",
      "416/416 [==============================] - 0s 462us/step - loss: 0.2861 - val_loss: 0.2827\n",
      "Epoch 57/100\n",
      "416/416 [==============================] - 0s 449us/step - loss: 0.2860 - val_loss: 0.2826\n",
      "Epoch 58/100\n",
      "416/416 [==============================] - 0s 467us/step - loss: 0.2861 - val_loss: 0.2823\n",
      "Epoch 59/100\n",
      "416/416 [==============================] - 0s 434us/step - loss: 0.2860 - val_loss: 0.2824\n",
      "Epoch 60/100\n",
      "416/416 [==============================] - 0s 456us/step - loss: 0.2859 - val_loss: 0.2827\n",
      "Epoch 61/100\n",
      "416/416 [==============================] - 0s 449us/step - loss: 0.2859 - val_loss: 0.2828\n",
      "Epoch 62/100\n",
      "416/416 [==============================] - 0s 452us/step - loss: 0.2859 - val_loss: 0.2825\n",
      "Epoch 63/100\n",
      "416/416 [==============================] - 0s 447us/step - loss: 0.2858 - val_loss: 0.2824\n",
      "Epoch 64/100\n",
      "416/416 [==============================] - 0s 422us/step - loss: 0.2858 - val_loss: 0.2831\n",
      "Epoch 65/100\n",
      "416/416 [==============================] - 0s 441us/step - loss: 0.2858 - val_loss: 0.2827\n",
      "Epoch 66/100\n",
      "416/416 [==============================] - 0s 441us/step - loss: 0.2857 - val_loss: 0.2824\n",
      "Epoch 67/100\n",
      "416/416 [==============================] - 0s 450us/step - loss: 0.2857 - val_loss: 0.2823\n",
      "Epoch 68/100\n",
      "416/416 [==============================] - 0s 440us/step - loss: 0.2856 - val_loss: 0.2830\n",
      "Epoch 69/100\n",
      "416/416 [==============================] - 0s 453us/step - loss: 0.2856 - val_loss: 0.2825\n",
      "Epoch 70/100\n",
      "416/416 [==============================] - 0s 421us/step - loss: 0.2856 - val_loss: 0.2825\n",
      "Epoch 71/100\n",
      "416/416 [==============================] - 0s 448us/step - loss: 0.2856 - val_loss: 0.2831\n",
      "Epoch 72/100\n",
      "416/416 [==============================] - 0s 439us/step - loss: 0.2855 - val_loss: 0.2823\n",
      "Epoch 73/100\n",
      "416/416 [==============================] - 0s 443us/step - loss: 0.2856 - val_loss: 0.2824\n",
      "Epoch 74/100\n",
      "416/416 [==============================] - 0s 437us/step - loss: 0.2855 - val_loss: 0.2827\n",
      "Epoch 75/100\n",
      "416/416 [==============================] - 0s 445us/step - loss: 0.2854 - val_loss: 0.2825\n",
      "Epoch 76/100\n",
      "416/416 [==============================] - 0s 439us/step - loss: 0.2854 - val_loss: 0.2825\n",
      "Epoch 77/100\n",
      "416/416 [==============================] - 0s 444us/step - loss: 0.2854 - val_loss: 0.2824\n",
      "Epoch 78/100\n",
      "416/416 [==============================] - 0s 436us/step - loss: 0.2853 - val_loss: 0.2824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/100\n",
      "416/416 [==============================] - 0s 448us/step - loss: 0.2853 - val_loss: 0.2824\n",
      "Epoch 80/100\n",
      "416/416 [==============================] - 0s 449us/step - loss: 0.2853 - val_loss: 0.2826\n",
      "Epoch 81/100\n",
      "416/416 [==============================] - 0s 423us/step - loss: 0.2852 - val_loss: 0.2824\n",
      "Epoch 82/100\n",
      "416/416 [==============================] - 0s 446us/step - loss: 0.2852 - val_loss: 0.2824\n",
      "Epoch 83/100\n",
      "416/416 [==============================] - 0s 424us/step - loss: 0.2852 - val_loss: 0.2824\n",
      "Epoch 84/100\n",
      "416/416 [==============================] - 0s 418us/step - loss: 0.2851 - val_loss: 0.2825\n",
      "Epoch 85/100\n",
      "416/416 [==============================] - 0s 441us/step - loss: 0.2851 - val_loss: 0.2826\n",
      "Epoch 86/100\n",
      "416/416 [==============================] - 0s 448us/step - loss: 0.2851 - val_loss: 0.2825\n",
      "Epoch 87/100\n",
      "416/416 [==============================] - 0s 451us/step - loss: 0.2850 - val_loss: 0.2825\n",
      "Epoch 88/100\n",
      "416/416 [==============================] - 0s 458us/step - loss: 0.2850 - val_loss: 0.2826\n",
      "Epoch 89/100\n",
      "416/416 [==============================] - 0s 437us/step - loss: 0.2850 - val_loss: 0.2824\n",
      "Epoch 90/100\n",
      "416/416 [==============================] - 0s 457us/step - loss: 0.2850 - val_loss: 0.2826\n",
      "Epoch 91/100\n",
      "416/416 [==============================] - 0s 446us/step - loss: 0.2849 - val_loss: 0.2825\n",
      "Epoch 92/100\n",
      "416/416 [==============================] - 0s 447us/step - loss: 0.2849 - val_loss: 0.2828\n",
      "Epoch 93/100\n",
      "416/416 [==============================] - 0s 454us/step - loss: 0.2848 - val_loss: 0.2824\n",
      "Epoch 94/100\n",
      "416/416 [==============================] - 0s 430us/step - loss: 0.2848 - val_loss: 0.2827\n",
      "Epoch 95/100\n",
      "416/416 [==============================] - 0s 448us/step - loss: 0.2848 - val_loss: 0.2823\n",
      "Epoch 96/100\n",
      "416/416 [==============================] - 0s 448us/step - loss: 0.2847 - val_loss: 0.2824\n",
      "Epoch 97/100\n",
      "416/416 [==============================] - 0s 445us/step - loss: 0.2848 - val_loss: 0.2823\n",
      "Epoch 98/100\n",
      "416/416 [==============================] - 0s 438us/step - loss: 0.2847 - val_loss: 0.2824\n",
      "Epoch 99/100\n",
      "416/416 [==============================] - 0s 453us/step - loss: 0.2846 - val_loss: 0.2825\n",
      "Epoch 100/100\n",
      "416/416 [==============================] - 0s 448us/step - loss: 0.2846 - val_loss: 0.2824\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a1326c898>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#encoder is the model of the autoencoder slice in the middle \n",
    "encoder_my = Model(input_img_my, y_my)\n",
    "\n",
    "autoencoder_my.compile(loss='mse', optimizer='rmsprop') # reporting the loss\n",
    "\n",
    "autoencoder_my.fit(X_train_my, X_train_my,\n",
    "      epochs=100,\n",
    "      batch_size=10,\n",
    "      shuffle=True,\n",
    "      validation_data=(X_test_my, X_test_my))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if you want an encoded flatten representation of every test MNIST\n",
    "reduced_representation_my =encoder_my.predict(X_test_my)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out2_my = Dense(1, activation='linear')(encoder_my.output)\n",
    "newmodel_my = Model(encoder_my.input,out2_my)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newmodel_my.compile(loss='mean_squared_error', optimizer='rmsprop', \n",
    "          metrics=['accuracy']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 416 samples, validate on 175 samples\n",
      "Epoch 1/100\n",
      "416/416 [==============================] - 0s 859us/step - loss: 2.6098 - acc: 0.5721 - val_loss: 2.1248 - val_acc: 0.4800\n",
      "Epoch 2/100\n",
      "416/416 [==============================] - 0s 50us/step - loss: 2.0376 - acc: 0.4471 - val_loss: 1.7910 - val_acc: 0.2914\n",
      "Epoch 3/100\n",
      "416/416 [==============================] - 0s 59us/step - loss: 1.8215 - acc: 0.2740 - val_loss: 1.7306 - val_acc: 0.2971\n",
      "Epoch 4/100\n",
      "416/416 [==============================] - 0s 52us/step - loss: 1.7702 - acc: 0.2572 - val_loss: 1.6777 - val_acc: 0.2514\n",
      "Epoch 5/100\n",
      "416/416 [==============================] - 0s 63us/step - loss: 1.7394 - acc: 0.2236 - val_loss: 1.6710 - val_acc: 0.2400\n",
      "Epoch 6/100\n",
      "416/416 [==============================] - 0s 57us/step - loss: 1.7299 - acc: 0.2115 - val_loss: 1.6629 - val_acc: 0.2343\n",
      "Epoch 7/100\n",
      "416/416 [==============================] - 0s 48us/step - loss: 1.7243 - acc: 0.2067 - val_loss: 1.6551 - val_acc: 0.2000\n",
      "Epoch 8/100\n",
      "416/416 [==============================] - 0s 67us/step - loss: 1.7188 - acc: 0.1995 - val_loss: 1.6589 - val_acc: 0.2229\n",
      "Epoch 9/100\n",
      "416/416 [==============================] - 0s 50us/step - loss: 1.7240 - acc: 0.1947 - val_loss: 1.6797 - val_acc: 0.2514\n",
      "Epoch 10/100\n",
      "416/416 [==============================] - 0s 56us/step - loss: 1.7229 - acc: 0.2404 - val_loss: 1.6712 - val_acc: 0.2286\n",
      "Epoch 11/100\n",
      "416/416 [==============================] - 0s 49us/step - loss: 1.7048 - acc: 0.2188 - val_loss: 1.6956 - val_acc: 0.2457\n",
      "Epoch 12/100\n",
      "416/416 [==============================] - 0s 51us/step - loss: 1.7052 - acc: 0.2332 - val_loss: 1.6850 - val_acc: 0.2400\n",
      "Epoch 13/100\n",
      "416/416 [==============================] - 0s 53us/step - loss: 1.7089 - acc: 0.2284 - val_loss: 1.6726 - val_acc: 0.2286\n",
      "Epoch 14/100\n",
      "416/416 [==============================] - 0s 60us/step - loss: 1.7043 - acc: 0.2332 - val_loss: 1.6782 - val_acc: 0.2286\n",
      "Epoch 15/100\n",
      "416/416 [==============================] - 0s 45us/step - loss: 1.7005 - acc: 0.2260 - val_loss: 1.6791 - val_acc: 0.2286\n",
      "Epoch 16/100\n",
      "416/416 [==============================] - 0s 49us/step - loss: 1.6922 - acc: 0.2380 - val_loss: 1.6680 - val_acc: 0.1771\n",
      "Epoch 17/100\n",
      "416/416 [==============================] - 0s 51us/step - loss: 1.6861 - acc: 0.2115 - val_loss: 1.6894 - val_acc: 0.2457\n",
      "Epoch 18/100\n",
      "416/416 [==============================] - 0s 58us/step - loss: 1.6905 - acc: 0.2380 - val_loss: 1.6699 - val_acc: 0.2057\n",
      "Epoch 19/100\n",
      "416/416 [==============================] - 0s 49us/step - loss: 1.6914 - acc: 0.2067 - val_loss: 1.6731 - val_acc: 0.2057\n",
      "Epoch 20/100\n",
      "416/416 [==============================] - 0s 53us/step - loss: 1.6776 - acc: 0.2260 - val_loss: 1.6717 - val_acc: 0.2000\n",
      "Epoch 21/100\n",
      "416/416 [==============================] - 0s 63us/step - loss: 1.6816 - acc: 0.2236 - val_loss: 1.6763 - val_acc: 0.1829\n",
      "Epoch 22/100\n",
      "416/416 [==============================] - 0s 48us/step - loss: 1.6909 - acc: 0.2236 - val_loss: 1.6763 - val_acc: 0.1771\n",
      "Epoch 23/100\n",
      "416/416 [==============================] - 0s 53us/step - loss: 1.6810 - acc: 0.2139 - val_loss: 1.6716 - val_acc: 0.1829\n",
      "Epoch 24/100\n",
      "416/416 [==============================] - 0s 54us/step - loss: 1.6745 - acc: 0.2163 - val_loss: 1.6686 - val_acc: 0.1771\n",
      "Epoch 25/100\n",
      "416/416 [==============================] - 0s 48us/step - loss: 1.6753 - acc: 0.2236 - val_loss: 1.6771 - val_acc: 0.1771\n",
      "Epoch 26/100\n",
      "416/416 [==============================] - 0s 55us/step - loss: 1.6760 - acc: 0.2212 - val_loss: 1.6776 - val_acc: 0.1714\n",
      "Epoch 27/100\n",
      "416/416 [==============================] - 0s 48us/step - loss: 1.6654 - acc: 0.2188 - val_loss: 1.6946 - val_acc: 0.2514\n",
      "Epoch 28/100\n",
      "416/416 [==============================] - 0s 49us/step - loss: 1.6717 - acc: 0.2476 - val_loss: 1.6650 - val_acc: 0.1771\n",
      "Epoch 29/100\n",
      "416/416 [==============================] - 0s 60us/step - loss: 1.6631 - acc: 0.2188 - val_loss: 1.6655 - val_acc: 0.2057\n",
      "Epoch 30/100\n",
      "416/416 [==============================] - 0s 52us/step - loss: 1.6579 - acc: 0.2260 - val_loss: 1.6662 - val_acc: 0.2057\n",
      "Epoch 31/100\n",
      "416/416 [==============================] - 0s 52us/step - loss: 1.6649 - acc: 0.2404 - val_loss: 1.6726 - val_acc: 0.2229\n",
      "Epoch 32/100\n",
      "416/416 [==============================] - 0s 49us/step - loss: 1.6576 - acc: 0.2356 - val_loss: 1.6584 - val_acc: 0.1943\n",
      "Epoch 33/100\n",
      "416/416 [==============================] - 0s 64us/step - loss: 1.6576 - acc: 0.2284 - val_loss: 1.6841 - val_acc: 0.1429\n",
      "Epoch 34/100\n",
      "416/416 [==============================] - 0s 48us/step - loss: 1.6769 - acc: 0.2212 - val_loss: 1.6657 - val_acc: 0.2057\n",
      "Epoch 35/100\n",
      "416/416 [==============================] - 0s 51us/step - loss: 1.6497 - acc: 0.2284 - val_loss: 1.6657 - val_acc: 0.1943\n",
      "Epoch 36/100\n",
      "416/416 [==============================] - 0s 51us/step - loss: 1.6507 - acc: 0.2212 - val_loss: 1.6548 - val_acc: 0.1943\n",
      "Epoch 37/100\n",
      "416/416 [==============================] - 0s 73us/step - loss: 1.6479 - acc: 0.2308 - val_loss: 1.6651 - val_acc: 0.2171\n",
      "Epoch 38/100\n",
      "416/416 [==============================] - 0s 65us/step - loss: 1.6462 - acc: 0.2284 - val_loss: 1.6740 - val_acc: 0.1600\n",
      "Epoch 39/100\n",
      "416/416 [==============================] - 0s 58us/step - loss: 1.6529 - acc: 0.2284 - val_loss: 1.6531 - val_acc: 0.2114\n",
      "Epoch 40/100\n",
      "416/416 [==============================] - 0s 69us/step - loss: 1.6607 - acc: 0.2332 - val_loss: 1.6627 - val_acc: 0.1886\n",
      "Epoch 41/100\n",
      "416/416 [==============================] - 0s 72us/step - loss: 1.6429 - acc: 0.2308 - val_loss: 1.6656 - val_acc: 0.2514\n",
      "Epoch 42/100\n",
      "416/416 [==============================] - 0s 70us/step - loss: 1.6444 - acc: 0.2260 - val_loss: 1.6661 - val_acc: 0.2457\n",
      "Epoch 43/100\n",
      "416/416 [==============================] - 0s 76us/step - loss: 1.6405 - acc: 0.2332 - val_loss: 1.6570 - val_acc: 0.2114\n",
      "Epoch 44/100\n",
      "416/416 [==============================] - 0s 75us/step - loss: 1.6444 - acc: 0.2404 - val_loss: 1.6593 - val_acc: 0.1771\n",
      "Epoch 45/100\n",
      "416/416 [==============================] - 0s 75us/step - loss: 1.6397 - acc: 0.2284 - val_loss: 1.6575 - val_acc: 0.2000\n",
      "Epoch 46/100\n",
      "416/416 [==============================] - 0s 73us/step - loss: 1.6426 - acc: 0.2356 - val_loss: 1.6845 - val_acc: 0.2686\n",
      "Epoch 47/100\n",
      "416/416 [==============================] - 0s 76us/step - loss: 1.6380 - acc: 0.2596 - val_loss: 1.6638 - val_acc: 0.1714\n",
      "Epoch 48/100\n",
      "416/416 [==============================] - 0s 74us/step - loss: 1.6375 - acc: 0.2356 - val_loss: 1.6716 - val_acc: 0.2171\n",
      "Epoch 49/100\n",
      "416/416 [==============================] - 0s 76us/step - loss: 1.6350 - acc: 0.2356 - val_loss: 1.6556 - val_acc: 0.2057\n",
      "Epoch 50/100\n",
      "416/416 [==============================] - 0s 93us/step - loss: 1.6295 - acc: 0.2260 - val_loss: 1.6538 - val_acc: 0.2114\n",
      "Epoch 51/100\n",
      "416/416 [==============================] - 0s 94us/step - loss: 1.6236 - acc: 0.2260 - val_loss: 1.6542 - val_acc: 0.2114\n",
      "Epoch 52/100\n",
      "416/416 [==============================] - 0s 88us/step - loss: 1.6243 - acc: 0.2236 - val_loss: 1.6737 - val_acc: 0.2571\n",
      "Epoch 53/100\n",
      "416/416 [==============================] - 0s 82us/step - loss: 1.6388 - acc: 0.2500 - val_loss: 1.6473 - val_acc: 0.2229\n",
      "Epoch 54/100\n",
      "416/416 [==============================] - 0s 86us/step - loss: 1.6304 - acc: 0.2308 - val_loss: 1.6806 - val_acc: 0.2686\n",
      "Epoch 55/100\n",
      "416/416 [==============================] - 0s 103us/step - loss: 1.6273 - acc: 0.2548 - val_loss: 1.6579 - val_acc: 0.2229\n",
      "Epoch 56/100\n",
      "416/416 [==============================] - 0s 85us/step - loss: 1.6155 - acc: 0.2428 - val_loss: 1.6557 - val_acc: 0.1943\n",
      "Epoch 57/100\n",
      "416/416 [==============================] - 0s 89us/step - loss: 1.6167 - acc: 0.2308 - val_loss: 1.6551 - val_acc: 0.2114\n",
      "Epoch 58/100\n",
      "416/416 [==============================] - 0s 65us/step - loss: 1.6170 - acc: 0.2236 - val_loss: 1.6602 - val_acc: 0.2114\n",
      "Epoch 59/100\n",
      "416/416 [==============================] - 0s 77us/step - loss: 1.6186 - acc: 0.2356 - val_loss: 1.6616 - val_acc: 0.2000\n",
      "Epoch 60/100\n",
      "416/416 [==============================] - 0s 81us/step - loss: 1.6175 - acc: 0.2356 - val_loss: 1.6784 - val_acc: 0.1657\n",
      "Epoch 61/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "416/416 [==============================] - 0s 89us/step - loss: 1.6145 - acc: 0.2284 - val_loss: 1.6746 - val_acc: 0.2571\n",
      "Epoch 62/100\n",
      "416/416 [==============================] - 0s 92us/step - loss: 1.6163 - acc: 0.2740 - val_loss: 1.6601 - val_acc: 0.1943\n",
      "Epoch 63/100\n",
      "416/416 [==============================] - 0s 63us/step - loss: 1.6112 - acc: 0.2380 - val_loss: 1.6592 - val_acc: 0.2229\n",
      "Epoch 64/100\n",
      "416/416 [==============================] - 0s 38us/step - loss: 1.6023 - acc: 0.2428 - val_loss: 1.6550 - val_acc: 0.2000\n",
      "Epoch 65/100\n",
      "416/416 [==============================] - 0s 55us/step - loss: 1.6094 - acc: 0.2404 - val_loss: 1.6584 - val_acc: 0.2000\n",
      "Epoch 66/100\n",
      "416/416 [==============================] - 0s 68us/step - loss: 1.6006 - acc: 0.2380 - val_loss: 1.6845 - val_acc: 0.2743\n",
      "Epoch 67/100\n",
      "416/416 [==============================] - 0s 62us/step - loss: 1.6048 - acc: 0.2740 - val_loss: 1.6484 - val_acc: 0.2114\n",
      "Epoch 68/100\n",
      "416/416 [==============================] - 0s 48us/step - loss: 1.6052 - acc: 0.2476 - val_loss: 1.6608 - val_acc: 0.2000\n",
      "Epoch 69/100\n",
      "416/416 [==============================] - 0s 78us/step - loss: 1.5992 - acc: 0.2404 - val_loss: 1.6759 - val_acc: 0.1886\n",
      "Epoch 70/100\n",
      "416/416 [==============================] - 0s 69us/step - loss: 1.6049 - acc: 0.2356 - val_loss: 1.6625 - val_acc: 0.2000\n",
      "Epoch 71/100\n",
      "416/416 [==============================] - ETA: 0s - loss: 1.4753 - acc: 0.226 - 0s 80us/step - loss: 1.5957 - acc: 0.2308 - val_loss: 1.6594 - val_acc: 0.2000\n",
      "Epoch 72/100\n",
      "416/416 [==============================] - 0s 69us/step - loss: 1.5925 - acc: 0.2428 - val_loss: 1.6614 - val_acc: 0.1943\n",
      "Epoch 73/100\n",
      "416/416 [==============================] - 0s 79us/step - loss: 1.5935 - acc: 0.2356 - val_loss: 1.6611 - val_acc: 0.2229\n",
      "Epoch 74/100\n",
      "416/416 [==============================] - 0s 70us/step - loss: 1.5879 - acc: 0.2428 - val_loss: 1.6949 - val_acc: 0.2800\n",
      "Epoch 75/100\n",
      "416/416 [==============================] - 0s 61us/step - loss: 1.5986 - acc: 0.2837 - val_loss: 1.6656 - val_acc: 0.2571\n",
      "Epoch 76/100\n",
      "416/416 [==============================] - 0s 54us/step - loss: 1.5876 - acc: 0.2572 - val_loss: 1.6522 - val_acc: 0.2400\n",
      "Epoch 77/100\n",
      "416/416 [==============================] - 0s 58us/step - loss: 1.5862 - acc: 0.2572 - val_loss: 1.6659 - val_acc: 0.2114\n",
      "Epoch 78/100\n",
      "416/416 [==============================] - 0s 54us/step - loss: 1.5785 - acc: 0.2428 - val_loss: 1.6690 - val_acc: 0.2400\n",
      "Epoch 79/100\n",
      "416/416 [==============================] - 0s 52us/step - loss: 1.5768 - acc: 0.2620 - val_loss: 1.6641 - val_acc: 0.2000\n",
      "Epoch 80/100\n",
      "416/416 [==============================] - 0s 58us/step - loss: 1.5752 - acc: 0.2428 - val_loss: 1.6568 - val_acc: 0.2171\n",
      "Epoch 81/100\n",
      "416/416 [==============================] - 0s 58us/step - loss: 1.5750 - acc: 0.2548 - val_loss: 1.6614 - val_acc: 0.2457\n",
      "Epoch 82/100\n",
      "416/416 [==============================] - 0s 80us/step - loss: 1.5766 - acc: 0.2428 - val_loss: 1.6611 - val_acc: 0.2171\n",
      "Epoch 83/100\n",
      "416/416 [==============================] - 0s 61us/step - loss: 1.5800 - acc: 0.2548 - val_loss: 1.6776 - val_acc: 0.2343\n",
      "Epoch 84/100\n",
      "416/416 [==============================] - 0s 69us/step - loss: 1.5716 - acc: 0.2500 - val_loss: 1.6622 - val_acc: 0.2400\n",
      "Epoch 85/100\n",
      "416/416 [==============================] - 0s 54us/step - loss: 1.5726 - acc: 0.2668 - val_loss: 1.6627 - val_acc: 0.2286\n",
      "Epoch 86/100\n",
      "416/416 [==============================] - 0s 54us/step - loss: 1.5713 - acc: 0.2692 - val_loss: 1.6604 - val_acc: 0.2114\n",
      "Epoch 87/100\n",
      "416/416 [==============================] - 0s 59us/step - loss: 1.5716 - acc: 0.2740 - val_loss: 1.6718 - val_acc: 0.2057\n",
      "Epoch 88/100\n",
      "416/416 [==============================] - 0s 50us/step - loss: 1.5766 - acc: 0.2524 - val_loss: 1.6714 - val_acc: 0.2057\n",
      "Epoch 89/100\n",
      "416/416 [==============================] - 0s 66us/step - loss: 1.5701 - acc: 0.2596 - val_loss: 1.6666 - val_acc: 0.2171\n",
      "Epoch 90/100\n",
      "416/416 [==============================] - 0s 49us/step - loss: 1.5558 - acc: 0.2596 - val_loss: 1.6850 - val_acc: 0.2800\n",
      "Epoch 91/100\n",
      "416/416 [==============================] - 0s 59us/step - loss: 1.5737 - acc: 0.2764 - val_loss: 1.6742 - val_acc: 0.2514\n",
      "Epoch 92/100\n",
      "416/416 [==============================] - 0s 54us/step - loss: 1.5552 - acc: 0.2764 - val_loss: 1.6708 - val_acc: 0.2514\n",
      "Epoch 93/100\n",
      "416/416 [==============================] - 0s 77us/step - loss: 1.5480 - acc: 0.2692 - val_loss: 1.6781 - val_acc: 0.2057\n",
      "Epoch 94/100\n",
      "416/416 [==============================] - 0s 79us/step - loss: 1.5479 - acc: 0.2668 - val_loss: 1.6822 - val_acc: 0.2286\n",
      "Epoch 95/100\n",
      "416/416 [==============================] - 0s 80us/step - loss: 1.5459 - acc: 0.2716 - val_loss: 1.6942 - val_acc: 0.2057\n",
      "Epoch 96/100\n",
      "416/416 [==============================] - 0s 84us/step - loss: 1.5532 - acc: 0.2644 - val_loss: 1.6824 - val_acc: 0.2514\n",
      "Epoch 97/100\n",
      "416/416 [==============================] - 0s 67us/step - loss: 1.5437 - acc: 0.2788 - val_loss: 1.6963 - val_acc: 0.2400\n",
      "Epoch 98/100\n",
      "416/416 [==============================] - 0s 69us/step - loss: 1.5545 - acc: 0.2740 - val_loss: 1.6742 - val_acc: 0.2514\n",
      "Epoch 99/100\n",
      "416/416 [==============================] - 0s 61us/step - loss: 1.5473 - acc: 0.2885 - val_loss: 1.6747 - val_acc: 0.2514\n",
      "Epoch 100/100\n",
      "416/416 [==============================] - 0s 55us/step - loss: 1.5358 - acc: 0.2764 - val_loss: 1.7081 - val_acc: 0.2571\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a12fcd048>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newmodel_my.fit(X_train_my, Y_train_my,\n",
    "      epochs=100,\n",
    "      batch_size=128,\n",
    "      shuffle=True,\n",
    "      validation_data=(X_test_my, Y_test_my))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175/175 [==============================] - 0s 1ms/step\n",
      "1.70810932473\n",
      "175/175 [==============================] - 0s 93us/step\n",
      "[ 1.85871899  2.87476921  2.01511073  2.40642548  2.11304402  1.60386717\n",
      "  1.70133293  2.3633945   0.90049863  1.94989061  2.07837796  2.00807619\n",
      "  2.32843232  2.59511399  1.08046043  1.89831054  1.77510548  2.39557219\n",
      "  2.72008491  1.12093019  2.69212484  2.67510653  2.04625607  2.16133308\n",
      "  1.82447982  1.65590596  1.04878223  3.51599669  2.21123195  2.4143033\n",
      "  3.00106692  2.4843595   1.92918515  2.63360333  2.47121143  2.43895769\n",
      "  2.99623823  1.90667284  1.94910979  1.92227459  2.18640852  2.56006384\n",
      "  2.25464725  1.38339984  2.37464356  1.95904744  1.74801183  1.79893148\n",
      "  1.30594242  1.95656359  1.87769115  1.84134173  1.90879691  2.19134402\n",
      "  1.73937523  1.83098447  3.02311873  2.53750038  2.44663072  2.50017166\n",
      "  0.73390704  2.59239244  1.56949568  1.36093974  2.17798495  1.82704437\n",
      "  1.36643529  1.74714887  1.91300809  2.18809915  2.06315565  2.16293001\n",
      "  2.16055632  2.07299066  2.70339894  2.36829996  1.80959392  2.14585352\n",
      "  1.68416345  1.50865257  0.81357497  1.95896888  1.11526251  1.71761882\n",
      "  1.76853299  1.68701482  1.72660625  1.9994601   1.2781105   3.42949653\n",
      "  1.33221877  2.43045902  3.20776033  1.73650336  1.19732118  2.34913588\n",
      "  2.38222766  1.85102773  1.99747765  2.01165366  2.53579617  2.54672194\n",
      "  2.19581771  1.2818023   2.06347346  1.78525233  1.96231544  1.57707119\n",
      "  2.0005753   1.13526094  1.55755866  1.82075131  1.79138935  1.64562333\n",
      "  0.73390704  2.17086673  1.9904362   1.25650287  0.22575018  1.98825133\n",
      "  2.25519848  1.32344556  1.30093062  1.64501429  2.49567986  1.94148314\n",
      "  1.89444029  1.66498494  2.21297812  2.19480228  1.88461292  1.80425107\n",
      "  1.40562022  2.67811275  1.45803952  2.17178226  2.03871131  1.91124022\n",
      "  1.77494395  1.83857512  2.28802729  1.82706141  1.47740626  1.32761633\n",
      "  1.80467415  1.80479062  1.37497783  1.7007736   2.34059143  1.83857512\n",
      "  1.63445604  1.44966424  1.22828197  1.06213236  0.73154396  1.7750597\n",
      "  1.61958945  2.01471686  1.82553875  0.97618616  1.06447172  1.14458275\n",
      "  0.65654266  1.60583389  1.53158629  1.33788502  1.55376518  1.83857512\n",
      "  1.75184405  1.37795532  2.56984043  1.57707119  1.94391918  1.08913946\n",
      "  2.022475  ]\n",
      "175/175 [==============================] - 0s 90us/step\n",
      "[ 4.  5.  5.  5.  1.  4.  1.  1.  2.  2.  1.  1.  5.  1.  1.  2.  1.  1.\n",
      "  3.  2.  5.  1.  4.  3.  2.  1.  1.  5.  1.  1.  1.  1.  3.  4.  4.  5.\n",
      "  4.  1.  3.  1.  5.  2.  5.  1.  4.  1.  5.  1.  1.  1.  1.  4.  2.  4.\n",
      "  3.  1.  1.  2.  5.  4.  1.  4.  3.  1.  4.  1.  1.  5.  2.  4.  4.  2.\n",
      "  2.  1.  1.  1.  1.  2.  1.  1.  1.  4.  1.  3.  1.  3.  3.  1.  4.  2.\n",
      "  1.  4.  3.  1.  1.  3.  1.  1.  1.  5.  1.  1.  4.  1.  1.  1.  5.  1.\n",
      "  1.  1.  1.  1.  3.  1.  1.  1.  5.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  3.  1.  2.  1.  1.  1.  1.  1.  1.  3.  1.  1.  3.  1.  1.  1.  3.  3.\n",
      "  1.  1.  1.  3.  5.  3.  1.  1.  1.  1.  1.  1.  2.  1.  2.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  2.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "#print(newmodel_my.predict(X_test_my, verbose=1).flatten())\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mse = mean_squared_error(newmodel_my.predict(X_test_my, verbose=1).flatten(), Y_test_my)\n",
    "\n",
    "print(mse)\n",
    "\n",
    "print(newmodel_my.predict(X_test_my, verbose=1).flatten())\n",
    "\n",
    "#print('Rounded')\n",
    "#print(np.around(newmodel_my.predict(X_test_my, verbose=1).flatten(),decimals=0))\n",
    "\n",
    "scores_my = newmodel_my.evaluate(X_test_my, Y_test_my, verbose=1) \n",
    "print(y_test_my)\n",
    "\n",
    "#print(\"Accuracy: \", scores_my[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
