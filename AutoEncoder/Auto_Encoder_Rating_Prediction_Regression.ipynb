{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model \n",
    "from keras.layers import Input, Dense \n",
    "from keras.utils import np_utils \n",
    "import numpy as np\n",
    "from tensorflow.python.ops.variables import trainable_variables\n",
    "from numpy import genfromtxt\n",
    "\n",
    "X_train_my = genfromtxt('../UserData/405/trainX_405.csv', delimiter=',')\n",
    "y_train_my = genfromtxt('../UserData/405/trainY_405.csv', delimiter=',')\n",
    "X_test_my = genfromtxt('../UserData/405/testX_405.csv', delimiter=',')\n",
    "y_test_my = genfromtxt('../UserData/405/testY_405.csv', delimiter=',')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(462, 20)\n"
     ]
    }
   ],
   "source": [
    "X_train_my = X_train_my.astype('float32') \n",
    "X_train_my = X_train_my.astype('float32')\n",
    "print(X_train_my.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Y_train_my = y_train_my\n",
    "\n",
    "Y_test_my = y_test_my\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"input_5:0\", shape=(?, 20), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "input_img_my = Input(shape=(20,))\n",
    "\n",
    "print(input_img_my)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"dense_33/Relu:0\", shape=(?, 20), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x_my = Dense(20, activation='relu')(input_img_my)\n",
    "print(x_my)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoded1_my = Dense(15, activation='relu')(x_my)\n",
    "encoded2_my = Dense(12, activation='relu')(encoded1_my)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_my = Dense(10, activation='relu')(encoded2_my)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoded2_my = Dense(12, activation='relu')(y_my)\n",
    "decoded1_my = Dense(15, activation='relu')(decoded2_my)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z_my = Dense(20, activation='relu')(decoded1_my)\n",
    "autoencoder_my = Model(input_img_my, z_my)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.training.Model object at 0x1a135a5668>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#encoder is the model of the autoencoder slice in the middle \n",
    "encoder_my = Model(input_img_my, y_my)\n",
    "\n",
    "print(encoder_my)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "autoencoder_my.compile(loss='mse', optimizer='rmsprop') # reporting the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(273, 20)\n",
      "(462, 20)\n"
     ]
    }
   ],
   "source": [
    "print(X_test_my.shape)\n",
    "\n",
    "print(X_train_my.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.  0.  5. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  1.  0.  4.]\n",
      " [ 0.  0.  0. ...,  0.  1.  0.]\n",
      " ..., \n",
      " [ 4.  0.  3. ...,  0.  2.  0.]\n",
      " [ 0.  5.  5. ...,  5.  0.  0.]\n",
      " [ 4.  5.  4. ...,  5.  3.  4.]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train_my)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 462 samples, validate on 273 samples\n",
      "Epoch 1/100\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 3.7720 - val_loss: 3.4074\n",
      "Epoch 2/100\n",
      "462/462 [==============================] - 0s 500us/step - loss: 3.2685 - val_loss: 3.3636\n",
      "Epoch 3/100\n",
      "462/462 [==============================] - 0s 502us/step - loss: 3.2365 - val_loss: 3.3223\n",
      "Epoch 4/100\n",
      "462/462 [==============================] - 0s 512us/step - loss: 3.1527 - val_loss: 3.0956\n",
      "Epoch 5/100\n",
      "462/462 [==============================] - 0s 540us/step - loss: 2.9548 - val_loss: 3.0206\n",
      "Epoch 6/100\n",
      "462/462 [==============================] - 0s 519us/step - loss: 2.8927 - val_loss: 2.9167\n",
      "Epoch 7/100\n",
      "462/462 [==============================] - 0s 518us/step - loss: 2.7347 - val_loss: 2.6972\n",
      "Epoch 8/100\n",
      "462/462 [==============================] - 0s 494us/step - loss: 2.5815 - val_loss: 2.6483\n",
      "Epoch 9/100\n",
      "462/462 [==============================] - 0s 481us/step - loss: 2.5377 - val_loss: 2.6312\n",
      "Epoch 10/100\n",
      "462/462 [==============================] - 0s 525us/step - loss: 2.5186 - val_loss: 2.5964\n",
      "Epoch 11/100\n",
      "462/462 [==============================] - 0s 476us/step - loss: 2.4997 - val_loss: 2.6054\n",
      "Epoch 12/100\n",
      "462/462 [==============================] - 0s 388us/step - loss: 2.4839 - val_loss: 2.5723\n",
      "Epoch 13/100\n",
      "462/462 [==============================] - 0s 467us/step - loss: 2.4682 - val_loss: 2.5695\n",
      "Epoch 14/100\n",
      "462/462 [==============================] - 0s 466us/step - loss: 2.4630 - val_loss: 2.5661\n",
      "Epoch 15/100\n",
      "462/462 [==============================] - 0s 461us/step - loss: 2.4485 - val_loss: 2.5858\n",
      "Epoch 16/100\n",
      "462/462 [==============================] - 0s 462us/step - loss: 2.4467 - val_loss: 2.5398\n",
      "Epoch 17/100\n",
      "462/462 [==============================] - 0s 459us/step - loss: 2.4333 - val_loss: 2.5350\n",
      "Epoch 18/100\n",
      "462/462 [==============================] - 0s 465us/step - loss: 2.4298 - val_loss: 2.5326\n",
      "Epoch 19/100\n",
      "462/462 [==============================] - 0s 441us/step - loss: 2.4215 - val_loss: 2.5155\n",
      "Epoch 20/100\n",
      "462/462 [==============================] - 0s 471us/step - loss: 2.4163 - val_loss: 2.5092\n",
      "Epoch 21/100\n",
      "462/462 [==============================] - 0s 461us/step - loss: 2.4101 - val_loss: 2.5094\n",
      "Epoch 22/100\n",
      "462/462 [==============================] - 0s 449us/step - loss: 2.4030 - val_loss: 2.4975\n",
      "Epoch 23/100\n",
      "462/462 [==============================] - 0s 462us/step - loss: 2.3994 - val_loss: 2.5115\n",
      "Epoch 24/100\n",
      "462/462 [==============================] - 0s 474us/step - loss: 2.3912 - val_loss: 2.4916\n",
      "Epoch 25/100\n",
      "462/462 [==============================] - 0s 451us/step - loss: 2.3835 - val_loss: 2.5229\n",
      "Epoch 26/100\n",
      "462/462 [==============================] - 0s 471us/step - loss: 2.3802 - val_loss: 2.4884\n",
      "Epoch 27/100\n",
      "462/462 [==============================] - 0s 460us/step - loss: 2.3692 - val_loss: 2.4855\n",
      "Epoch 28/100\n",
      "462/462 [==============================] - 0s 470us/step - loss: 2.3593 - val_loss: 2.4598\n",
      "Epoch 29/100\n",
      "462/462 [==============================] - 0s 474us/step - loss: 2.3498 - val_loss: 2.5198\n",
      "Epoch 30/100\n",
      "462/462 [==============================] - 0s 473us/step - loss: 2.3454 - val_loss: 2.4567\n",
      "Epoch 31/100\n",
      "462/462 [==============================] - 0s 461us/step - loss: 2.3279 - val_loss: 2.4264\n",
      "Epoch 32/100\n",
      "462/462 [==============================] - 0s 444us/step - loss: 2.3180 - val_loss: 2.4273\n",
      "Epoch 33/100\n",
      "462/462 [==============================] - 0s 456us/step - loss: 2.3017 - val_loss: 2.4226\n",
      "Epoch 34/100\n",
      "462/462 [==============================] - 0s 470us/step - loss: 2.2898 - val_loss: 2.3870\n",
      "Epoch 35/100\n",
      "462/462 [==============================] - 0s 465us/step - loss: 2.2772 - val_loss: 2.3743\n",
      "Epoch 36/100\n",
      "462/462 [==============================] - 0s 468us/step - loss: 2.2613 - val_loss: 2.3691\n",
      "Epoch 37/100\n",
      "462/462 [==============================] - 0s 467us/step - loss: 2.2495 - val_loss: 2.3662\n",
      "Epoch 38/100\n",
      "462/462 [==============================] - 0s 469us/step - loss: 2.2376 - val_loss: 2.3338\n",
      "Epoch 39/100\n",
      "462/462 [==============================] - 0s 446us/step - loss: 2.2291 - val_loss: 2.3293\n",
      "Epoch 40/100\n",
      "462/462 [==============================] - 0s 458us/step - loss: 2.2166 - val_loss: 2.3202\n",
      "Epoch 41/100\n",
      "462/462 [==============================] - 0s 472us/step - loss: 2.2098 - val_loss: 2.3333\n",
      "Epoch 42/100\n",
      "462/462 [==============================] - 0s 466us/step - loss: 2.2030 - val_loss: 2.2993\n",
      "Epoch 43/100\n",
      "462/462 [==============================] - 0s 451us/step - loss: 2.1948 - val_loss: 2.2952\n",
      "Epoch 44/100\n",
      "462/462 [==============================] - 0s 470us/step - loss: 2.1894 - val_loss: 2.3100\n",
      "Epoch 45/100\n",
      "462/462 [==============================] - 0s 468us/step - loss: 2.1860 - val_loss: 2.3023\n",
      "Epoch 46/100\n",
      "462/462 [==============================] - 0s 470us/step - loss: 2.1764 - val_loss: 2.2688\n",
      "Epoch 47/100\n",
      "462/462 [==============================] - 0s 462us/step - loss: 2.1752 - val_loss: 2.2823\n",
      "Epoch 48/100\n",
      "462/462 [==============================] - 0s 457us/step - loss: 2.1662 - val_loss: 2.2629\n",
      "Epoch 49/100\n",
      "462/462 [==============================] - 0s 473us/step - loss: 2.1613 - val_loss: 2.2622\n",
      "Epoch 50/100\n",
      "462/462 [==============================] - 0s 466us/step - loss: 2.1559 - val_loss: 2.2790\n",
      "Epoch 51/100\n",
      "462/462 [==============================] - 0s 437us/step - loss: 2.1495 - val_loss: 2.2564\n",
      "Epoch 52/100\n",
      "462/462 [==============================] - 0s 438us/step - loss: 2.1458 - val_loss: 2.2746\n",
      "Epoch 53/100\n",
      "462/462 [==============================] - 0s 469us/step - loss: 2.1389 - val_loss: 2.3506\n",
      "Epoch 54/100\n",
      "462/462 [==============================] - 0s 489us/step - loss: 2.1317 - val_loss: 2.2596\n",
      "Epoch 55/100\n",
      "462/462 [==============================] - 0s 467us/step - loss: 2.1288 - val_loss: 2.2809\n",
      "Epoch 56/100\n",
      "462/462 [==============================] - 0s 455us/step - loss: 2.1220 - val_loss: 2.2746\n",
      "Epoch 57/100\n",
      "462/462 [==============================] - 0s 471us/step - loss: 2.1201 - val_loss: 2.2364\n",
      "Epoch 58/100\n",
      "462/462 [==============================] - 0s 548us/step - loss: 2.1139 - val_loss: 2.2318\n",
      "Epoch 59/100\n",
      "462/462 [==============================] - 0s 557us/step - loss: 2.1131 - val_loss: 2.2446\n",
      "Epoch 60/100\n",
      "462/462 [==============================] - 0s 555us/step - loss: 2.1051 - val_loss: 2.2291\n",
      "Epoch 61/100\n",
      "462/462 [==============================] - 0s 509us/step - loss: 2.1015 - val_loss: 2.2686\n",
      "Epoch 62/100\n",
      "462/462 [==============================] - 0s 437us/step - loss: 2.0984 - val_loss: 2.2317\n",
      "Epoch 63/100\n",
      "462/462 [==============================] - 0s 335us/step - loss: 2.0973 - val_loss: 2.2633\n",
      "Epoch 64/100\n",
      "462/462 [==============================] - 0s 474us/step - loss: 2.0930 - val_loss: 2.2370\n",
      "Epoch 65/100\n",
      "462/462 [==============================] - 0s 442us/step - loss: 2.0908 - val_loss: 2.2391\n",
      "Epoch 66/100\n",
      "462/462 [==============================] - 0s 477us/step - loss: 2.0889 - val_loss: 2.2169\n",
      "Epoch 67/100\n",
      "462/462 [==============================] - 0s 475us/step - loss: 2.0855 - val_loss: 2.2185\n",
      "Epoch 68/100\n",
      "462/462 [==============================] - 0s 453us/step - loss: 2.0804 - val_loss: 2.2230\n",
      "Epoch 69/100\n",
      "462/462 [==============================] - 0s 456us/step - loss: 2.0840 - val_loss: 2.2554\n",
      "Epoch 70/100\n",
      "462/462 [==============================] - 0s 478us/step - loss: 2.0779 - val_loss: 2.2843\n",
      "Epoch 71/100\n",
      "462/462 [==============================] - 0s 463us/step - loss: 2.0772 - val_loss: 2.2177\n",
      "Epoch 72/100\n",
      "462/462 [==============================] - 0s 456us/step - loss: 2.0777 - val_loss: 2.2132\n",
      "Epoch 73/100\n",
      "462/462 [==============================] - 0s 468us/step - loss: 2.0724 - val_loss: 2.2143\n",
      "Epoch 74/100\n",
      "462/462 [==============================] - 0s 454us/step - loss: 2.0699 - val_loss: 2.2158\n",
      "Epoch 75/100\n",
      "462/462 [==============================] - 0s 464us/step - loss: 2.0692 - val_loss: 2.2091\n",
      "Epoch 76/100\n",
      "462/462 [==============================] - 0s 464us/step - loss: 2.0682 - val_loss: 2.2459\n",
      "Epoch 77/100\n",
      "462/462 [==============================] - 0s 457us/step - loss: 2.0651 - val_loss: 2.2100\n",
      "Epoch 78/100\n",
      "462/462 [==============================] - 0s 474us/step - loss: 2.0648 - val_loss: 2.2043\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "462/462 [==============================] - 0s 454us/step - loss: 2.0635 - val_loss: 2.2065\n",
      "Epoch 80/100\n",
      "462/462 [==============================] - 0s 479us/step - loss: 2.0589 - val_loss: 2.2034\n",
      "Epoch 81/100\n",
      "462/462 [==============================] - 0s 455us/step - loss: 2.0556 - val_loss: 2.2031\n",
      "Epoch 82/100\n",
      "462/462 [==============================] - 0s 463us/step - loss: 2.0546 - val_loss: 2.2025\n",
      "Epoch 83/100\n",
      "462/462 [==============================] - 0s 482us/step - loss: 2.0504 - val_loss: 2.2032\n",
      "Epoch 84/100\n",
      "462/462 [==============================] - 0s 463us/step - loss: 2.0503 - val_loss: 2.2480\n",
      "Epoch 85/100\n",
      "462/462 [==============================] - 0s 463us/step - loss: 2.0521 - val_loss: 2.1987\n",
      "Epoch 86/100\n",
      "462/462 [==============================] - 0s 465us/step - loss: 2.0477 - val_loss: 2.1982\n",
      "Epoch 87/100\n",
      "462/462 [==============================] - 0s 462us/step - loss: 2.0427 - val_loss: 2.1878\n",
      "Epoch 88/100\n",
      "462/462 [==============================] - 0s 459us/step - loss: 2.0462 - val_loss: 2.1896\n",
      "Epoch 89/100\n",
      "462/462 [==============================] - 0s 464us/step - loss: 2.0401 - val_loss: 2.1957\n",
      "Epoch 90/100\n",
      "462/462 [==============================] - 0s 466us/step - loss: 2.0383 - val_loss: 2.1954\n",
      "Epoch 91/100\n",
      "462/462 [==============================] - 0s 459us/step - loss: 2.0390 - val_loss: 2.1874\n",
      "Epoch 92/100\n",
      "462/462 [==============================] - 0s 454us/step - loss: 2.0361 - val_loss: 2.1926\n",
      "Epoch 93/100\n",
      "462/462 [==============================] - 0s 463us/step - loss: 2.0343 - val_loss: 2.1913\n",
      "Epoch 94/100\n",
      "462/462 [==============================] - 0s 462us/step - loss: 2.0320 - val_loss: 2.2347\n",
      "Epoch 95/100\n",
      "462/462 [==============================] - 0s 449us/step - loss: 2.0308 - val_loss: 2.1996\n",
      "Epoch 96/100\n",
      "462/462 [==============================] - 0s 453us/step - loss: 2.0329 - val_loss: 2.1877\n",
      "Epoch 97/100\n",
      "462/462 [==============================] - 0s 454us/step - loss: 2.0304 - val_loss: 2.2149\n",
      "Epoch 98/100\n",
      "462/462 [==============================] - 0s 473us/step - loss: 2.0278 - val_loss: 2.1803\n",
      "Epoch 99/100\n",
      "462/462 [==============================] - 0s 475us/step - loss: 2.0268 - val_loss: 2.1805\n",
      "Epoch 100/100\n",
      "462/462 [==============================] - 0s 452us/step - loss: 2.0249 - val_loss: 2.1948\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a12be1898>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder_my.fit(X_train_my, X_train_my,\n",
    "      epochs=100,\n",
    "      batch_size=10,\n",
    "      shuffle=True,\n",
    "      validation_data=(X_test_my, X_test_my)\n",
    "      )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.          14.60200214   4.38574409 ...,   0.           9.43326378\n",
      "    9.51659775]\n",
      " [  0.30003288   2.69222283   1.8530966  ...,   1.21046364   1.86465275\n",
      "    2.03811955]\n",
      " [  0.          13.77743435   4.16941071 ...,   0.           5.88387346\n",
      "    1.96670711]\n",
      " ..., \n",
      " [  0.30003288   2.69222283   1.8530966  ...,   1.21046364   1.86465275\n",
      "    2.03811955]\n",
      " [  2.63208961   3.99827862   6.1031065  ...,   0.           9.61084843\n",
      "    6.39769793]\n",
      " [  0.30003288   2.69222283   1.8530966  ...,   1.21046364   1.86465275\n",
      "    2.03811955]]\n"
     ]
    }
   ],
   "source": [
    "# if you want an encoded flatten representation of every test MNIST\n",
    "reduced_representation_my =encoder_my.predict(X_test_my)\n",
    "print(reduced_representation_my)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 462 samples, validate on 273 samples\n",
      "Epoch 1/100\n",
      "462/462 [==============================] - 1s 1ms/step - loss: 2.0306 - val_loss: 2.1781\n",
      "Epoch 2/100\n",
      "462/462 [==============================] - 0s 485us/step - loss: 2.0220 - val_loss: 2.1870\n",
      "Epoch 3/100\n",
      "462/462 [==============================] - 0s 483us/step - loss: 2.0239 - val_loss: 2.1840\n",
      "Epoch 4/100\n",
      "462/462 [==============================] - 0s 482us/step - loss: 2.0199 - val_loss: 2.1914\n",
      "Epoch 5/100\n",
      "462/462 [==============================] - 0s 475us/step - loss: 2.0157 - val_loss: 2.2053\n",
      "Epoch 6/100\n",
      "462/462 [==============================] - 0s 494us/step - loss: 2.0113 - val_loss: 2.1877\n",
      "Epoch 7/100\n",
      "462/462 [==============================] - 0s 487us/step - loss: 2.0003 - val_loss: 2.1630\n",
      "Epoch 8/100\n",
      "462/462 [==============================] - 0s 478us/step - loss: 1.9344 - val_loss: 2.0509\n",
      "Epoch 9/100\n",
      "462/462 [==============================] - 0s 481us/step - loss: 1.8828 - val_loss: 2.0734\n",
      "Epoch 10/100\n",
      "462/462 [==============================] - 0s 492us/step - loss: 1.8754 - val_loss: 2.0281\n",
      "Epoch 11/100\n",
      "462/462 [==============================] - 0s 481us/step - loss: 1.8659 - val_loss: 2.0501\n",
      "Epoch 12/100\n",
      "462/462 [==============================] - 0s 480us/step - loss: 1.8636 - val_loss: 2.0365\n",
      "Epoch 13/100\n",
      "462/462 [==============================] - 0s 479us/step - loss: 1.8580 - val_loss: 2.0109\n",
      "Epoch 14/100\n",
      "462/462 [==============================] - 0s 479us/step - loss: 1.8518 - val_loss: 2.0135\n",
      "Epoch 15/100\n",
      "462/462 [==============================] - 0s 477us/step - loss: 1.8522 - val_loss: 2.0569\n",
      "Epoch 16/100\n",
      "462/462 [==============================] - 0s 477us/step - loss: 1.8493 - val_loss: 2.0032\n",
      "Epoch 17/100\n",
      "462/462 [==============================] - 0s 482us/step - loss: 1.8449 - val_loss: 2.0153\n",
      "Epoch 18/100\n",
      "462/462 [==============================] - 0s 455us/step - loss: 1.8439 - val_loss: 2.0120\n",
      "Epoch 19/100\n",
      "462/462 [==============================] - 0s 490us/step - loss: 1.8413 - val_loss: 2.0414\n",
      "Epoch 20/100\n",
      "462/462 [==============================] - 0s 469us/step - loss: 1.8410 - val_loss: 2.0192\n",
      "Epoch 21/100\n",
      "462/462 [==============================] - 0s 486us/step - loss: 1.8379 - val_loss: 2.0208\n",
      "Epoch 22/100\n",
      "462/462 [==============================] - 0s 481us/step - loss: 1.8385 - val_loss: 2.0256\n",
      "Epoch 23/100\n",
      "462/462 [==============================] - 0s 463us/step - loss: 1.8357 - val_loss: 2.0243\n",
      "Epoch 24/100\n",
      "462/462 [==============================] - 0s 491us/step - loss: 1.8350 - val_loss: 2.0136\n",
      "Epoch 25/100\n",
      "462/462 [==============================] - 0s 575us/step - loss: 1.8319 - val_loss: 2.0087\n",
      "Epoch 26/100\n",
      "462/462 [==============================] - 0s 598us/step - loss: 1.8334 - val_loss: 2.0139\n",
      "Epoch 27/100\n",
      "462/462 [==============================] - 0s 628us/step - loss: 1.8322 - val_loss: 2.0044\n",
      "Epoch 28/100\n",
      "462/462 [==============================] - 0s 675us/step - loss: 1.8292 - val_loss: 2.0221\n",
      "Epoch 29/100\n",
      "462/462 [==============================] - 0s 676us/step - loss: 1.8288 - val_loss: 2.0170\n",
      "Epoch 30/100\n",
      "462/462 [==============================] - 0s 526us/step - loss: 1.8268 - val_loss: 1.9943\n",
      "Epoch 31/100\n",
      "462/462 [==============================] - 0s 484us/step - loss: 1.8250 - val_loss: 1.9870\n",
      "Epoch 32/100\n",
      "462/462 [==============================] - 0s 496us/step - loss: 1.8264 - val_loss: 1.9922\n",
      "Epoch 33/100\n",
      "462/462 [==============================] - 0s 496us/step - loss: 1.8236 - val_loss: 1.9962\n",
      "Epoch 34/100\n",
      "462/462 [==============================] - 0s 465us/step - loss: 1.8206 - val_loss: 1.9891\n",
      "Epoch 35/100\n",
      "462/462 [==============================] - 0s 462us/step - loss: 1.8224 - val_loss: 2.0313\n",
      "Epoch 36/100\n",
      "462/462 [==============================] - 0s 456us/step - loss: 1.8218 - val_loss: 1.9948\n",
      "Epoch 37/100\n",
      "462/462 [==============================] - 0s 450us/step - loss: 1.8194 - val_loss: 1.9935\n",
      "Epoch 38/100\n",
      "462/462 [==============================] - 0s 458us/step - loss: 1.8174 - val_loss: 2.0168\n",
      "Epoch 39/100\n",
      "462/462 [==============================] - 0s 468us/step - loss: 1.8171 - val_loss: 1.9871\n",
      "Epoch 40/100\n",
      "462/462 [==============================] - 0s 473us/step - loss: 1.8146 - val_loss: 1.9932\n",
      "Epoch 41/100\n",
      "462/462 [==============================] - 0s 447us/step - loss: 1.8153 - val_loss: 1.9942\n",
      "Epoch 42/100\n",
      "462/462 [==============================] - 0s 376us/step - loss: 1.8137 - val_loss: 2.0089\n",
      "Epoch 43/100\n",
      "462/462 [==============================] - 0s 466us/step - loss: 1.8146 - val_loss: 2.0092\n",
      "Epoch 44/100\n",
      "462/462 [==============================] - 0s 463us/step - loss: 1.8114 - val_loss: 1.9946\n",
      "Epoch 45/100\n",
      "462/462 [==============================] - 0s 475us/step - loss: 1.8100 - val_loss: 2.0595\n",
      "Epoch 46/100\n",
      "462/462 [==============================] - 0s 454us/step - loss: 1.8099 - val_loss: 2.0161\n",
      "Epoch 47/100\n",
      "462/462 [==============================] - 0s 462us/step - loss: 1.8095 - val_loss: 1.9880\n",
      "Epoch 48/100\n",
      "462/462 [==============================] - 0s 460us/step - loss: 1.8092 - val_loss: 2.1184\n",
      "Epoch 49/100\n",
      "462/462 [==============================] - 0s 468us/step - loss: 1.8070 - val_loss: 1.9858\n",
      "Epoch 50/100\n",
      "462/462 [==============================] - 0s 456us/step - loss: 1.8046 - val_loss: 1.9824\n",
      "Epoch 51/100\n",
      "462/462 [==============================] - 0s 457us/step - loss: 1.8048 - val_loss: 2.0683\n",
      "Epoch 52/100\n",
      "462/462 [==============================] - 0s 463us/step - loss: 1.8054 - val_loss: 2.0059\n",
      "Epoch 53/100\n",
      "462/462 [==============================] - 0s 468us/step - loss: 1.8030 - val_loss: 1.9940\n",
      "Epoch 54/100\n",
      "462/462 [==============================] - 0s 449us/step - loss: 1.8032 - val_loss: 2.0012\n",
      "Epoch 55/100\n",
      "462/462 [==============================] - 0s 469us/step - loss: 1.8017 - val_loss: 2.0116\n",
      "Epoch 56/100\n",
      "462/462 [==============================] - 0s 464us/step - loss: 1.8004 - val_loss: 2.0375\n",
      "Epoch 57/100\n",
      "462/462 [==============================] - 0s 453us/step - loss: 1.8017 - val_loss: 1.9954\n",
      "Epoch 58/100\n",
      "462/462 [==============================] - 0s 477us/step - loss: 1.8007 - val_loss: 2.0145\n",
      "Epoch 59/100\n",
      "462/462 [==============================] - 0s 472us/step - loss: 1.7988 - val_loss: 1.9897\n",
      "Epoch 60/100\n",
      "462/462 [==============================] - 0s 460us/step - loss: 1.7977 - val_loss: 2.0131\n",
      "Epoch 61/100\n",
      "462/462 [==============================] - 0s 467us/step - loss: 1.7975 - val_loss: 1.9848\n",
      "Epoch 62/100\n",
      "462/462 [==============================] - 0s 454us/step - loss: 1.7954 - val_loss: 2.0080\n",
      "Epoch 63/100\n",
      "462/462 [==============================] - 0s 455us/step - loss: 1.7958 - val_loss: 1.9915\n",
      "Epoch 64/100\n",
      "462/462 [==============================] - 0s 474us/step - loss: 1.7943 - val_loss: 2.0041\n",
      "Epoch 65/100\n",
      "462/462 [==============================] - 0s 465us/step - loss: 1.7951 - val_loss: 1.9821\n",
      "Epoch 66/100\n",
      "462/462 [==============================] - 0s 432us/step - loss: 1.7960 - val_loss: 2.0097\n",
      "Epoch 67/100\n",
      "462/462 [==============================] - 0s 456us/step - loss: 1.7912 - val_loss: 2.0090\n",
      "Epoch 68/100\n",
      "462/462 [==============================] - 0s 446us/step - loss: 1.7939 - val_loss: 1.9770\n",
      "Epoch 69/100\n",
      "462/462 [==============================] - 0s 443us/step - loss: 1.7909 - val_loss: 1.9766\n",
      "Epoch 70/100\n",
      "462/462 [==============================] - 0s 467us/step - loss: 1.7932 - val_loss: 2.0231\n",
      "Epoch 71/100\n",
      "462/462 [==============================] - 0s 458us/step - loss: 1.7909 - val_loss: 2.0054\n",
      "Epoch 72/100\n",
      "462/462 [==============================] - 0s 455us/step - loss: 1.7899 - val_loss: 2.0023\n",
      "Epoch 73/100\n",
      "462/462 [==============================] - 0s 469us/step - loss: 1.7909 - val_loss: 2.0318\n",
      "Epoch 74/100\n",
      "462/462 [==============================] - 0s 464us/step - loss: 1.7914 - val_loss: 1.9969\n",
      "Epoch 75/100\n",
      "462/462 [==============================] - 0s 464us/step - loss: 1.7831 - val_loss: 1.9952\n",
      "Epoch 76/100\n",
      "462/462 [==============================] - 0s 462us/step - loss: 1.7879 - val_loss: 1.9831\n",
      "Epoch 77/100\n",
      "462/462 [==============================] - 0s 467us/step - loss: 1.7869 - val_loss: 1.9984\n",
      "Epoch 78/100\n",
      "462/462 [==============================] - 0s 470us/step - loss: 1.7874 - val_loss: 2.0135\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "462/462 [==============================] - 0s 472us/step - loss: 1.7882 - val_loss: 2.0251\n",
      "Epoch 80/100\n",
      "462/462 [==============================] - 0s 454us/step - loss: 1.7820 - val_loss: 2.0054\n",
      "Epoch 81/100\n",
      "462/462 [==============================] - 0s 456us/step - loss: 1.7842 - val_loss: 1.9757\n",
      "Epoch 82/100\n",
      "462/462 [==============================] - 0s 346us/step - loss: 1.7813 - val_loss: 2.0442\n",
      "Epoch 83/100\n",
      "462/462 [==============================] - 0s 357us/step - loss: 1.7843 - val_loss: 1.9902\n",
      "Epoch 84/100\n",
      "462/462 [==============================] - 0s 390us/step - loss: 1.7854 - val_loss: 2.0102\n",
      "Epoch 85/100\n",
      "462/462 [==============================] - 0s 462us/step - loss: 1.7816 - val_loss: 1.9983\n",
      "Epoch 86/100\n",
      "462/462 [==============================] - 0s 321us/step - loss: 1.7816 - val_loss: 1.9880\n",
      "Epoch 87/100\n",
      "462/462 [==============================] - 0s 301us/step - loss: 1.7841 - val_loss: 2.0687\n",
      "Epoch 88/100\n",
      "462/462 [==============================] - 0s 370us/step - loss: 1.7793 - val_loss: 1.9785\n",
      "Epoch 89/100\n",
      "462/462 [==============================] - 0s 481us/step - loss: 1.7786 - val_loss: 2.0009\n",
      "Epoch 90/100\n",
      "462/462 [==============================] - 0s 463us/step - loss: 1.7796 - val_loss: 2.0144\n",
      "Epoch 91/100\n",
      "462/462 [==============================] - 0s 462us/step - loss: 1.7765 - val_loss: 2.0153\n",
      "Epoch 92/100\n",
      "462/462 [==============================] - 0s 469us/step - loss: 1.7788 - val_loss: 1.9995\n",
      "Epoch 93/100\n",
      "462/462 [==============================] - 0s 472us/step - loss: 1.7752 - val_loss: 1.9943\n",
      "Epoch 94/100\n",
      "462/462 [==============================] - 0s 462us/step - loss: 1.7778 - val_loss: 2.0397\n",
      "Epoch 95/100\n",
      "462/462 [==============================] - 0s 463us/step - loss: 1.7766 - val_loss: 1.9916\n",
      "Epoch 96/100\n",
      "462/462 [==============================] - 0s 521us/step - loss: 1.7755 - val_loss: 1.9798\n",
      "Epoch 97/100\n",
      "462/462 [==============================] - 0s 454us/step - loss: 1.7758 - val_loss: 2.0109\n",
      "Epoch 98/100\n",
      "462/462 [==============================] - 0s 455us/step - loss: 1.7726 - val_loss: 1.9900\n",
      "Epoch 99/100\n",
      "462/462 [==============================] - 0s 472us/step - loss: 1.7743 - val_loss: 2.0094\n",
      "Epoch 100/100\n",
      "462/462 [==============================] - 0s 455us/step - loss: 1.7740 - val_loss: 1.9992\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a13de7390>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#encoder is the model of the autoencoder slice in the middle \n",
    "encoder_my = Model(input_img_my, y_my)\n",
    "\n",
    "autoencoder_my.compile(loss='mse', optimizer='rmsprop') # reporting the loss\n",
    "\n",
    "autoencoder_my.fit(X_train_my, X_train_my,\n",
    "      epochs=100,\n",
    "      batch_size=10,\n",
    "      shuffle=True,\n",
    "      validation_data=(X_test_my, X_test_my))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if you want an encoded flatten representation of every test MNIST\n",
    "reduced_representation_my =encoder_my.predict(X_test_my)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out2_my = Dense(1, activation='linear')(encoder_my.output)\n",
    "newmodel_my = Model(encoder_my.input,out2_my)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newmodel_my.compile(loss='mean_squared_error', optimizer='rmsprop', \n",
    "          metrics=['accuracy']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 462 samples, validate on 273 samples\n",
      "Epoch 1/100\n",
      "462/462 [==============================] - 0s 1ms/step - loss: 280.1270 - acc: 0.0000e+00 - val_loss: 178.2938 - val_acc: 0.0000e+00\n",
      "Epoch 2/100\n",
      "462/462 [==============================] - 0s 62us/step - loss: 154.2253 - acc: 0.0000e+00 - val_loss: 113.0547 - val_acc: 0.0000e+00\n",
      "Epoch 3/100\n",
      "462/462 [==============================] - 0s 61us/step - loss: 99.7399 - acc: 0.0065 - val_loss: 71.4394 - val_acc: 0.0000e+00\n",
      "Epoch 4/100\n",
      "462/462 [==============================] - 0s 63us/step - loss: 61.1434 - acc: 0.0130 - val_loss: 42.4151 - val_acc: 0.0110\n",
      "Epoch 5/100\n",
      "462/462 [==============================] - 0s 56us/step - loss: 36.5873 - acc: 0.0260 - val_loss: 26.0373 - val_acc: 0.0476\n",
      "Epoch 6/100\n",
      "462/462 [==============================] - 0s 65us/step - loss: 23.2211 - acc: 0.0411 - val_loss: 17.6528 - val_acc: 0.0952\n",
      "Epoch 7/100\n",
      "462/462 [==============================] - 0s 64us/step - loss: 16.4365 - acc: 0.0606 - val_loss: 13.6841 - val_acc: 0.1062\n",
      "Epoch 8/100\n",
      "462/462 [==============================] - 0s 67us/step - loss: 13.1683 - acc: 0.0758 - val_loss: 11.4206 - val_acc: 0.1136\n",
      "Epoch 9/100\n",
      "462/462 [==============================] - 0s 67us/step - loss: 11.1842 - acc: 0.1104 - val_loss: 9.8978 - val_acc: 0.1502\n",
      "Epoch 10/100\n",
      "462/462 [==============================] - 0s 65us/step - loss: 9.7896 - acc: 0.1299 - val_loss: 8.7673 - val_acc: 0.1502\n",
      "Epoch 11/100\n",
      "462/462 [==============================] - 0s 67us/step - loss: 8.6205 - acc: 0.1450 - val_loss: 7.7483 - val_acc: 0.1575\n",
      "Epoch 12/100\n",
      "462/462 [==============================] - 0s 60us/step - loss: 7.6197 - acc: 0.1450 - val_loss: 6.9220 - val_acc: 0.1648\n",
      "Epoch 13/100\n",
      "462/462 [==============================] - 0s 57us/step - loss: 6.7540 - acc: 0.1429 - val_loss: 6.1525 - val_acc: 0.1722\n",
      "Epoch 14/100\n",
      "462/462 [==============================] - 0s 56us/step - loss: 5.9752 - acc: 0.1558 - val_loss: 5.5755 - val_acc: 0.1722\n",
      "Epoch 15/100\n",
      "462/462 [==============================] - ETA: 0s - loss: 4.9108 - acc: 0.187 - 0s 82us/step - loss: 5.3638 - acc: 0.1602 - val_loss: 4.9362 - val_acc: 0.1905\n",
      "Epoch 16/100\n",
      "462/462 [==============================] - 0s 68us/step - loss: 4.7879 - acc: 0.1688 - val_loss: 4.4817 - val_acc: 0.1868\n",
      "Epoch 17/100\n",
      "462/462 [==============================] - 0s 62us/step - loss: 4.2794 - acc: 0.1775 - val_loss: 3.9889 - val_acc: 0.2051\n",
      "Epoch 18/100\n",
      "462/462 [==============================] - 0s 67us/step - loss: 3.9060 - acc: 0.1970 - val_loss: 3.5912 - val_acc: 0.2088\n",
      "Epoch 19/100\n",
      "462/462 [==============================] - 0s 60us/step - loss: 3.5657 - acc: 0.1948 - val_loss: 3.3000 - val_acc: 0.2234\n",
      "Epoch 20/100\n",
      "462/462 [==============================] - 0s 62us/step - loss: 3.3504 - acc: 0.2035 - val_loss: 3.1321 - val_acc: 0.2381\n",
      "Epoch 21/100\n",
      "462/462 [==============================] - ETA: 0s - loss: 3.2143 - acc: 0.210 - 0s 52us/step - loss: 3.0393 - acc: 0.2251 - val_loss: 2.7432 - val_acc: 0.4176\n",
      "Epoch 22/100\n",
      "462/462 [==============================] - 0s 74us/step - loss: 2.7322 - acc: 0.3939 - val_loss: 2.8491 - val_acc: 0.4469\n",
      "Epoch 23/100\n",
      "462/462 [==============================] - 0s 70us/step - loss: 2.5930 - acc: 0.4004 - val_loss: 2.3551 - val_acc: 0.4396\n",
      "Epoch 24/100\n",
      "462/462 [==============================] - 0s 69us/step - loss: 2.4560 - acc: 0.4069 - val_loss: 2.2027 - val_acc: 0.4505\n",
      "Epoch 25/100\n",
      "462/462 [==============================] - 0s 84us/step - loss: 2.2790 - acc: 0.4069 - val_loss: 2.0674 - val_acc: 0.4542\n",
      "Epoch 26/100\n",
      "462/462 [==============================] - 0s 89us/step - loss: 2.1088 - acc: 0.4351 - val_loss: 2.0537 - val_acc: 0.4689\n",
      "Epoch 27/100\n",
      "462/462 [==============================] - 0s 88us/step - loss: 2.0330 - acc: 0.4394 - val_loss: 1.9308 - val_acc: 0.4725\n",
      "Epoch 28/100\n",
      "462/462 [==============================] - 0s 66us/step - loss: 1.9268 - acc: 0.4502 - val_loss: 1.7829 - val_acc: 0.4652\n",
      "Epoch 29/100\n",
      "462/462 [==============================] - 0s 57us/step - loss: 1.7838 - acc: 0.4719 - val_loss: 1.7376 - val_acc: 0.4579\n",
      "Epoch 30/100\n",
      "462/462 [==============================] - 0s 54us/step - loss: 1.7106 - acc: 0.4632 - val_loss: 1.9553 - val_acc: 0.5018\n",
      "Epoch 31/100\n",
      "462/462 [==============================] - 0s 64us/step - loss: 1.7420 - acc: 0.4892 - val_loss: 1.6693 - val_acc: 0.4799\n",
      "Epoch 32/100\n",
      "462/462 [==============================] - 0s 62us/step - loss: 1.6120 - acc: 0.4632 - val_loss: 1.7189 - val_acc: 0.5018\n",
      "Epoch 33/100\n",
      "462/462 [==============================] - 0s 65us/step - loss: 1.5848 - acc: 0.4784 - val_loss: 1.5803 - val_acc: 0.4579\n",
      "Epoch 34/100\n",
      "462/462 [==============================] - 0s 81us/step - loss: 1.5236 - acc: 0.4762 - val_loss: 1.5433 - val_acc: 0.4835\n",
      "Epoch 35/100\n",
      "462/462 [==============================] - 0s 83us/step - loss: 1.5360 - acc: 0.4654 - val_loss: 1.5315 - val_acc: 0.4762\n",
      "Epoch 36/100\n",
      "462/462 [==============================] - 0s 82us/step - loss: 1.4355 - acc: 0.4719 - val_loss: 1.5212 - val_acc: 0.4542\n",
      "Epoch 37/100\n",
      "462/462 [==============================] - 0s 109us/step - loss: 1.4425 - acc: 0.4697 - val_loss: 1.4933 - val_acc: 0.4762\n",
      "Epoch 38/100\n",
      "462/462 [==============================] - 0s 75us/step - loss: 1.4022 - acc: 0.4892 - val_loss: 1.4664 - val_acc: 0.4615\n",
      "Epoch 39/100\n",
      "462/462 [==============================] - 0s 73us/step - loss: 1.3994 - acc: 0.4827 - val_loss: 1.4809 - val_acc: 0.4579\n",
      "Epoch 40/100\n",
      "462/462 [==============================] - 0s 65us/step - loss: 1.4450 - acc: 0.4719 - val_loss: 1.4573 - val_acc: 0.4908\n",
      "Epoch 41/100\n",
      "462/462 [==============================] - 0s 72us/step - loss: 1.3344 - acc: 0.4848 - val_loss: 1.4846 - val_acc: 0.5055\n",
      "Epoch 42/100\n",
      "462/462 [==============================] - 0s 72us/step - loss: 1.3205 - acc: 0.4827 - val_loss: 1.4129 - val_acc: 0.4872\n",
      "Epoch 43/100\n",
      "462/462 [==============================] - 0s 75us/step - loss: 1.3085 - acc: 0.4935 - val_loss: 1.5018 - val_acc: 0.5092\n",
      "Epoch 44/100\n",
      "462/462 [==============================] - 0s 67us/step - loss: 1.3132 - acc: 0.4978 - val_loss: 1.4102 - val_acc: 0.4835\n",
      "Epoch 45/100\n",
      "462/462 [==============================] - 0s 53us/step - loss: 1.3234 - acc: 0.4848 - val_loss: 1.3946 - val_acc: 0.4835\n",
      "Epoch 46/100\n",
      "462/462 [==============================] - 0s 69us/step - loss: 1.2909 - acc: 0.4913 - val_loss: 1.3958 - val_acc: 0.4579\n",
      "Epoch 47/100\n",
      "462/462 [==============================] - 0s 58us/step - loss: 1.2661 - acc: 0.4827 - val_loss: 1.3764 - val_acc: 0.4725\n",
      "Epoch 48/100\n",
      "462/462 [==============================] - 0s 70us/step - loss: 1.2530 - acc: 0.5000 - val_loss: 1.3547 - val_acc: 0.4652\n",
      "Epoch 49/100\n",
      "462/462 [==============================] - 0s 67us/step - loss: 1.2925 - acc: 0.4827 - val_loss: 1.3946 - val_acc: 0.4799\n",
      "Epoch 50/100\n",
      "462/462 [==============================] - 0s 74us/step - loss: 1.2529 - acc: 0.4957 - val_loss: 1.4575 - val_acc: 0.4982\n",
      "Epoch 51/100\n",
      "462/462 [==============================] - 0s 80us/step - loss: 1.2108 - acc: 0.5108 - val_loss: 1.3588 - val_acc: 0.4652\n",
      "Epoch 52/100\n",
      "462/462 [==============================] - 0s 90us/step - loss: 1.2440 - acc: 0.5043 - val_loss: 1.3653 - val_acc: 0.4542\n",
      "Epoch 53/100\n",
      "462/462 [==============================] - 0s 64us/step - loss: 1.1883 - acc: 0.4848 - val_loss: 1.3776 - val_acc: 0.4615\n",
      "Epoch 54/100\n",
      "462/462 [==============================] - 0s 76us/step - loss: 1.2065 - acc: 0.4978 - val_loss: 1.4862 - val_acc: 0.5055\n",
      "Epoch 55/100\n",
      "462/462 [==============================] - 0s 71us/step - loss: 1.1956 - acc: 0.5130 - val_loss: 1.3641 - val_acc: 0.4762\n",
      "Epoch 56/100\n",
      "462/462 [==============================] - 0s 70us/step - loss: 1.1853 - acc: 0.5065 - val_loss: 1.3830 - val_acc: 0.4725\n",
      "Epoch 57/100\n",
      "462/462 [==============================] - 0s 60us/step - loss: 1.1697 - acc: 0.5000 - val_loss: 1.3585 - val_acc: 0.4689\n",
      "Epoch 58/100\n",
      "462/462 [==============================] - 0s 80us/step - loss: 1.1901 - acc: 0.4805 - val_loss: 1.4929 - val_acc: 0.5128\n",
      "Epoch 59/100\n",
      "462/462 [==============================] - 0s 77us/step - loss: 1.1319 - acc: 0.5346 - val_loss: 1.3805 - val_acc: 0.4579\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "462/462 [==============================] - 0s 77us/step - loss: 1.1484 - acc: 0.5000 - val_loss: 1.3669 - val_acc: 0.4579\n",
      "Epoch 61/100\n",
      "462/462 [==============================] - 0s 71us/step - loss: 1.1909 - acc: 0.5173 - val_loss: 1.3519 - val_acc: 0.4652\n",
      "Epoch 62/100\n",
      "462/462 [==============================] - 0s 51us/step - loss: 1.1212 - acc: 0.5087 - val_loss: 1.3484 - val_acc: 0.4872\n",
      "Epoch 63/100\n",
      "462/462 [==============================] - 0s 60us/step - loss: 1.1523 - acc: 0.5108 - val_loss: 1.3323 - val_acc: 0.4652\n",
      "Epoch 64/100\n",
      "462/462 [==============================] - 0s 52us/step - loss: 1.1195 - acc: 0.5087 - val_loss: 1.3310 - val_acc: 0.4872\n",
      "Epoch 65/100\n",
      "462/462 [==============================] - 0s 54us/step - loss: 1.1657 - acc: 0.4827 - val_loss: 1.3469 - val_acc: 0.4799\n",
      "Epoch 66/100\n",
      "462/462 [==============================] - 0s 46us/step - loss: 1.1171 - acc: 0.5108 - val_loss: 1.3354 - val_acc: 0.4542\n",
      "Epoch 67/100\n",
      "462/462 [==============================] - 0s 53us/step - loss: 1.1033 - acc: 0.4935 - val_loss: 1.3375 - val_acc: 0.4542\n",
      "Epoch 68/100\n",
      "462/462 [==============================] - ETA: 0s - loss: 1.0403 - acc: 0.492 - 0s 57us/step - loss: 1.1140 - acc: 0.5195 - val_loss: 1.3309 - val_acc: 0.4689\n",
      "Epoch 69/100\n",
      "462/462 [==============================] - 0s 55us/step - loss: 1.1210 - acc: 0.5022 - val_loss: 1.3284 - val_acc: 0.4579\n",
      "Epoch 70/100\n",
      "462/462 [==============================] - 0s 53us/step - loss: 1.1302 - acc: 0.4913 - val_loss: 1.3950 - val_acc: 0.4945\n",
      "Epoch 71/100\n",
      "462/462 [==============================] - 0s 59us/step - loss: 1.1032 - acc: 0.5130 - val_loss: 1.5059 - val_acc: 0.5092\n",
      "Epoch 72/100\n",
      "462/462 [==============================] - 0s 60us/step - loss: 1.1092 - acc: 0.5152 - val_loss: 1.3647 - val_acc: 0.4505\n",
      "Epoch 73/100\n",
      "462/462 [==============================] - 0s 46us/step - loss: 1.0924 - acc: 0.5043 - val_loss: 1.4052 - val_acc: 0.4762\n",
      "Epoch 74/100\n",
      "462/462 [==============================] - 0s 48us/step - loss: 1.1022 - acc: 0.5043 - val_loss: 1.3495 - val_acc: 0.4579\n",
      "Epoch 75/100\n",
      "462/462 [==============================] - 0s 57us/step - loss: 1.0693 - acc: 0.5087 - val_loss: 1.3527 - val_acc: 0.4615\n",
      "Epoch 76/100\n",
      "462/462 [==============================] - 0s 62us/step - loss: 1.0897 - acc: 0.5087 - val_loss: 1.3658 - val_acc: 0.4579\n",
      "Epoch 77/100\n",
      "462/462 [==============================] - 0s 48us/step - loss: 1.0700 - acc: 0.5043 - val_loss: 1.3355 - val_acc: 0.4615\n",
      "Epoch 78/100\n",
      "462/462 [==============================] - 0s 49us/step - loss: 1.0634 - acc: 0.5022 - val_loss: 1.4100 - val_acc: 0.5018\n",
      "Epoch 79/100\n",
      "462/462 [==============================] - 0s 56us/step - loss: 1.0734 - acc: 0.5238 - val_loss: 1.3553 - val_acc: 0.4652\n",
      "Epoch 80/100\n",
      "462/462 [==============================] - 0s 49us/step - loss: 1.0664 - acc: 0.5087 - val_loss: 1.5041 - val_acc: 0.4359\n",
      "Epoch 81/100\n",
      "462/462 [==============================] - 0s 53us/step - loss: 1.0852 - acc: 0.5000 - val_loss: 1.3384 - val_acc: 0.4652\n",
      "Epoch 82/100\n",
      "462/462 [==============================] - 0s 58us/step - loss: 1.0298 - acc: 0.5173 - val_loss: 1.5013 - val_acc: 0.5055\n",
      "Epoch 83/100\n",
      "462/462 [==============================] - 0s 57us/step - loss: 1.0975 - acc: 0.5173 - val_loss: 1.3677 - val_acc: 0.4835\n",
      "Epoch 84/100\n",
      "462/462 [==============================] - 0s 55us/step - loss: 1.0208 - acc: 0.5281 - val_loss: 1.4026 - val_acc: 0.4725\n",
      "Epoch 85/100\n",
      "462/462 [==============================] - 0s 53us/step - loss: 1.0327 - acc: 0.5022 - val_loss: 1.3701 - val_acc: 0.4762\n",
      "Epoch 86/100\n",
      "462/462 [==============================] - 0s 58us/step - loss: 1.0762 - acc: 0.5368 - val_loss: 1.3442 - val_acc: 0.4652\n",
      "Epoch 87/100\n",
      "462/462 [==============================] - 0s 48us/step - loss: 1.0121 - acc: 0.5043 - val_loss: 1.3617 - val_acc: 0.4542\n",
      "Epoch 88/100\n",
      "462/462 [==============================] - 0s 57us/step - loss: 1.0790 - acc: 0.5238 - val_loss: 1.3744 - val_acc: 0.4579\n",
      "Epoch 89/100\n",
      "462/462 [==============================] - 0s 59us/step - loss: 1.0489 - acc: 0.5152 - val_loss: 1.3556 - val_acc: 0.4505\n",
      "Epoch 90/100\n",
      "462/462 [==============================] - 0s 79us/step - loss: 1.0308 - acc: 0.5303 - val_loss: 1.3719 - val_acc: 0.4505\n",
      "Epoch 91/100\n",
      "462/462 [==============================] - 0s 90us/step - loss: 0.9952 - acc: 0.5173 - val_loss: 1.3820 - val_acc: 0.4799\n",
      "Epoch 92/100\n",
      "462/462 [==============================] - 0s 72us/step - loss: 0.9871 - acc: 0.5390 - val_loss: 1.4433 - val_acc: 0.4432\n",
      "Epoch 93/100\n",
      "462/462 [==============================] - 0s 81us/step - loss: 0.9922 - acc: 0.5368 - val_loss: 1.3794 - val_acc: 0.4542\n",
      "Epoch 94/100\n",
      "462/462 [==============================] - 0s 65us/step - loss: 0.9866 - acc: 0.5390 - val_loss: 1.3680 - val_acc: 0.4689\n",
      "Epoch 95/100\n",
      "462/462 [==============================] - 0s 51us/step - loss: 1.0032 - acc: 0.5238 - val_loss: 1.3745 - val_acc: 0.4542\n",
      "Epoch 96/100\n",
      "462/462 [==============================] - 0s 61us/step - loss: 1.0206 - acc: 0.5260 - val_loss: 1.3975 - val_acc: 0.4835\n",
      "Epoch 97/100\n",
      "462/462 [==============================] - 0s 48us/step - loss: 0.9903 - acc: 0.5303 - val_loss: 1.3706 - val_acc: 0.4359\n",
      "Epoch 98/100\n",
      "462/462 [==============================] - 0s 60us/step - loss: 0.9987 - acc: 0.5238 - val_loss: 1.3611 - val_acc: 0.4396\n",
      "Epoch 99/100\n",
      "462/462 [==============================] - 0s 57us/step - loss: 0.9779 - acc: 0.5303 - val_loss: 1.3532 - val_acc: 0.4469\n",
      "Epoch 100/100\n",
      "462/462 [==============================] - 0s 56us/step - loss: 1.0724 - acc: 0.5065 - val_loss: 1.3668 - val_acc: 0.4542\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a13e4f3c8>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newmodel_my.fit(X_train_my, Y_train_my,\n",
    "      epochs=100,\n",
    "      batch_size=128,\n",
    "      shuffle=True,\n",
    "      validation_data=(X_test_my, Y_test_my))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "273/273 [==============================] - 0s 980us/step\n",
      "1.36679310147\n",
      "273/273 [==============================] - 0s 64us/step\n",
      "[ 2.23419547  1.10440838  2.61137724  2.8235383   2.31036687  1.83054078\n",
      "  1.10440838  1.41245329  1.38969183  2.06786728  2.05968976  2.42504239\n",
      "  2.17143631  1.02276516  3.27699399  1.10440838  1.75576782  1.10440838\n",
      "  1.10440838  0.73615319  1.3554672   1.10440838  2.6145184   1.10440838\n",
      "  1.66607773  1.10440838  3.06424141  1.11397231  1.44997132  1.10440838\n",
      "  2.00190425  1.89415693  2.92790437  2.12809658  0.77909613  3.01442242\n",
      "  3.04443645  1.10440838  2.33922553  1.61002576  3.23282981  1.10440838\n",
      "  3.01414847  1.22622633  1.3628118   1.10440838  1.10440838  1.10440838\n",
      "  1.35385942  0.93926799  1.49215758  1.10440838  2.46725321  1.11786282\n",
      "  1.33499444  1.10440838  2.29512548  1.10440838  0.54624891  2.11787415\n",
      "  2.71258688  1.00634253  0.2616275   1.10440838  1.51819682  0.84649873\n",
      "  1.1144042   1.23218071  1.04152036  3.3000288   1.94888651  1.11968768\n",
      "  1.10440838  1.71158588  1.42579877  3.10663486  2.44609356  1.16299665\n",
      "  1.10440838  1.10440838  0.91923094  2.26098752  1.29935598  2.64873648\n",
      "  1.10440838  0.61426711  3.97784996  1.10440838  1.10440838  2.12504983\n",
      "  1.70695889  1.40201497  1.79233456  3.76504588  3.85006738  2.13317275\n",
      "  1.30205953  1.10440838  1.90171969  1.09814608  1.1668812   2.7318294\n",
      "  0.97044581  1.13051939  1.10440838  2.00655794  1.27421796  1.10440838\n",
      "  1.89959633  3.04443979  1.10440838  3.84304929  2.4878335   2.57813311\n",
      "  2.9447298   1.25995779  2.09811735  1.45579863  4.01092339  1.03621459\n",
      "  1.41003454  2.32977152  0.8730849   1.58380628  2.69753242  2.38924527\n",
      "  1.36279869  1.05054259  1.10440838  4.30929232  2.23775363  1.16752696\n",
      "  1.10440838  1.56942654  2.86139607  2.89809728  1.6545372   1.10440838\n",
      "  2.45254064  2.3127203   0.89128441  2.01634622  2.37595153  1.92511451\n",
      "  2.0481534   1.72949135  0.62853307  0.99230325  2.12918639  0.66434234\n",
      "  3.26709437  1.10440838  2.32995915  3.08274007  2.25664949  1.13818884\n",
      "  1.49944675  3.0842247   1.77301061  1.10440838  4.11925316  1.48979783\n",
      "  1.92308283  3.0365355   2.91317439  0.65267724  1.41003454  1.10440838\n",
      "  1.10440838  1.62304246  1.3006295   1.67474961  2.09828138  1.92446291\n",
      "  1.10440838  3.4102447   1.67815268  2.12708235  1.40338349  1.06567705\n",
      "  1.68812001  1.03621459  2.2156074   3.84080958  1.30749071  2.67455983\n",
      "  2.3796978   3.17849612  1.10440838  1.09996557  1.10440838  2.65249801\n",
      "  1.10440838  1.10440838  1.57701755  2.86022925  1.86230862  2.06834912\n",
      "  1.70173252  4.04001093  2.39162469  3.02523017  0.91963154  2.06887054\n",
      "  3.44006824  1.31303549  1.10440838  2.69253731  1.52617383  3.16217136\n",
      "  0.89474183  1.10440838  1.32200265  1.10440838  1.75443232  2.07522678\n",
      "  1.5325855   3.93371415  1.29102397  3.67442441  1.10440838  1.61933362\n",
      "  3.55647349  2.46081829  1.10440838  1.10440838  1.9465816   4.50716209\n",
      "  1.98689592  3.10468411  3.66921902  2.15134454  1.85656428  1.09791541\n",
      "  2.10046601  3.54369044  1.16889536  2.04249072  1.08132231  2.32668495\n",
      "  4.20936871  2.47397113  4.1010046   2.42717957  3.51372457  1.10440838\n",
      "  1.84838545  2.52845931  1.75861776  3.14399981  1.09791541  1.10440838\n",
      "  2.2669661   3.94402885  1.48252213  3.07692957  3.36415744  2.45660853\n",
      "  2.02576017  1.62356341  1.54600942  2.65044093  1.78785253  1.10440838\n",
      "  2.25891018  2.82795167  1.10440838  1.10440838  1.28100002  1.82807076\n",
      "  1.10440838  2.43664217  1.10440838]\n",
      "273/273 [==============================] - 0s 63us/step\n",
      "[ 2.  1.  4.  1.  1.  5.  1.  1.  2.  2.  1.  3.  5.  1.  1.  1.  4.  1.\n",
      "  1.  1.  3.  4.  3.  1.  2.  1.  1.  1.  3.  1.  2.  3.  4.  1.  1.  3.\n",
      "  2.  1.  1.  1.  2.  1.  5.  2.  3.  1.  1.  2.  1.  1.  1.  1.  3.  1.\n",
      "  1.  1.  2.  1.  3.  3.  2.  1.  5.  1.  1.  1.  1.  1.  4.  4.  1.  1.\n",
      "  1.  1.  1.  3.  1.  1.  1.  1.  1.  1.  1.  5.  1.  1.  5.  1.  1.  3.\n",
      "  4.  1.  4.  1.  1.  3.  1.  1.  1.  1.  1.  3.  3.  1.  1.  1.  1.  1.\n",
      "  2.  2.  1.  5.  2.  5.  1.  1.  5.  1.  3.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  5.  1.  1.  1.  1.  1.  1.  3.  1.  1.  4.  1.  3.  1.  2.\n",
      "  2.  1.  3.  1.  3.  1.  5.  1.  3.  1.  1.  2.  2.  3.  1.  1.  5.  1.\n",
      "  1.  2.  3.  1.  1.  1.  1.  1.  1.  2.  4.  1.  1.  1.  1.  1.  1.  1.\n",
      "  3.  1.  2.  3.  1.  1.  1.  1.  1.  2.  1.  1.  3.  1.  1.  2.  1.  3.\n",
      "  1.  4.  2.  1.  1.  1.  5.  1.  1.  1.  1.  1.  5.  1.  1.  1.  1.  1.\n",
      "  1.  4.  1.  5.  1.  1.  5.  5.  1.  1.  1.  5.  1.  5.  2.  1.  1.  1.\n",
      "  1.  5.  2.  1.  1.  1.  5.  1.  5.  1.  4.  4.  1.  4.  1.  5.  2.  1.\n",
      "  5.  4.  1.  4.  1.  1.  1.  1.  1.  2.  1.  2.  1.  2.  1.  1.  1.  1.\n",
      "  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "#print(newmodel_my.predict(X_test_my, verbose=1).flatten())\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mse = mean_squared_error(newmodel_my.predict(X_test_my, verbose=1).flatten(), Y_test_my)\n",
    "\n",
    "print(mse)\n",
    "\n",
    "print(newmodel_my.predict(X_test_my, verbose=1).flatten())\n",
    "\n",
    "#print('Rounded')\n",
    "#print(np.around(newmodel_my.predict(X_test_my, verbose=1).flatten(),decimals=0))\n",
    "\n",
    "scores_my = newmodel_my.evaluate(X_test_my, Y_test_my, verbose=1) \n",
    "print(y_test_my)\n",
    "\n",
    "#print(\"Accuracy: \", scores_my[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
