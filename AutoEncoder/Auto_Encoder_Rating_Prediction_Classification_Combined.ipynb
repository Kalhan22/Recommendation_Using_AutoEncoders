{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model \n",
    "from keras.layers import Input, Dense \n",
    "from keras.utils import np_utils \n",
    "import numpy as np\n",
    "from tensorflow.python.ops.variables import trainable_variables\n",
    "from numpy import genfromtxt\n",
    "\n",
    "X_train_my = genfromtxt('../UserData/655/trainX_combined_655.csv', delimiter=',')\n",
    "y_train_my = genfromtxt('../UserData/655/trainY_combined_655.csv', delimiter=',')\n",
    "X_test_my = genfromtxt('../UserData/655/testX_combined_655.csv', delimiter=',')\n",
    "y_test_my = genfromtxt('../UserData/655/testY_combined_655.csv', delimiter=',')\n",
    "\n",
    "num_classes_my = 6 # there are 5 classes (1 per rating class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(406, 39)\n"
     ]
    }
   ],
   "source": [
    "X_train_my = X_train_my.astype('float32') \n",
    "X_train_my = X_train_my.astype('float32')\n",
    "print(X_train_my.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Y_train_my = np_utils.to_categorical(y_train_my, num_classes_my) # One-hot encode the labels\n",
    "\n",
    "Y_test_my = np_utils.to_categorical(y_test_my, num_classes_my) # One-hot encode the labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"input_3:0\", shape=(?, 39), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "input_img_my = Input(shape=(39,))\n",
    "\n",
    "print(input_img_my)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"dense_17/Tanh:0\", shape=(?, 39), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x_my = Dense(39, activation='tanh')(input_img_my)\n",
    "print(x_my)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoded1_my = Dense(15, activation='tanh')(x_my)\n",
    "encoded2_my = Dense(12, activation='tanh')(encoded1_my)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_my = Dense(10, activation='tanh')(encoded2_my)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoded2_my = Dense(12, activation='tanh')(y_my)\n",
    "decoded1_my = Dense(15, activation='tanh')(decoded2_my)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z_my = Dense(39, activation='tanh')(decoded1_my)\n",
    "autoencoder_my = Model(input_img_my, z_my)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.training.Model object at 0x1a2011e390>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#encoder is the model of the autoencoder slice in the middle \n",
    "encoder_my = Model(input_img_my, y_my)\n",
    "\n",
    "print(encoder_my)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "autoencoder_my.compile(optimizer='adadelta', loss='mse') # reporting the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(167, 39)\n",
      "(406, 39)\n"
     ]
    }
   ],
   "source": [
    "print(X_test_my.shape)\n",
    "\n",
    "print(X_train_my.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 406 samples, validate on 167 samples\n",
      "Epoch 1/50\n",
      "406/406 [==============================] - 1s 2ms/step - loss: 3.2255 - val_loss: 3.3113\n",
      "Epoch 2/50\n",
      "406/406 [==============================] - 0s 686us/step - loss: 2.8978 - val_loss: 2.9098\n",
      "Epoch 3/50\n",
      "406/406 [==============================] - 0s 679us/step - loss: 2.5392 - val_loss: 2.6087\n",
      "Epoch 4/50\n",
      "406/406 [==============================] - 0s 656us/step - loss: 2.3368 - val_loss: 2.4700\n",
      "Epoch 5/50\n",
      "406/406 [==============================] - 0s 649us/step - loss: 2.2459 - val_loss: 2.4038\n",
      "Epoch 6/50\n",
      "406/406 [==============================] - 0s 632us/step - loss: 2.1984 - val_loss: 2.3613\n",
      "Epoch 7/50\n",
      "406/406 [==============================] - 0s 653us/step - loss: 2.1546 - val_loss: 2.3174\n",
      "Epoch 8/50\n",
      "406/406 [==============================] - 0s 648us/step - loss: 2.1292 - val_loss: 2.2976\n",
      "Epoch 9/50\n",
      "406/406 [==============================] - 0s 617us/step - loss: 2.1111 - val_loss: 2.2825\n",
      "Epoch 10/50\n",
      "406/406 [==============================] - 0s 637us/step - loss: 2.0971 - val_loss: 2.2689\n",
      "Epoch 11/50\n",
      "406/406 [==============================] - 0s 626us/step - loss: 2.0849 - val_loss: 2.2583\n",
      "Epoch 12/50\n",
      "406/406 [==============================] - 0s 631us/step - loss: 2.0750 - val_loss: 2.2493\n",
      "Epoch 13/50\n",
      "406/406 [==============================] - 0s 609us/step - loss: 2.0658 - val_loss: 2.2405\n",
      "Epoch 14/50\n",
      "406/406 [==============================] - 0s 597us/step - loss: 2.0584 - val_loss: 2.2332\n",
      "Epoch 15/50\n",
      "406/406 [==============================] - 0s 623us/step - loss: 2.0518 - val_loss: 2.2273\n",
      "Epoch 16/50\n",
      "406/406 [==============================] - 0s 625us/step - loss: 2.0465 - val_loss: 2.2227\n",
      "Epoch 17/50\n",
      "406/406 [==============================] - 0s 622us/step - loss: 2.0421 - val_loss: 2.2182\n",
      "Epoch 18/50\n",
      "406/406 [==============================] - 0s 615us/step - loss: 2.0382 - val_loss: 2.2153\n",
      "Epoch 19/50\n",
      "406/406 [==============================] - 0s 633us/step - loss: 2.0350 - val_loss: 2.2123\n",
      "Epoch 20/50\n",
      "406/406 [==============================] - 0s 627us/step - loss: 2.0324 - val_loss: 2.2094\n",
      "Epoch 21/50\n",
      "406/406 [==============================] - 0s 620us/step - loss: 2.0300 - val_loss: 2.2075\n",
      "Epoch 22/50\n",
      "406/406 [==============================] - 0s 598us/step - loss: 2.0278 - val_loss: 2.2056\n",
      "Epoch 23/50\n",
      "406/406 [==============================] - 0s 616us/step - loss: 2.0260 - val_loss: 2.2037\n",
      "Epoch 24/50\n",
      "406/406 [==============================] - 0s 608us/step - loss: 2.0243 - val_loss: 2.2022\n",
      "Epoch 25/50\n",
      "406/406 [==============================] - 0s 598us/step - loss: 2.0228 - val_loss: 2.2008\n",
      "Epoch 26/50\n",
      "406/406 [==============================] - 0s 597us/step - loss: 2.0214 - val_loss: 2.1995\n",
      "Epoch 27/50\n",
      "406/406 [==============================] - 0s 619us/step - loss: 2.0201 - val_loss: 2.1995\n",
      "Epoch 28/50\n",
      "406/406 [==============================] - 0s 621us/step - loss: 2.0189 - val_loss: 2.1972\n",
      "Epoch 29/50\n",
      "406/406 [==============================] - 0s 609us/step - loss: 2.0176 - val_loss: 2.1962\n",
      "Epoch 30/50\n",
      "406/406 [==============================] - 0s 591us/step - loss: 2.0166 - val_loss: 2.1953\n",
      "Epoch 31/50\n",
      "406/406 [==============================] - 0s 588us/step - loss: 2.0156 - val_loss: 2.1944\n",
      "Epoch 32/50\n",
      "406/406 [==============================] - 0s 625us/step - loss: 2.0145 - val_loss: 2.1936\n",
      "Epoch 33/50\n",
      "406/406 [==============================] - 0s 608us/step - loss: 2.0135 - val_loss: 2.1925\n",
      "Epoch 34/50\n",
      "406/406 [==============================] - 0s 637us/step - loss: 2.0125 - val_loss: 2.1923\n",
      "Epoch 35/50\n",
      "406/406 [==============================] - 0s 598us/step - loss: 2.0115 - val_loss: 2.1908\n",
      "Epoch 36/50\n",
      "406/406 [==============================] - 0s 616us/step - loss: 2.0106 - val_loss: 2.1903\n",
      "Epoch 37/50\n",
      "406/406 [==============================] - 0s 611us/step - loss: 2.0096 - val_loss: 2.1895\n",
      "Epoch 38/50\n",
      "406/406 [==============================] - 0s 599us/step - loss: 2.0088 - val_loss: 2.1885\n",
      "Epoch 39/50\n",
      "406/406 [==============================] - 0s 622us/step - loss: 2.0078 - val_loss: 2.1878\n",
      "Epoch 40/50\n",
      "406/406 [==============================] - 0s 615us/step - loss: 2.0070 - val_loss: 2.1873\n",
      "Epoch 41/50\n",
      "406/406 [==============================] - 0s 591us/step - loss: 2.0060 - val_loss: 2.1862\n",
      "Epoch 42/50\n",
      "406/406 [==============================] - 0s 570us/step - loss: 2.0051 - val_loss: 2.1857\n",
      "Epoch 43/50\n",
      "406/406 [==============================] - 0s 616us/step - loss: 2.0041 - val_loss: 2.1846\n",
      "Epoch 44/50\n",
      "406/406 [==============================] - 0s 603us/step - loss: 2.0034 - val_loss: 2.1838\n",
      "Epoch 45/50\n",
      "406/406 [==============================] - 0s 610us/step - loss: 2.0023 - val_loss: 2.1831\n",
      "Epoch 46/50\n",
      "406/406 [==============================] - 0s 593us/step - loss: 2.0014 - val_loss: 2.1820\n",
      "Epoch 47/50\n",
      "406/406 [==============================] - 0s 597us/step - loss: 2.0005 - val_loss: 2.1812\n",
      "Epoch 48/50\n",
      "406/406 [==============================] - 0s 622us/step - loss: 1.9996 - val_loss: 2.1804\n",
      "Epoch 49/50\n",
      "406/406 [==============================] - 0s 628us/step - loss: 1.9987 - val_loss: 2.1802\n",
      "Epoch 50/50\n",
      "406/406 [==============================] - 0s 615us/step - loss: 1.9979 - val_loss: 2.1792\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a2019cc50>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder_my.fit(X_train_my, X_train_my,\n",
    "      epochs=50,\n",
    "      batch_size=10,\n",
    "      shuffle=True,\n",
    "      validation_data=(X_test_my, X_test_my)\n",
    "      )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if you want an encoded flatten representation of every test MNIST\n",
    "reduced_representation_my =encoder_my.predict(X_test_my)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 406 samples, validate on 167 samples\n",
      "Epoch 1/50\n",
      "406/406 [==============================] - 1s 2ms/step - loss: 1.9970 - val_loss: 2.1782\n",
      "Epoch 2/50\n",
      "406/406 [==============================] - 0s 610us/step - loss: 1.9958 - val_loss: 2.1770\n",
      "Epoch 3/50\n",
      "406/406 [==============================] - 0s 625us/step - loss: 1.9946 - val_loss: 2.1765\n",
      "Epoch 4/50\n",
      "406/406 [==============================] - 0s 621us/step - loss: 1.9935 - val_loss: 2.1756\n",
      "Epoch 5/50\n",
      "406/406 [==============================] - 0s 646us/step - loss: 1.9926 - val_loss: 2.1748\n",
      "Epoch 6/50\n",
      "406/406 [==============================] - 0s 624us/step - loss: 1.9917 - val_loss: 2.1741\n",
      "Epoch 7/50\n",
      "406/406 [==============================] - 0s 659us/step - loss: 1.9906 - val_loss: 2.1734\n",
      "Epoch 8/50\n",
      "406/406 [==============================] - 0s 632us/step - loss: 1.9897 - val_loss: 2.1731\n",
      "Epoch 9/50\n",
      "406/406 [==============================] - 0s 634us/step - loss: 1.9889 - val_loss: 2.1725\n",
      "Epoch 10/50\n",
      "406/406 [==============================] - 0s 643us/step - loss: 1.9880 - val_loss: 2.1719\n",
      "Epoch 11/50\n",
      "406/406 [==============================] - 0s 620us/step - loss: 1.9873 - val_loss: 2.1713\n",
      "Epoch 12/50\n",
      "406/406 [==============================] - 0s 660us/step - loss: 1.9864 - val_loss: 2.1707\n",
      "Epoch 13/50\n",
      "406/406 [==============================] - 0s 628us/step - loss: 1.9857 - val_loss: 2.1702\n",
      "Epoch 14/50\n",
      "406/406 [==============================] - 0s 675us/step - loss: 1.9851 - val_loss: 2.1699\n",
      "Epoch 15/50\n",
      "406/406 [==============================] - 0s 687us/step - loss: 1.9844 - val_loss: 2.1692\n",
      "Epoch 16/50\n",
      "406/406 [==============================] - 0s 598us/step - loss: 1.9836 - val_loss: 2.1686\n",
      "Epoch 17/50\n",
      "406/406 [==============================] - 0s 653us/step - loss: 1.9830 - val_loss: 2.1688\n",
      "Epoch 18/50\n",
      "406/406 [==============================] - 0s 664us/step - loss: 1.9824 - val_loss: 2.1675\n",
      "Epoch 19/50\n",
      "406/406 [==============================] - 0s 741us/step - loss: 1.9816 - val_loss: 2.1674\n",
      "Epoch 20/50\n",
      "406/406 [==============================] - 0s 665us/step - loss: 1.9810 - val_loss: 2.1664\n",
      "Epoch 21/50\n",
      "406/406 [==============================] - 0s 638us/step - loss: 1.9804 - val_loss: 2.1662\n",
      "Epoch 22/50\n",
      "406/406 [==============================] - 0s 632us/step - loss: 1.9798 - val_loss: 2.1659\n",
      "Epoch 23/50\n",
      "406/406 [==============================] - 0s 635us/step - loss: 1.9793 - val_loss: 2.1652\n",
      "Epoch 24/50\n",
      "406/406 [==============================] - 0s 643us/step - loss: 1.9787 - val_loss: 2.1647\n",
      "Epoch 25/50\n",
      "406/406 [==============================] - 0s 621us/step - loss: 1.9782 - val_loss: 2.1643\n",
      "Epoch 26/50\n",
      "406/406 [==============================] - 0s 631us/step - loss: 1.9777 - val_loss: 2.1642\n",
      "Epoch 27/50\n",
      "406/406 [==============================] - 0s 645us/step - loss: 1.9771 - val_loss: 2.1634\n",
      "Epoch 28/50\n",
      "406/406 [==============================] - 0s 649us/step - loss: 1.9766 - val_loss: 2.1632\n",
      "Epoch 29/50\n",
      "406/406 [==============================] - 0s 646us/step - loss: 1.9761 - val_loss: 2.1629\n",
      "Epoch 30/50\n",
      "406/406 [==============================] - 0s 642us/step - loss: 1.9756 - val_loss: 2.1625\n",
      "Epoch 31/50\n",
      "406/406 [==============================] - 0s 605us/step - loss: 1.9751 - val_loss: 2.1625\n",
      "Epoch 32/50\n",
      "406/406 [==============================] - 0s 620us/step - loss: 1.9746 - val_loss: 2.1619\n",
      "Epoch 33/50\n",
      "406/406 [==============================] - 0s 652us/step - loss: 1.9742 - val_loss: 2.1614\n",
      "Epoch 34/50\n",
      "406/406 [==============================] - 0s 627us/step - loss: 1.9736 - val_loss: 2.1615\n",
      "Epoch 35/50\n",
      "406/406 [==============================] - 0s 643us/step - loss: 1.9732 - val_loss: 2.1612\n",
      "Epoch 36/50\n",
      "406/406 [==============================] - 0s 622us/step - loss: 1.9728 - val_loss: 2.1608\n",
      "Epoch 37/50\n",
      "406/406 [==============================] - 0s 636us/step - loss: 1.9723 - val_loss: 2.1606\n",
      "Epoch 38/50\n",
      "406/406 [==============================] - 0s 615us/step - loss: 1.9718 - val_loss: 2.1603\n",
      "Epoch 39/50\n",
      "406/406 [==============================] - 0s 653us/step - loss: 1.9714 - val_loss: 2.1605\n",
      "Epoch 40/50\n",
      "406/406 [==============================] - 0s 640us/step - loss: 1.9710 - val_loss: 2.1601\n",
      "Epoch 41/50\n",
      "406/406 [==============================] - 0s 626us/step - loss: 1.9705 - val_loss: 2.1597\n",
      "Epoch 42/50\n",
      "406/406 [==============================] - 0s 656us/step - loss: 1.9701 - val_loss: 2.1595\n",
      "Epoch 43/50\n",
      "406/406 [==============================] - 0s 633us/step - loss: 1.9698 - val_loss: 2.1598\n",
      "Epoch 44/50\n",
      "406/406 [==============================] - 0s 642us/step - loss: 1.9693 - val_loss: 2.1591\n",
      "Epoch 45/50\n",
      "406/406 [==============================] - 0s 651us/step - loss: 1.9689 - val_loss: 2.1591\n",
      "Epoch 46/50\n",
      "406/406 [==============================] - 0s 622us/step - loss: 1.9685 - val_loss: 2.1586\n",
      "Epoch 47/50\n",
      "406/406 [==============================] - 0s 636us/step - loss: 1.9680 - val_loss: 2.1583\n",
      "Epoch 48/50\n",
      "406/406 [==============================] - 0s 641us/step - loss: 1.9677 - val_loss: 2.1582\n",
      "Epoch 49/50\n",
      "406/406 [==============================] - 0s 603us/step - loss: 1.9673 - val_loss: 2.1578\n",
      "Epoch 50/50\n",
      "406/406 [==============================] - 0s 633us/step - loss: 1.9670 - val_loss: 2.1574\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a206e22e8>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#encoder is the model of the autoencoder slice in the middle \n",
    "encoder_my = Model(input_img_my, y_my)\n",
    "\n",
    "autoencoder_my.compile(optimizer='adadelta', loss='mse') # reporting the loss\n",
    "\n",
    "autoencoder_my.fit(X_train_my, X_train_my,\n",
    "      epochs=50,\n",
    "      batch_size=10,\n",
    "      shuffle=True,\n",
    "      validation_data=(X_test_my, X_test_my))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if you want an encoded flatten representation of every test MNIST\n",
    "reduced_representation_my =encoder_my.predict(X_test_my)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out2_my = Dense(6, activation='softmax')(encoder_my.output)\n",
    "newmodel_my = Model(encoder_my.input,out2_my)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newmodel_my.compile(loss='categorical_crossentropy',\n",
    "          optimizer='adam', \n",
    "          metrics=['accuracy']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 406 samples, validate on 167 samples\n",
      "Epoch 1/50\n",
      "406/406 [==============================] - 0s 1ms/step - loss: 2.0252 - acc: 0.1133 - val_loss: 1.8479 - val_acc: 0.2455\n",
      "Epoch 2/50\n",
      "406/406 [==============================] - 0s 57us/step - loss: 1.8792 - acc: 0.1576 - val_loss: 1.7342 - val_acc: 0.2695\n",
      "Epoch 3/50\n",
      "406/406 [==============================] - 0s 58us/step - loss: 1.7435 - acc: 0.2685 - val_loss: 1.6296 - val_acc: 0.3832\n",
      "Epoch 4/50\n",
      "406/406 [==============================] - 0s 58us/step - loss: 1.6358 - acc: 0.3892 - val_loss: 1.5492 - val_acc: 0.4731\n",
      "Epoch 5/50\n",
      "406/406 [==============================] - 0s 60us/step - loss: 1.5536 - acc: 0.4803 - val_loss: 1.4936 - val_acc: 0.5210\n",
      "Epoch 6/50\n",
      "406/406 [==============================] - 0s 55us/step - loss: 1.4950 - acc: 0.5074 - val_loss: 1.4530 - val_acc: 0.5329\n",
      "Epoch 7/50\n",
      "406/406 [==============================] - 0s 60us/step - loss: 1.4507 - acc: 0.5222 - val_loss: 1.4215 - val_acc: 0.5449\n",
      "Epoch 8/50\n",
      "406/406 [==============================] - 0s 57us/step - loss: 1.4159 - acc: 0.5345 - val_loss: 1.3955 - val_acc: 0.5509\n",
      "Epoch 9/50\n",
      "406/406 [==============================] - 0s 59us/step - loss: 1.3826 - acc: 0.5419 - val_loss: 1.3732 - val_acc: 0.5509\n",
      "Epoch 10/50\n",
      "406/406 [==============================] - 0s 62us/step - loss: 1.3533 - acc: 0.5443 - val_loss: 1.3548 - val_acc: 0.5509\n",
      "Epoch 11/50\n",
      "406/406 [==============================] - 0s 61us/step - loss: 1.3269 - acc: 0.5443 - val_loss: 1.3397 - val_acc: 0.5509\n",
      "Epoch 12/50\n",
      "406/406 [==============================] - 0s 58us/step - loss: 1.3048 - acc: 0.5468 - val_loss: 1.3279 - val_acc: 0.5509\n",
      "Epoch 13/50\n",
      "406/406 [==============================] - 0s 58us/step - loss: 1.2856 - acc: 0.5493 - val_loss: 1.3167 - val_acc: 0.5509\n",
      "Epoch 14/50\n",
      "406/406 [==============================] - 0s 62us/step - loss: 1.2676 - acc: 0.5468 - val_loss: 1.3042 - val_acc: 0.5569\n",
      "Epoch 15/50\n",
      "406/406 [==============================] - 0s 54us/step - loss: 1.2501 - acc: 0.5493 - val_loss: 1.2927 - val_acc: 0.5569\n",
      "Epoch 16/50\n",
      "406/406 [==============================] - 0s 59us/step - loss: 1.2333 - acc: 0.5517 - val_loss: 1.2839 - val_acc: 0.5569\n",
      "Epoch 17/50\n",
      "406/406 [==============================] - 0s 64us/step - loss: 1.2177 - acc: 0.5517 - val_loss: 1.2745 - val_acc: 0.5569\n",
      "Epoch 18/50\n",
      "406/406 [==============================] - 0s 64us/step - loss: 1.2043 - acc: 0.5517 - val_loss: 1.2665 - val_acc: 0.5569\n",
      "Epoch 19/50\n",
      "406/406 [==============================] - 0s 57us/step - loss: 1.1915 - acc: 0.5517 - val_loss: 1.2603 - val_acc: 0.5569\n",
      "Epoch 20/50\n",
      "406/406 [==============================] - 0s 59us/step - loss: 1.1798 - acc: 0.5542 - val_loss: 1.2549 - val_acc: 0.5569\n",
      "Epoch 21/50\n",
      "406/406 [==============================] - 0s 61us/step - loss: 1.1680 - acc: 0.5640 - val_loss: 1.2519 - val_acc: 0.5509\n",
      "Epoch 22/50\n",
      "406/406 [==============================] - 0s 58us/step - loss: 1.1575 - acc: 0.5690 - val_loss: 1.2485 - val_acc: 0.5389\n",
      "Epoch 23/50\n",
      "406/406 [==============================] - 0s 60us/step - loss: 1.1480 - acc: 0.5788 - val_loss: 1.2457 - val_acc: 0.5509\n",
      "Epoch 24/50\n",
      "406/406 [==============================] - 0s 62us/step - loss: 1.1383 - acc: 0.5837 - val_loss: 1.2432 - val_acc: 0.5329\n",
      "Epoch 25/50\n",
      "406/406 [==============================] - 0s 59us/step - loss: 1.1301 - acc: 0.5911 - val_loss: 1.2423 - val_acc: 0.5269\n",
      "Epoch 26/50\n",
      "406/406 [==============================] - 0s 57us/step - loss: 1.1206 - acc: 0.5985 - val_loss: 1.2391 - val_acc: 0.5269\n",
      "Epoch 27/50\n",
      "406/406 [==============================] - 0s 64us/step - loss: 1.1116 - acc: 0.6034 - val_loss: 1.2371 - val_acc: 0.5269\n",
      "Epoch 28/50\n",
      "406/406 [==============================] - 0s 61us/step - loss: 1.1033 - acc: 0.5911 - val_loss: 1.2362 - val_acc: 0.5150\n",
      "Epoch 29/50\n",
      "406/406 [==============================] - 0s 65us/step - loss: 1.0951 - acc: 0.6010 - val_loss: 1.2360 - val_acc: 0.5090\n",
      "Epoch 30/50\n",
      "406/406 [==============================] - 0s 63us/step - loss: 1.0876 - acc: 0.6084 - val_loss: 1.2367 - val_acc: 0.4910\n",
      "Epoch 31/50\n",
      "406/406 [==============================] - 0s 60us/step - loss: 1.0810 - acc: 0.6158 - val_loss: 1.2378 - val_acc: 0.4850\n",
      "Epoch 32/50\n",
      "406/406 [==============================] - 0s 59us/step - loss: 1.0727 - acc: 0.6232 - val_loss: 1.2368 - val_acc: 0.4850\n",
      "Epoch 33/50\n",
      "406/406 [==============================] - 0s 59us/step - loss: 1.0642 - acc: 0.6182 - val_loss: 1.2346 - val_acc: 0.5150\n",
      "Epoch 34/50\n",
      "406/406 [==============================] - 0s 62us/step - loss: 1.0592 - acc: 0.6133 - val_loss: 1.2340 - val_acc: 0.5210\n",
      "Epoch 35/50\n",
      "406/406 [==============================] - 0s 63us/step - loss: 1.0529 - acc: 0.6034 - val_loss: 1.2361 - val_acc: 0.5210\n",
      "Epoch 36/50\n",
      "406/406 [==============================] - 0s 59us/step - loss: 1.0442 - acc: 0.6158 - val_loss: 1.2399 - val_acc: 0.4671\n",
      "Epoch 37/50\n",
      "406/406 [==============================] - 0s 58us/step - loss: 1.0392 - acc: 0.6256 - val_loss: 1.2442 - val_acc: 0.4611\n",
      "Epoch 38/50\n",
      "406/406 [==============================] - 0s 60us/step - loss: 1.0331 - acc: 0.6429 - val_loss: 1.2403 - val_acc: 0.4611\n",
      "Epoch 39/50\n",
      "406/406 [==============================] - 0s 59us/step - loss: 1.0241 - acc: 0.6379 - val_loss: 1.2368 - val_acc: 0.4850\n",
      "Epoch 40/50\n",
      "406/406 [==============================] - 0s 55us/step - loss: 1.0160 - acc: 0.6453 - val_loss: 1.2364 - val_acc: 0.4850\n",
      "Epoch 41/50\n",
      "406/406 [==============================] - 0s 60us/step - loss: 1.0113 - acc: 0.6379 - val_loss: 1.2369 - val_acc: 0.4910\n",
      "Epoch 42/50\n",
      "406/406 [==============================] - 0s 54us/step - loss: 1.0031 - acc: 0.6404 - val_loss: 1.2402 - val_acc: 0.4671\n",
      "Epoch 43/50\n",
      "406/406 [==============================] - 0s 59us/step - loss: 0.9963 - acc: 0.6478 - val_loss: 1.2482 - val_acc: 0.4551\n",
      "Epoch 44/50\n",
      "406/406 [==============================] - 0s 61us/step - loss: 0.9915 - acc: 0.6379 - val_loss: 1.2530 - val_acc: 0.4551\n",
      "Epoch 45/50\n",
      "406/406 [==============================] - 0s 59us/step - loss: 0.9861 - acc: 0.6355 - val_loss: 1.2522 - val_acc: 0.4491\n",
      "Epoch 46/50\n",
      "406/406 [==============================] - 0s 72us/step - loss: 0.9755 - acc: 0.6379 - val_loss: 1.2476 - val_acc: 0.4850\n",
      "Epoch 47/50\n",
      "406/406 [==============================] - 0s 64us/step - loss: 0.9725 - acc: 0.6502 - val_loss: 1.2511 - val_acc: 0.4850\n",
      "Epoch 48/50\n",
      "406/406 [==============================] - 0s 56us/step - loss: 0.9643 - acc: 0.6502 - val_loss: 1.2590 - val_acc: 0.4671\n",
      "Epoch 49/50\n",
      "406/406 [==============================] - 0s 60us/step - loss: 0.9593 - acc: 0.6305 - val_loss: 1.2657 - val_acc: 0.4431\n",
      "Epoch 50/50\n",
      "406/406 [==============================] - 0s 54us/step - loss: 0.9507 - acc: 0.6355 - val_loss: 1.2644 - val_acc: 0.4551\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a1fef9240>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newmodel_my.fit(X_train_my, Y_train_my,\n",
    "      epochs=50,\n",
    "      batch_size=128,\n",
    "      shuffle=True,\n",
    "      validation_data=(X_test_my, Y_test_my))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167/167 [==============================] - 0s 1ms/step\n",
      "[[ 0.01789232  0.02763401  0.23514915  0.58026677  0.0987449   0.04031285]\n",
      " [ 0.01597838  0.02719324  0.48510382  0.27046201  0.1493932   0.0518694 ]\n",
      " [ 0.02160118  0.03498564  0.26265794  0.53373241  0.10391439  0.04310842]\n",
      " ..., \n",
      " [ 0.04211319  0.02582451  0.07069409  0.79870075  0.0453283   0.01733916]\n",
      " [ 0.02498754  0.02969855  0.0961271   0.76919228  0.05684377  0.02315088]\n",
      " [ 0.016823    0.02854998  0.49342442  0.25319141  0.15034719  0.05766406]]\n",
      "167/167 [==============================] - 0s 70us/step\n",
      "[3 2 3 3 3 2 3 2 3 3 2 3 3 3 2 3 2 3 4 3 3 3 3 3 3 3 3 3 3 2 3 3 3 2 3 3 3\n",
      " 2 2 3 3 3 3 3 2 3 3 3 2 3 3 3 3 3 3 2 3 3 3 3 2 2 3 3 2 2 2 3 3 2 3 2 3 2\n",
      " 3 2 2 2 2 3 3 2 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 2 3 2 3 2 3 3 3 2 3 3 3\n",
      " 3 3 3 3 3 3 3 3 2 3 2 2 3 3 3 3 3 2 3 3 3 2 2 2 3 3 3 3 3 2 3 3 3 3 3 2 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 2 3 3 3 3 3 2]\n",
      "167/167 [==============================] - 0s 96us/step\n",
      "[ 3.  3.  2.  3.  3.  3.  5.  3.  3.  2.  3.  3.  3.  4.  3.  2.  1.  3.\n",
      "  3.  4.  4.  3.  5.  3.  3.  3.  4.  3.  3.  3.  2.  2.  4.  1.  3.  3.\n",
      "  3.  3.  4.  5.  3.  4.  3.  3.  4.  3.  2.  3.  2.  4.  4.  3.  4.  2.\n",
      "  3.  3.  3.  4.  4.  3.  3.  2.  3.  3.  4.  3.  3.  3.  4.  4.  2.  3.\n",
      "  3.  3.  3.  2.  4.  4.  3.  3.  3.  3.  2.  4.  2.  3.  3.  3.  2.  2.\n",
      "  2.  2.  2.  5.  4.  2.  5.  2.  4.  2.  3.  1.  2.  3.  3.  2.  2.  2.\n",
      "  4.  3.  3.  3.  3.  2.  3.  4.  2.  2.  2.  4.  3.  3.  3.  4.  3.  3.\n",
      "  2.  2.  3.  3.  4.  3.  4.  3.  4.  3.  3.  3.  3.  4.  2.  3.  3.  3.\n",
      "  3.  2.  2.  3.  4.  3.  3.  3.  2.  3.  4.  3.  3.  3.  2.  3.  3.  3.\n",
      "  3.  3.  3.  3.  2.]\n",
      "Accuracy:  0.455089820716\n",
      "167/167 [==============================] - 0s 91us/step\n",
      "[[ 0  3  0  0  0]\n",
      " [ 0  8 29  0  0]\n",
      " [ 0 22 68  1  0]\n",
      " [ 0  9 22  0  0]\n",
      " [ 0  0  5  0  0]]\n"
     ]
    }
   ],
   "source": [
    "print(newmodel_my.predict(X_test_my, verbose=1))\n",
    "print(np.argmax(newmodel_my.predict(X_test_my, verbose=1),axis=1))\n",
    "\n",
    "scores_my = newmodel_my.evaluate(X_test_my, Y_test_my, verbose=1) \n",
    "print(y_test_my)\n",
    "print(\"Accuracy: \", scores_my[1])\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cfm = confusion_matrix(np.array(y_test_my),np.array(np.argmax(newmodel_my.predict(X_test_my, verbose=1),axis=1)))\n",
    "print(cfm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
