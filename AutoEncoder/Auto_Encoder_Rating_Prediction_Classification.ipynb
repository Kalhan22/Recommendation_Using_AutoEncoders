{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model \n",
    "from keras.layers import Input, Dense \n",
    "from keras.utils import np_utils \n",
    "import numpy as np\n",
    "from tensorflow.python.ops.variables import trainable_variables\n",
    "from numpy import genfromtxt\n",
    "\n",
    "X_train_my = genfromtxt('../UserData/655/trainX_655.csv', delimiter=',')\n",
    "y_train_my = genfromtxt('../UserData/655/trainY_655.csv', delimiter=',')\n",
    "X_test_my = genfromtxt('../UserData/655/testX_655.csv', delimiter=',')\n",
    "y_test_my = genfromtxt('../UserData/655/testY_655.csv', delimiter=',')\n",
    "\n",
    "num_classes_my = 6 # there are 5 classes (1 per rating class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(483, 20)\n"
     ]
    }
   ],
   "source": [
    "X_train_my = X_train_my.astype('float32') \n",
    "X_train_my = X_train_my.astype('float32')\n",
    "print(X_train_my.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Y_train_my = np_utils.to_categorical(y_train_my, num_classes_my) # One-hot encode the labels\n",
    "\n",
    "Y_test_my = np_utils.to_categorical(y_test_my, num_classes_my) # One-hot encode the labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"input_10:0\", shape=(?, 20), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "input_img_my = Input(shape=(20,))\n",
    "\n",
    "print(input_img_my)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"dense_63/Sigmoid:0\", shape=(?, 20), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x_my = Dense(20, activation='sigmoid')(input_img_my)\n",
    "print(x_my)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoded1_my = Dense(15, activation='sigmoid')(x_my)\n",
    "encoded2_my = Dense(12, activation='sigmoid')(encoded1_my)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_my = Dense(10, activation='sigmoid')(encoded2_my)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoded2_my = Dense(12, activation='sigmoid')(y_my)\n",
    "decoded1_my = Dense(15, activation='sigmoid')(decoded2_my)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z_my = Dense(20, activation='sigmoid')(decoded1_my)\n",
    "autoencoder_my = Model(input_img_my, z_my)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.training.Model object at 0x1a1fbda5f8>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#encoder is the model of the autoencoder slice in the middle \n",
    "encoder_my = Model(input_img_my, y_my)\n",
    "\n",
    "print(encoder_my)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "autoencoder_my.compile(optimizer='adadelta', loss='mse') # reporting the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 20)\n",
      "(483, 20)\n"
     ]
    }
   ],
   "source": [
    "print(X_test_my.shape)\n",
    "\n",
    "print(X_train_my.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 483 samples, validate on 200 samples\n",
      "Epoch 1/50\n",
      "483/483 [==============================] - 2s 4ms/step - loss: 4.2400 - val_loss: 4.0281\n",
      "Epoch 2/50\n",
      "483/483 [==============================] - 0s 677us/step - loss: 4.1744 - val_loss: 3.9689\n",
      "Epoch 3/50\n",
      "483/483 [==============================] - 0s 661us/step - loss: 4.1115 - val_loss: 3.9127\n",
      "Epoch 4/50\n",
      "483/483 [==============================] - 0s 674us/step - loss: 4.0513 - val_loss: 3.8603\n",
      "Epoch 5/50\n",
      "483/483 [==============================] - 0s 670us/step - loss: 3.9957 - val_loss: 3.8110\n",
      "Epoch 6/50\n",
      "483/483 [==============================] - 0s 681us/step - loss: 3.9442 - val_loss: 3.7682\n",
      "Epoch 7/50\n",
      "483/483 [==============================] - 0s 670us/step - loss: 3.9003 - val_loss: 3.7312\n",
      "Epoch 8/50\n",
      "483/483 [==============================] - 0s 699us/step - loss: 3.8626 - val_loss: 3.7011\n",
      "Epoch 9/50\n",
      "483/483 [==============================] - 0s 669us/step - loss: 3.8321 - val_loss: 3.6766\n",
      "Epoch 10/50\n",
      "483/483 [==============================] - 0s 704us/step - loss: 3.8069 - val_loss: 3.6564\n",
      "Epoch 11/50\n",
      "483/483 [==============================] - 0s 674us/step - loss: 3.7865 - val_loss: 3.6402\n",
      "Epoch 12/50\n",
      "483/483 [==============================] - 0s 809us/step - loss: 3.7698 - val_loss: 3.6274\n",
      "Epoch 13/50\n",
      "483/483 [==============================] - 0s 690us/step - loss: 3.7565 - val_loss: 3.6166\n",
      "Epoch 14/50\n",
      "483/483 [==============================] - 0s 673us/step - loss: 3.7452 - val_loss: 3.6077\n",
      "Epoch 15/50\n",
      "483/483 [==============================] - 0s 681us/step - loss: 3.7358 - val_loss: 3.6000\n",
      "Epoch 16/50\n",
      "483/483 [==============================] - 0s 681us/step - loss: 3.7276 - val_loss: 3.5932\n",
      "Epoch 17/50\n",
      "483/483 [==============================] - 0s 673us/step - loss: 3.7204 - val_loss: 3.5877\n",
      "Epoch 18/50\n",
      "483/483 [==============================] - 0s 672us/step - loss: 3.7146 - val_loss: 3.5831\n",
      "Epoch 19/50\n",
      "483/483 [==============================] - 0s 672us/step - loss: 3.7095 - val_loss: 3.5791\n",
      "Epoch 20/50\n",
      "483/483 [==============================] - 0s 669us/step - loss: 3.7052 - val_loss: 3.5757\n",
      "Epoch 21/50\n",
      "483/483 [==============================] - 0s 666us/step - loss: 3.7014 - val_loss: 3.5725\n",
      "Epoch 22/50\n",
      "483/483 [==============================] - 0s 662us/step - loss: 3.6980 - val_loss: 3.5699\n",
      "Epoch 23/50\n",
      "483/483 [==============================] - 0s 679us/step - loss: 3.6952 - val_loss: 3.5677\n",
      "Epoch 24/50\n",
      "483/483 [==============================] - 0s 675us/step - loss: 3.6927 - val_loss: 3.5657\n",
      "Epoch 25/50\n",
      "483/483 [==============================] - 0s 671us/step - loss: 3.6906 - val_loss: 3.5640\n",
      "Epoch 26/50\n",
      "483/483 [==============================] - 0s 676us/step - loss: 3.6886 - val_loss: 3.5624\n",
      "Epoch 27/50\n",
      "483/483 [==============================] - 0s 668us/step - loss: 3.6869 - val_loss: 3.5610\n",
      "Epoch 28/50\n",
      "483/483 [==============================] - 0s 655us/step - loss: 3.6854 - val_loss: 3.5599\n",
      "Epoch 29/50\n",
      "483/483 [==============================] - 0s 670us/step - loss: 3.6841 - val_loss: 3.5588\n",
      "Epoch 30/50\n",
      "483/483 [==============================] - 0s 687us/step - loss: 3.6829 - val_loss: 3.5578\n",
      "Epoch 31/50\n",
      "483/483 [==============================] - 0s 682us/step - loss: 3.6818 - val_loss: 3.5571\n",
      "Epoch 32/50\n",
      "483/483 [==============================] - 0s 657us/step - loss: 3.6810 - val_loss: 3.5563\n",
      "Epoch 33/50\n",
      "483/483 [==============================] - 0s 652us/step - loss: 3.6802 - val_loss: 3.5558\n",
      "Epoch 34/50\n",
      "483/483 [==============================] - 0s 673us/step - loss: 3.6795 - val_loss: 3.5552\n",
      "Epoch 35/50\n",
      "483/483 [==============================] - 0s 675us/step - loss: 3.6789 - val_loss: 3.5547\n",
      "Epoch 36/50\n",
      "483/483 [==============================] - 0s 668us/step - loss: 3.6784 - val_loss: 3.5543\n",
      "Epoch 37/50\n",
      "483/483 [==============================] - 0s 661us/step - loss: 3.6779 - val_loss: 3.5539\n",
      "Epoch 38/50\n",
      "483/483 [==============================] - 0s 675us/step - loss: 3.6774 - val_loss: 3.5536\n",
      "Epoch 39/50\n",
      "483/483 [==============================] - 0s 669us/step - loss: 3.6771 - val_loss: 3.5533\n",
      "Epoch 40/50\n",
      "483/483 [==============================] - 0s 686us/step - loss: 3.6767 - val_loss: 3.5530\n",
      "Epoch 41/50\n",
      "483/483 [==============================] - 0s 651us/step - loss: 3.6765 - val_loss: 3.5528\n",
      "Epoch 42/50\n",
      "483/483 [==============================] - 0s 648us/step - loss: 3.6762 - val_loss: 3.5526\n",
      "Epoch 43/50\n",
      "483/483 [==============================] - 0s 670us/step - loss: 3.6759 - val_loss: 3.5524\n",
      "Epoch 44/50\n",
      "483/483 [==============================] - 0s 672us/step - loss: 3.6757 - val_loss: 3.5522\n",
      "Epoch 45/50\n",
      "483/483 [==============================] - 0s 672us/step - loss: 3.6755 - val_loss: 3.5521\n",
      "Epoch 46/50\n",
      "483/483 [==============================] - 0s 672us/step - loss: 3.6754 - val_loss: 3.5519\n",
      "Epoch 47/50\n",
      "483/483 [==============================] - 0s 669us/step - loss: 3.6752 - val_loss: 3.5518\n",
      "Epoch 48/50\n",
      "483/483 [==============================] - 0s 628us/step - loss: 3.6751 - val_loss: 3.5517\n",
      "Epoch 49/50\n",
      "483/483 [==============================] - 0s 640us/step - loss: 3.6749 - val_loss: 3.5516\n",
      "Epoch 50/50\n",
      "483/483 [==============================] - 0s 671us/step - loss: 3.6748 - val_loss: 3.5515\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a202d1908>"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder_my.fit(X_train_my, X_train_my,\n",
    "      epochs=50,\n",
    "      batch_size=10,\n",
    "      shuffle=True,\n",
    "      validation_data=(X_test_my, X_test_my)\n",
    "      )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if you want an encoded flatten representation of every test MNIST\n",
    "reduced_representation_my =encoder_my.predict(X_test_my)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 483 samples, validate on 200 samples\n",
      "Epoch 1/50\n",
      "483/483 [==============================] - 1s 3ms/step - loss: 3.6747 - val_loss: 3.5514\n",
      "Epoch 2/50\n",
      "483/483 [==============================] - 0s 681us/step - loss: 3.6746 - val_loss: 3.5513\n",
      "Epoch 3/50\n",
      "483/483 [==============================] - 0s 709us/step - loss: 3.6745 - val_loss: 3.5513\n",
      "Epoch 4/50\n",
      "483/483 [==============================] - 0s 721us/step - loss: 3.6745 - val_loss: 3.5512\n",
      "Epoch 5/50\n",
      "483/483 [==============================] - 0s 691us/step - loss: 3.6744 - val_loss: 3.5511\n",
      "Epoch 6/50\n",
      "483/483 [==============================] - 0s 689us/step - loss: 3.6743 - val_loss: 3.5511\n",
      "Epoch 7/50\n",
      "483/483 [==============================] - 0s 749us/step - loss: 3.6743 - val_loss: 3.5510\n",
      "Epoch 8/50\n",
      "483/483 [==============================] - 0s 898us/step - loss: 3.6742 - val_loss: 3.5510\n",
      "Epoch 9/50\n",
      "483/483 [==============================] - 0s 727us/step - loss: 3.6741 - val_loss: 3.5510\n",
      "Epoch 10/50\n",
      "483/483 [==============================] - 0s 737us/step - loss: 3.6741 - val_loss: 3.5509\n",
      "Epoch 11/50\n",
      "483/483 [==============================] - 0s 680us/step - loss: 3.6740 - val_loss: 3.5509\n",
      "Epoch 12/50\n",
      "483/483 [==============================] - 0s 692us/step - loss: 3.6740 - val_loss: 3.5508\n",
      "Epoch 13/50\n",
      "483/483 [==============================] - 0s 677us/step - loss: 3.6740 - val_loss: 3.5508\n",
      "Epoch 14/50\n",
      "483/483 [==============================] - 0s 694us/step - loss: 3.6739 - val_loss: 3.5508\n",
      "Epoch 15/50\n",
      "483/483 [==============================] - 0s 658us/step - loss: 3.6739 - val_loss: 3.5507\n",
      "Epoch 16/50\n",
      "483/483 [==============================] - 0s 698us/step - loss: 3.6739 - val_loss: 3.5507\n",
      "Epoch 17/50\n",
      "483/483 [==============================] - 0s 670us/step - loss: 3.6738 - val_loss: 3.5507\n",
      "Epoch 18/50\n",
      "483/483 [==============================] - 0s 657us/step - loss: 3.6738 - val_loss: 3.5507\n",
      "Epoch 19/50\n",
      "483/483 [==============================] - 0s 694us/step - loss: 3.6738 - val_loss: 3.5506\n",
      "Epoch 20/50\n",
      "483/483 [==============================] - 0s 661us/step - loss: 3.6737 - val_loss: 3.5506\n",
      "Epoch 21/50\n",
      "483/483 [==============================] - 0s 684us/step - loss: 3.6737 - val_loss: 3.5506\n",
      "Epoch 22/50\n",
      "483/483 [==============================] - 0s 693us/step - loss: 3.6737 - val_loss: 3.5506\n",
      "Epoch 23/50\n",
      "483/483 [==============================] - 0s 656us/step - loss: 3.6737 - val_loss: 3.5505\n",
      "Epoch 24/50\n",
      "483/483 [==============================] - 0s 678us/step - loss: 3.6736 - val_loss: 3.5505\n",
      "Epoch 25/50\n",
      "483/483 [==============================] - 0s 663us/step - loss: 3.6736 - val_loss: 3.5505\n",
      "Epoch 26/50\n",
      "483/483 [==============================] - 0s 692us/step - loss: 3.6736 - val_loss: 3.5505\n",
      "Epoch 27/50\n",
      "483/483 [==============================] - 0s 676us/step - loss: 3.6736 - val_loss: 3.5505\n",
      "Epoch 28/50\n",
      "483/483 [==============================] - 0s 671us/step - loss: 3.6736 - val_loss: 3.5505\n",
      "Epoch 29/50\n",
      "483/483 [==============================] - 0s 655us/step - loss: 3.6736 - val_loss: 3.5505\n",
      "Epoch 30/50\n",
      "483/483 [==============================] - 0s 677us/step - loss: 3.6735 - val_loss: 3.5504\n",
      "Epoch 31/50\n",
      "483/483 [==============================] - 0s 694us/step - loss: 3.6735 - val_loss: 3.5504\n",
      "Epoch 32/50\n",
      "483/483 [==============================] - 0s 681us/step - loss: 3.6735 - val_loss: 3.5504\n",
      "Epoch 33/50\n",
      "483/483 [==============================] - 0s 673us/step - loss: 3.6735 - val_loss: 3.5504\n",
      "Epoch 34/50\n",
      "483/483 [==============================] - 0s 685us/step - loss: 3.6735 - val_loss: 3.5504\n",
      "Epoch 35/50\n",
      "483/483 [==============================] - 0s 692us/step - loss: 3.6735 - val_loss: 3.5504\n",
      "Epoch 36/50\n",
      "483/483 [==============================] - 0s 669us/step - loss: 3.6734 - val_loss: 3.5504\n",
      "Epoch 37/50\n",
      "483/483 [==============================] - 0s 675us/step - loss: 3.6734 - val_loss: 3.5504\n",
      "Epoch 38/50\n",
      "483/483 [==============================] - 0s 690us/step - loss: 3.6734 - val_loss: 3.5504\n",
      "Epoch 39/50\n",
      "483/483 [==============================] - 0s 675us/step - loss: 3.6734 - val_loss: 3.5503\n",
      "Epoch 40/50\n",
      "483/483 [==============================] - 0s 687us/step - loss: 3.6734 - val_loss: 3.5503\n",
      "Epoch 41/50\n",
      "483/483 [==============================] - 0s 667us/step - loss: 3.6734 - val_loss: 3.5503\n",
      "Epoch 42/50\n",
      "483/483 [==============================] - 0s 689us/step - loss: 3.6734 - val_loss: 3.5503\n",
      "Epoch 43/50\n",
      "483/483 [==============================] - 0s 779us/step - loss: 3.6734 - val_loss: 3.5503\n",
      "Epoch 44/50\n",
      "483/483 [==============================] - 0s 759us/step - loss: 3.6734 - val_loss: 3.5503\n",
      "Epoch 45/50\n",
      "483/483 [==============================] - 0s 794us/step - loss: 3.6734 - val_loss: 3.5503\n",
      "Epoch 46/50\n",
      "483/483 [==============================] - 0s 699us/step - loss: 3.6733 - val_loss: 3.5503\n",
      "Epoch 47/50\n",
      "483/483 [==============================] - 0s 689us/step - loss: 3.6733 - val_loss: 3.5503\n",
      "Epoch 48/50\n",
      "483/483 [==============================] - 0s 689us/step - loss: 3.6733 - val_loss: 3.5503\n",
      "Epoch 49/50\n",
      "483/483 [==============================] - 0s 679us/step - loss: 3.6733 - val_loss: 3.5503\n",
      "Epoch 50/50\n",
      "483/483 [==============================] - 0s 670us/step - loss: 3.6733 - val_loss: 3.5503\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a1fbedd30>"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#encoder is the model of the autoencoder slice in the middle \n",
    "encoder_my = Model(input_img_my, y_my)\n",
    "\n",
    "autoencoder_my.compile(optimizer='adadelta', loss='mse') # reporting the loss\n",
    "\n",
    "autoencoder_my.fit(X_train_my, X_train_my,\n",
    "      epochs=50,\n",
    "      batch_size=10,\n",
    "      shuffle=True,\n",
    "      validation_data=(X_test_my, X_test_my))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if you want an encoded flatten representation of every test MNIST\n",
    "reduced_representation_my =encoder_my.predict(X_test_my)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out2_my = Dense(6, activation='softmax')(encoder_my.output)\n",
    "newmodel_my = Model(encoder_my.input,out2_my)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newmodel_my.compile(loss='categorical_crossentropy',\n",
    "          optimizer='adam', \n",
    "          metrics=['accuracy']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 483 samples, validate on 200 samples\n",
      "Epoch 1/50\n",
      "483/483 [==============================] - 1s 2ms/step - loss: 1.5223 - acc: 0.2526 - val_loss: 1.4918 - val_acc: 0.2400\n",
      "Epoch 2/50\n",
      "483/483 [==============================] - 0s 63us/step - loss: 1.4959 - acc: 0.3996 - val_loss: 1.4671 - val_acc: 0.5850\n",
      "Epoch 3/50\n",
      "483/483 [==============================] - 0s 66us/step - loss: 1.4722 - acc: 0.5611 - val_loss: 1.4432 - val_acc: 0.5850\n",
      "Epoch 4/50\n",
      "483/483 [==============================] - 0s 66us/step - loss: 1.4492 - acc: 0.5611 - val_loss: 1.4207 - val_acc: 0.5850\n",
      "Epoch 5/50\n",
      "483/483 [==============================] - 0s 64us/step - loss: 1.4269 - acc: 0.5611 - val_loss: 1.3997 - val_acc: 0.5850\n",
      "Epoch 6/50\n",
      "483/483 [==============================] - 0s 64us/step - loss: 1.4070 - acc: 0.5611 - val_loss: 1.3798 - val_acc: 0.5850\n",
      "Epoch 7/50\n",
      "483/483 [==============================] - 0s 64us/step - loss: 1.3877 - acc: 0.5611 - val_loss: 1.3613 - val_acc: 0.5850\n",
      "Epoch 8/50\n",
      "483/483 [==============================] - 0s 83us/step - loss: 1.3698 - acc: 0.5611 - val_loss: 1.3441 - val_acc: 0.5850\n",
      "Epoch 9/50\n",
      "483/483 [==============================] - 0s 80us/step - loss: 1.3534 - acc: 0.5611 - val_loss: 1.3280 - val_acc: 0.5850\n",
      "Epoch 10/50\n",
      "483/483 [==============================] - 0s 72us/step - loss: 1.3377 - acc: 0.5611 - val_loss: 1.3131 - val_acc: 0.5850\n",
      "Epoch 11/50\n",
      "483/483 [==============================] - 0s 89us/step - loss: 1.3233 - acc: 0.5611 - val_loss: 1.2994 - val_acc: 0.5850\n",
      "Epoch 12/50\n",
      "483/483 [==============================] - 0s 78us/step - loss: 1.3096 - acc: 0.5611 - val_loss: 1.2867 - val_acc: 0.5850\n",
      "Epoch 13/50\n",
      "483/483 [==============================] - 0s 64us/step - loss: 1.2975 - acc: 0.5611 - val_loss: 1.2749 - val_acc: 0.5850\n",
      "Epoch 14/50\n",
      "483/483 [==============================] - 0s 59us/step - loss: 1.2854 - acc: 0.5611 - val_loss: 1.2641 - val_acc: 0.5850\n",
      "Epoch 15/50\n",
      "483/483 [==============================] - 0s 83us/step - loss: 1.2746 - acc: 0.5611 - val_loss: 1.2541 - val_acc: 0.5850\n",
      "Epoch 16/50\n",
      "483/483 [==============================] - 0s 94us/step - loss: 1.2646 - acc: 0.5611 - val_loss: 1.2448 - val_acc: 0.5850\n",
      "Epoch 17/50\n",
      "483/483 [==============================] - 0s 71us/step - loss: 1.2553 - acc: 0.5611 - val_loss: 1.2361 - val_acc: 0.5850\n",
      "Epoch 18/50\n",
      "483/483 [==============================] - 0s 62us/step - loss: 1.2464 - acc: 0.5611 - val_loss: 1.2283 - val_acc: 0.5850\n",
      "Epoch 19/50\n",
      "483/483 [==============================] - 0s 70us/step - loss: 1.2383 - acc: 0.5611 - val_loss: 1.2208 - val_acc: 0.5850\n",
      "Epoch 20/50\n",
      "483/483 [==============================] - 0s 78us/step - loss: 1.2309 - acc: 0.5611 - val_loss: 1.2137 - val_acc: 0.5850\n",
      "Epoch 21/50\n",
      "483/483 [==============================] - 0s 60us/step - loss: 1.2234 - acc: 0.5611 - val_loss: 1.2074 - val_acc: 0.5850\n",
      "Epoch 22/50\n",
      "483/483 [==============================] - 0s 58us/step - loss: 1.2170 - acc: 0.5611 - val_loss: 1.2015 - val_acc: 0.5850\n",
      "Epoch 23/50\n",
      "483/483 [==============================] - 0s 62us/step - loss: 1.2105 - acc: 0.5611 - val_loss: 1.1960 - val_acc: 0.5850\n",
      "Epoch 24/50\n",
      "483/483 [==============================] - 0s 65us/step - loss: 1.2047 - acc: 0.5611 - val_loss: 1.1908 - val_acc: 0.5850\n",
      "Epoch 25/50\n",
      "483/483 [==============================] - 0s 72us/step - loss: 1.1992 - acc: 0.5611 - val_loss: 1.1861 - val_acc: 0.5850\n",
      "Epoch 26/50\n",
      "483/483 [==============================] - 0s 74us/step - loss: 1.1941 - acc: 0.5611 - val_loss: 1.1816 - val_acc: 0.5850\n",
      "Epoch 27/50\n",
      "483/483 [==============================] - 0s 64us/step - loss: 1.1891 - acc: 0.5611 - val_loss: 1.1774 - val_acc: 0.5850\n",
      "Epoch 28/50\n",
      "483/483 [==============================] - 0s 69us/step - loss: 1.1849 - acc: 0.5611 - val_loss: 1.1736 - val_acc: 0.5850\n",
      "Epoch 29/50\n",
      "483/483 [==============================] - 0s 66us/step - loss: 1.1803 - acc: 0.5611 - val_loss: 1.1700 - val_acc: 0.5850\n",
      "Epoch 30/50\n",
      "483/483 [==============================] - 0s 67us/step - loss: 1.1764 - acc: 0.5611 - val_loss: 1.1665 - val_acc: 0.5850\n",
      "Epoch 31/50\n",
      "483/483 [==============================] - 0s 67us/step - loss: 1.1725 - acc: 0.5611 - val_loss: 1.1633 - val_acc: 0.5850\n",
      "Epoch 32/50\n",
      "483/483 [==============================] - 0s 70us/step - loss: 1.1694 - acc: 0.5611 - val_loss: 1.1604 - val_acc: 0.5850\n",
      "Epoch 33/50\n",
      "483/483 [==============================] - 0s 71us/step - loss: 1.1659 - acc: 0.5611 - val_loss: 1.1576 - val_acc: 0.5850\n",
      "Epoch 34/50\n",
      "483/483 [==============================] - 0s 71us/step - loss: 1.1630 - acc: 0.5611 - val_loss: 1.1550 - val_acc: 0.5850\n",
      "Epoch 35/50\n",
      "483/483 [==============================] - 0s 67us/step - loss: 1.1597 - acc: 0.5611 - val_loss: 1.1525 - val_acc: 0.5850\n",
      "Epoch 36/50\n",
      "483/483 [==============================] - 0s 63us/step - loss: 1.1572 - acc: 0.5611 - val_loss: 1.1501 - val_acc: 0.5850\n",
      "Epoch 37/50\n",
      "483/483 [==============================] - 0s 61us/step - loss: 1.1546 - acc: 0.5611 - val_loss: 1.1479 - val_acc: 0.5850\n",
      "Epoch 38/50\n",
      "483/483 [==============================] - 0s 77us/step - loss: 1.1524 - acc: 0.5611 - val_loss: 1.1458 - val_acc: 0.5850\n",
      "Epoch 39/50\n",
      "483/483 [==============================] - 0s 69us/step - loss: 1.1498 - acc: 0.5611 - val_loss: 1.1438 - val_acc: 0.5850\n",
      "Epoch 40/50\n",
      "483/483 [==============================] - 0s 95us/step - loss: 1.1478 - acc: 0.5611 - val_loss: 1.1420 - val_acc: 0.5850\n",
      "Epoch 41/50\n",
      "483/483 [==============================] - 0s 66us/step - loss: 1.1456 - acc: 0.5611 - val_loss: 1.1403 - val_acc: 0.5850\n",
      "Epoch 42/50\n",
      "483/483 [==============================] - 0s 79us/step - loss: 1.1439 - acc: 0.5611 - val_loss: 1.1387 - val_acc: 0.5850\n",
      "Epoch 43/50\n",
      "483/483 [==============================] - 0s 59us/step - loss: 1.1420 - acc: 0.5611 - val_loss: 1.1372 - val_acc: 0.5850\n",
      "Epoch 44/50\n",
      "483/483 [==============================] - 0s 71us/step - loss: 1.1404 - acc: 0.5611 - val_loss: 1.1358 - val_acc: 0.5850\n",
      "Epoch 45/50\n",
      "483/483 [==============================] - 0s 100us/step - loss: 1.1388 - acc: 0.5611 - val_loss: 1.1344 - val_acc: 0.5850\n",
      "Epoch 46/50\n",
      "483/483 [==============================] - 0s 72us/step - loss: 1.1372 - acc: 0.5611 - val_loss: 1.1332 - val_acc: 0.5850\n",
      "Epoch 47/50\n",
      "483/483 [==============================] - 0s 86us/step - loss: 1.1358 - acc: 0.5611 - val_loss: 1.1319 - val_acc: 0.5850\n",
      "Epoch 48/50\n",
      "483/483 [==============================] - 0s 108us/step - loss: 1.1345 - acc: 0.5611 - val_loss: 1.1306 - val_acc: 0.5850\n",
      "Epoch 49/50\n",
      "483/483 [==============================] - 0s 102us/step - loss: 1.1331 - acc: 0.5611 - val_loss: 1.1295 - val_acc: 0.5850\n",
      "Epoch 50/50\n",
      "483/483 [==============================] - 0s 88us/step - loss: 1.1320 - acc: 0.5611 - val_loss: 1.1285 - val_acc: 0.5850\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a2049a278>"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newmodel_my.fit(X_train_my, Y_train_my,\n",
    "      epochs=50,\n",
    "      batch_size=128,\n",
    "      shuffle=True,\n",
    "      validation_data=(X_test_my, Y_test_my))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 1s 3ms/step\n",
      "[[ 0.02542705  0.03303972  0.24867606  0.54774761  0.13082184  0.01428762]\n",
      " [ 0.02524919  0.03246367  0.25298163  0.54429501  0.1309413   0.01406927]\n",
      " [ 0.02531124  0.03257883  0.25239184  0.544348    0.13123895  0.01413113]\n",
      " ..., \n",
      " [ 0.02553233  0.03303137  0.24987435  0.5465396   0.130688    0.0143343 ]\n",
      " [ 0.02530691  0.03255084  0.25286904  0.54387313  0.13129136  0.01410875]\n",
      " [ 0.02522945  0.03257915  0.25163555  0.54513258  0.13133782  0.01408541]]\n",
      "200/200 [==============================] - 0s 129us/step\n",
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "200/200 [==============================] - 0s 155us/step\n",
      "[ 3.  2.  3.  3.  5.  3.  2.  2.  3.  3.  3.  2.  4.  3.  2.  2.  3.  3.\n",
      "  2.  1.  2.  3.  4.  3.  3.  2.  4.  2.  3.  4.  5.  3.  3.  2.  3.  3.\n",
      "  3.  3.  2.  3.  2.  3.  2.  3.  4.  2.  3.  3.  2.  4.  1.  3.  2.  3.\n",
      "  3.  5.  3.  3.  2.  3.  3.  3.  3.  2.  3.  4.  3.  2.  3.  3.  5.  2.\n",
      "  3.  3.  3.  4.  3.  3.  3.  3.  3.  3.  3.  3.  3.  2.  2.  2.  3.  3.\n",
      "  4.  2.  3.  3.  3.  4.  3.  3.  3.  3.  2.  3.  3.  4.  3.  3.  4.  3.\n",
      "  3.  3.  2.  3.  3.  3.  3.  2.  3.  4.  5.  3.  3.  5.  2.  2.  2.  2.\n",
      "  2.  2.  3.  3.  3.  3.  3.  2.  3.  4.  3.  3.  3.  2.  3.  3.  4.  4.\n",
      "  2.  4.  3.  4.  2.  3.  3.  3.  2.  3.  3.  3.  2.  4.  4.  4.  3.  3.\n",
      "  3.  2.  3.  2.  3.  2.  3.  3.  2.  3.  3.  4.  3.  4.  3.  3.  3.  3.\n",
      "  3.  3.  4.  2.  2.  3.  3.  2.  3.  3.  1.  3.  3.  3.  3.  4.  3.  3.\n",
      "  2.  4.]\n",
      "Accuracy:  0.585\n",
      "200/200 [==============================] - 0s 122us/step\n",
      "[[  0   0   3   0   0]\n",
      " [  0   0  48   0   0]\n",
      " [  0   0 117   0   0]\n",
      " [  0   0  26   0   0]\n",
      " [  0   0   6   0   0]]\n"
     ]
    }
   ],
   "source": [
    "print(newmodel_my.predict(X_test_my, verbose=1))\n",
    "print(np.argmax(newmodel_my.predict(X_test_my, verbose=1),axis=1))\n",
    "\n",
    "scores_my = newmodel_my.evaluate(X_test_my, Y_test_my, verbose=1) \n",
    "print(y_test_my)\n",
    "print(\"Accuracy: \", scores_my[1])\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cfm = confusion_matrix(np.array(y_test_my),np.array(np.argmax(newmodel_my.predict(X_test_my, verbose=1),axis=1)))\n",
    "print(cfm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
