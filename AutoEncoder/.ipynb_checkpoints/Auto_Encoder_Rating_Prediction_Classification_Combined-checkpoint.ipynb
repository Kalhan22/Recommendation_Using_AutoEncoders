{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model \n",
    "from keras.layers import Input, Dense \n",
    "from keras.utils import np_utils \n",
    "import numpy as np\n",
    "from tensorflow.python.ops.variables import trainable_variables\n",
    "from numpy import genfromtxt\n",
    "\n",
    "X_train_my = genfromtxt('../UserData/655/trainX_item_655.csv', delimiter=',')\n",
    "y_train_my = genfromtxt('../UserData/655/trainY_item_655.csv', delimiter=',')\n",
    "X_test_my = genfromtxt('../UserData/655/testX_item_655.csv', delimiter=',')\n",
    "y_test_my = genfromtxt('../UserData/655/testY_item_655.csv', delimiter=',')\n",
    "\n",
    "num_classes_my = 6 # there are 5 classes (1 per rating class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(412, 19)\n"
     ]
    }
   ],
   "source": [
    "X_train_my = X_train_my.astype('float32') \n",
    "X_train_my = X_train_my.astype('float32')\n",
    "print(X_train_my.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Y_train_my = np_utils.to_categorical(y_train_my, num_classes_my) # One-hot encode the labels\n",
    "\n",
    "Y_test_my = np_utils.to_categorical(y_test_my, num_classes_my) # One-hot encode the labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"input_5:0\", shape=(?, 19), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "input_img_my = Input(shape=(19,))\n",
    "\n",
    "print(input_img_my)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"dense_24/Relu:0\", shape=(?, 19), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x_my = Dense(19, activation='relu')(input_img_my)\n",
    "print(x_my)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoded1_my = Dense(15, activation='relu')(x_my)\n",
    "encoded2_my = Dense(12, activation='relu')(encoded1_my)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_my = Dense(10, activation='relu')(encoded2_my)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoded2_my = Dense(12, activation='relu')(y_my)\n",
    "decoded1_my = Dense(15, activation='relu')(decoded2_my)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z_my = Dense(19, activation='sigmoid')(decoded1_my)\n",
    "autoencoder_my = Model(input_img_my, z_my)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.training.Model object at 0x1a159c1860>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#encoder is the model of the autoencoder slice in the middle \n",
    "encoder_my = Model(input_img_my, y_my)\n",
    "\n",
    "print(encoder_my)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "autoencoder_my.compile(optimizer='adadelta', loss='mse') # reporting the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(161, 19)\n",
      "(412, 19)\n"
     ]
    }
   ],
   "source": [
    "print(X_test_my.shape)\n",
    "\n",
    "print(X_train_my.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 412 samples, validate on 161 samples\n",
      "Epoch 1/50\n",
      "412/412 [==============================] - 1s 2ms/step - loss: 0.6655 - val_loss: 0.5398\n",
      "Epoch 2/50\n",
      "412/412 [==============================] - 0s 634us/step - loss: 0.4591 - val_loss: 0.4067\n",
      "Epoch 3/50\n",
      "412/412 [==============================] - 0s 636us/step - loss: 0.4158 - val_loss: 0.3990\n",
      "Epoch 4/50\n",
      "412/412 [==============================] - 0s 636us/step - loss: 0.4123 - val_loss: 0.3978\n",
      "Epoch 5/50\n",
      "412/412 [==============================] - 0s 650us/step - loss: 0.4117 - val_loss: 0.3966\n",
      "Epoch 6/50\n",
      "412/412 [==============================] - 0s 646us/step - loss: 0.4112 - val_loss: 0.3962\n",
      "Epoch 7/50\n",
      "412/412 [==============================] - 0s 625us/step - loss: 0.4108 - val_loss: 0.3954\n",
      "Epoch 8/50\n",
      "412/412 [==============================] - 0s 686us/step - loss: 0.4104 - val_loss: 0.3957\n",
      "Epoch 9/50\n",
      "412/412 [==============================] - 0s 822us/step - loss: 0.4100 - val_loss: 0.3959\n",
      "Epoch 10/50\n",
      "412/412 [==============================] - 0s 821us/step - loss: 0.4095 - val_loss: 0.3945\n",
      "Epoch 11/50\n",
      "412/412 [==============================] - 0s 661us/step - loss: 0.4089 - val_loss: 0.3938\n",
      "Epoch 12/50\n",
      "412/412 [==============================] - 0s 627us/step - loss: 0.4083 - val_loss: 0.3934\n",
      "Epoch 13/50\n",
      "412/412 [==============================] - 0s 909us/step - loss: 0.4076 - val_loss: 0.3925\n",
      "Epoch 14/50\n",
      "412/412 [==============================] - 0s 731us/step - loss: 0.4066 - val_loss: 0.3916\n",
      "Epoch 15/50\n",
      "412/412 [==============================] - 0s 884us/step - loss: 0.4056 - val_loss: 0.3908\n",
      "Epoch 16/50\n",
      "412/412 [==============================] - 0s 880us/step - loss: 0.4045 - val_loss: 0.3897\n",
      "Epoch 17/50\n",
      "412/412 [==============================] - 0s 645us/step - loss: 0.4032 - val_loss: 0.3888\n",
      "Epoch 18/50\n",
      "412/412 [==============================] - 0s 612us/step - loss: 0.4019 - val_loss: 0.3877\n",
      "Epoch 19/50\n",
      "412/412 [==============================] - 0s 876us/step - loss: 0.4005 - val_loss: 0.3862\n",
      "Epoch 20/50\n",
      "412/412 [==============================] - 0s 588us/step - loss: 0.3992 - val_loss: 0.3849\n",
      "Epoch 21/50\n",
      "412/412 [==============================] - 0s 810us/step - loss: 0.3981 - val_loss: 0.3839\n",
      "Epoch 22/50\n",
      "412/412 [==============================] - 0s 646us/step - loss: 0.3971 - val_loss: 0.3831\n",
      "Epoch 23/50\n",
      "412/412 [==============================] - 0s 786us/step - loss: 0.3961 - val_loss: 0.3822\n",
      "Epoch 24/50\n",
      "412/412 [==============================] - 0s 787us/step - loss: 0.3952 - val_loss: 0.3814\n",
      "Epoch 25/50\n",
      "412/412 [==============================] - 0s 738us/step - loss: 0.3944 - val_loss: 0.3804\n",
      "Epoch 26/50\n",
      "412/412 [==============================] - 0s 626us/step - loss: 0.3936 - val_loss: 0.3796\n",
      "Epoch 27/50\n",
      "412/412 [==============================] - 0s 806us/step - loss: 0.3929 - val_loss: 0.3790\n",
      "Epoch 28/50\n",
      "412/412 [==============================] - 0s 721us/step - loss: 0.3922 - val_loss: 0.3781\n",
      "Epoch 29/50\n",
      "412/412 [==============================] - 0s 760us/step - loss: 0.3915 - val_loss: 0.3772\n",
      "Epoch 30/50\n",
      "412/412 [==============================] - 0s 640us/step - loss: 0.3909 - val_loss: 0.3768\n",
      "Epoch 31/50\n",
      "412/412 [==============================] - 0s 779us/step - loss: 0.3903 - val_loss: 0.3762\n",
      "Epoch 32/50\n",
      "412/412 [==============================] - 0s 762us/step - loss: 0.3897 - val_loss: 0.3756\n",
      "Epoch 33/50\n",
      "412/412 [==============================] - 0s 676us/step - loss: 0.3893 - val_loss: 0.3749\n",
      "Epoch 34/50\n",
      "412/412 [==============================] - 0s 781us/step - loss: 0.3888 - val_loss: 0.3745\n",
      "Epoch 35/50\n",
      "412/412 [==============================] - 0s 811us/step - loss: 0.3885 - val_loss: 0.3741\n",
      "Epoch 36/50\n",
      "412/412 [==============================] - 0s 652us/step - loss: 0.3880 - val_loss: 0.3737\n",
      "Epoch 37/50\n",
      "412/412 [==============================] - 0s 785us/step - loss: 0.3878 - val_loss: 0.3734\n",
      "Epoch 38/50\n",
      "412/412 [==============================] - 0s 711us/step - loss: 0.3875 - val_loss: 0.3726\n",
      "Epoch 39/50\n",
      "412/412 [==============================] - 0s 778us/step - loss: 0.3871 - val_loss: 0.3724\n",
      "Epoch 40/50\n",
      "412/412 [==============================] - 0s 743us/step - loss: 0.3869 - val_loss: 0.3722\n",
      "Epoch 41/50\n",
      "412/412 [==============================] - 0s 628us/step - loss: 0.3867 - val_loss: 0.3717\n",
      "Epoch 42/50\n",
      "412/412 [==============================] - 0s 801us/step - loss: 0.3864 - val_loss: 0.3715\n",
      "Epoch 43/50\n",
      "412/412 [==============================] - 0s 949us/step - loss: 0.3861 - val_loss: 0.3711\n",
      "Epoch 44/50\n",
      "412/412 [==============================] - 0s 817us/step - loss: 0.3859 - val_loss: 0.3709\n",
      "Epoch 45/50\n",
      "412/412 [==============================] - 0s 632us/step - loss: 0.3857 - val_loss: 0.3705\n",
      "Epoch 46/50\n",
      "412/412 [==============================] - 0s 668us/step - loss: 0.3854 - val_loss: 0.3704\n",
      "Epoch 47/50\n",
      "412/412 [==============================] - 0s 785us/step - loss: 0.3852 - val_loss: 0.3700\n",
      "Epoch 48/50\n",
      "412/412 [==============================] - 0s 670us/step - loss: 0.3850 - val_loss: 0.3697\n",
      "Epoch 49/50\n",
      "412/412 [==============================] - 0s 644us/step - loss: 0.3848 - val_loss: 0.3705\n",
      "Epoch 50/50\n",
      "412/412 [==============================] - 0s 820us/step - loss: 0.3847 - val_loss: 0.3697\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a15a47fd0>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder_my.fit(X_train_my, X_train_my,\n",
    "      epochs=50,\n",
    "      batch_size=10,\n",
    "      shuffle=True,\n",
    "      validation_data=(X_test_my, X_test_my)\n",
    "      )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if you want an encoded flatten representation of every test MNIST\n",
    "reduced_representation_my =encoder_my.predict(X_test_my)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 412 samples, validate on 161 samples\n",
      "Epoch 1/50\n",
      "412/412 [==============================] - 1s 2ms/step - loss: 0.3845 - val_loss: 0.3690\n",
      "Epoch 2/50\n",
      "412/412 [==============================] - 0s 660us/step - loss: 0.3842 - val_loss: 0.3686\n",
      "Epoch 3/50\n",
      "412/412 [==============================] - 0s 624us/step - loss: 0.3839 - val_loss: 0.3683\n",
      "Epoch 4/50\n",
      "412/412 [==============================] - 0s 630us/step - loss: 0.3838 - val_loss: 0.3680\n",
      "Epoch 5/50\n",
      "412/412 [==============================] - 0s 647us/step - loss: 0.3834 - val_loss: 0.3675\n",
      "Epoch 6/50\n",
      "412/412 [==============================] - 0s 659us/step - loss: 0.3832 - val_loss: 0.3675\n",
      "Epoch 7/50\n",
      "412/412 [==============================] - 0s 652us/step - loss: 0.3829 - val_loss: 0.3674\n",
      "Epoch 8/50\n",
      "412/412 [==============================] - 0s 650us/step - loss: 0.3827 - val_loss: 0.3667\n",
      "Epoch 9/50\n",
      "412/412 [==============================] - 0s 724us/step - loss: 0.3824 - val_loss: 0.3662\n",
      "Epoch 10/50\n",
      "412/412 [==============================] - 0s 657us/step - loss: 0.3821 - val_loss: 0.3657\n",
      "Epoch 11/50\n",
      "412/412 [==============================] - 0s 745us/step - loss: 0.3818 - val_loss: 0.3654\n",
      "Epoch 12/50\n",
      "412/412 [==============================] - 0s 773us/step - loss: 0.3815 - val_loss: 0.3648\n",
      "Epoch 13/50\n",
      "412/412 [==============================] - 0s 831us/step - loss: 0.3812 - val_loss: 0.3644\n",
      "Epoch 14/50\n",
      "412/412 [==============================] - 0s 622us/step - loss: 0.3809 - val_loss: 0.3641\n",
      "Epoch 15/50\n",
      "412/412 [==============================] - 0s 554us/step - loss: 0.3806 - val_loss: 0.3635\n",
      "Epoch 16/50\n",
      "412/412 [==============================] - 0s 796us/step - loss: 0.3802 - val_loss: 0.3631\n",
      "Epoch 17/50\n",
      "412/412 [==============================] - 0s 653us/step - loss: 0.3799 - val_loss: 0.3628\n",
      "Epoch 18/50\n",
      "412/412 [==============================] - 0s 788us/step - loss: 0.3795 - val_loss: 0.3627\n",
      "Epoch 19/50\n",
      "412/412 [==============================] - 0s 638us/step - loss: 0.3792 - val_loss: 0.3620\n",
      "Epoch 20/50\n",
      "412/412 [==============================] - 0s 801us/step - loss: 0.3788 - val_loss: 0.3616\n",
      "Epoch 21/50\n",
      "412/412 [==============================] - 0s 765us/step - loss: 0.3784 - val_loss: 0.3611\n",
      "Epoch 22/50\n",
      "412/412 [==============================] - 0s 699us/step - loss: 0.3780 - val_loss: 0.3603\n",
      "Epoch 23/50\n",
      "412/412 [==============================] - 0s 809us/step - loss: 0.3776 - val_loss: 0.3604\n",
      "Epoch 24/50\n",
      "412/412 [==============================] - 0s 821us/step - loss: 0.3773 - val_loss: 0.3594\n",
      "Epoch 25/50\n",
      "412/412 [==============================] - 0s 892us/step - loss: 0.3770 - val_loss: 0.3589\n",
      "Epoch 26/50\n",
      "412/412 [==============================] - 0s 715us/step - loss: 0.3766 - val_loss: 0.3587\n",
      "Epoch 27/50\n",
      "412/412 [==============================] - 0s 810us/step - loss: 0.3763 - val_loss: 0.3582\n",
      "Epoch 28/50\n",
      "412/412 [==============================] - 0s 716us/step - loss: 0.3760 - val_loss: 0.3578\n",
      "Epoch 29/50\n",
      "412/412 [==============================] - 0s 789us/step - loss: 0.3757 - val_loss: 0.3573\n",
      "Epoch 30/50\n",
      "412/412 [==============================] - 0s 731us/step - loss: 0.3754 - val_loss: 0.3574\n",
      "Epoch 31/50\n",
      "412/412 [==============================] - 0s 747us/step - loss: 0.3751 - val_loss: 0.3567\n",
      "Epoch 32/50\n",
      "412/412 [==============================] - 0s 655us/step - loss: 0.3749 - val_loss: 0.3563\n",
      "Epoch 33/50\n",
      "412/412 [==============================] - 0s 693us/step - loss: 0.3745 - val_loss: 0.3562\n",
      "Epoch 34/50\n",
      "412/412 [==============================] - 0s 822us/step - loss: 0.3743 - val_loss: 0.3557\n",
      "Epoch 35/50\n",
      "412/412 [==============================] - 0s 680us/step - loss: 0.3740 - val_loss: 0.3556\n",
      "Epoch 36/50\n",
      "412/412 [==============================] - 0s 814us/step - loss: 0.3738 - val_loss: 0.3549\n",
      "Epoch 37/50\n",
      "412/412 [==============================] - 0s 845us/step - loss: 0.3735 - val_loss: 0.3547\n",
      "Epoch 38/50\n",
      "412/412 [==============================] - 0s 689us/step - loss: 0.3733 - val_loss: 0.3544\n",
      "Epoch 39/50\n",
      "412/412 [==============================] - 0s 813us/step - loss: 0.3731 - val_loss: 0.3542\n",
      "Epoch 40/50\n",
      "412/412 [==============================] - 0s 658us/step - loss: 0.3728 - val_loss: 0.3537\n",
      "Epoch 41/50\n",
      "412/412 [==============================] - 0s 824us/step - loss: 0.3727 - val_loss: 0.3536\n",
      "Epoch 42/50\n",
      "412/412 [==============================] - 0s 744us/step - loss: 0.3725 - val_loss: 0.3533\n",
      "Epoch 43/50\n",
      "412/412 [==============================] - 0s 713us/step - loss: 0.3722 - val_loss: 0.3535\n",
      "Epoch 44/50\n",
      "412/412 [==============================] - 0s 780us/step - loss: 0.3721 - val_loss: 0.3531\n",
      "Epoch 45/50\n",
      "412/412 [==============================] - 0s 974us/step - loss: 0.3719 - val_loss: 0.3525\n",
      "Epoch 46/50\n",
      "412/412 [==============================] - 0s 839us/step - loss: 0.3717 - val_loss: 0.3525\n",
      "Epoch 47/50\n",
      "412/412 [==============================] - 0s 761us/step - loss: 0.3715 - val_loss: 0.3526\n",
      "Epoch 48/50\n",
      "412/412 [==============================] - 0s 647us/step - loss: 0.3714 - val_loss: 0.3520\n",
      "Epoch 49/50\n",
      "412/412 [==============================] - 0s 743us/step - loss: 0.3712 - val_loss: 0.3520\n",
      "Epoch 50/50\n",
      "412/412 [==============================] - 0s 883us/step - loss: 0.3710 - val_loss: 0.3518\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a15f4b400>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#encoder is the model of the autoencoder slice in the middle \n",
    "encoder_my = Model(input_img_my, y_my)\n",
    "\n",
    "autoencoder_my.compile(optimizer='adadelta', loss='mse') # reporting the loss\n",
    "\n",
    "autoencoder_my.fit(X_train_my, X_train_my,\n",
    "      epochs=50,\n",
    "      batch_size=10,\n",
    "      shuffle=True,\n",
    "      validation_data=(X_test_my, X_test_my))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want an encoded flatten representation of every test MNIST\n",
    "reduced_representation_my =encoder_my.predict(X_test_my)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out2_my = Dense(6, activation='softmax')(encoder_my.output)\n",
    "newmodel_my = Model(encoder_my.input,out2_my)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newmodel_my.compile(loss='categorical_crossentropy',\n",
    "          optimizer='adam', \n",
    "          metrics=['accuracy']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 412 samples, validate on 161 samples\n",
      "Epoch 1/50\n",
      "412/412 [==============================] - 0s 928us/step - loss: 9.0371 - acc: 0.0170 - val_loss: 7.9762 - val_acc: 0.0373\n",
      "Epoch 2/50\n",
      "412/412 [==============================] - 0s 74us/step - loss: 7.7543 - acc: 0.0243 - val_loss: 6.9155 - val_acc: 0.0373\n",
      "Epoch 3/50\n",
      "412/412 [==============================] - 0s 67us/step - loss: 6.5887 - acc: 0.0388 - val_loss: 5.9619 - val_acc: 0.0559\n",
      "Epoch 4/50\n",
      "412/412 [==============================] - 0s 69us/step - loss: 5.5674 - acc: 0.0485 - val_loss: 5.1478 - val_acc: 0.0745\n",
      "Epoch 5/50\n",
      "412/412 [==============================] - 0s 76us/step - loss: 4.7126 - acc: 0.0583 - val_loss: 4.4615 - val_acc: 0.0745\n",
      "Epoch 6/50\n",
      "412/412 [==============================] - 0s 70us/step - loss: 4.0125 - acc: 0.0680 - val_loss: 3.8849 - val_acc: 0.0807\n",
      "Epoch 7/50\n",
      "412/412 [==============================] - 0s 62us/step - loss: 3.4567 - acc: 0.0680 - val_loss: 3.4401 - val_acc: 0.0745\n",
      "Epoch 8/50\n",
      "412/412 [==============================] - 0s 77us/step - loss: 3.0186 - acc: 0.0922 - val_loss: 3.0799 - val_acc: 0.0870\n",
      "Epoch 9/50\n",
      "412/412 [==============================] - 0s 70us/step - loss: 2.6887 - acc: 0.1189 - val_loss: 2.8056 - val_acc: 0.1118\n",
      "Epoch 10/50\n",
      "412/412 [==============================] - 0s 86us/step - loss: 2.4408 - acc: 0.1966 - val_loss: 2.6044 - val_acc: 0.2857\n",
      "Epoch 11/50\n",
      "412/412 [==============================] - 0s 96us/step - loss: 2.2771 - acc: 0.3058 - val_loss: 2.4613 - val_acc: 0.3540\n",
      "Epoch 12/50\n",
      "412/412 [==============================] - 0s 78us/step - loss: 2.1613 - acc: 0.4078 - val_loss: 2.3614 - val_acc: 0.4534\n",
      "Epoch 13/50\n",
      "412/412 [==============================] - 0s 70us/step - loss: 2.0809 - acc: 0.4466 - val_loss: 2.2838 - val_acc: 0.4596\n",
      "Epoch 14/50\n",
      "412/412 [==============================] - 0s 77us/step - loss: 2.0192 - acc: 0.4684 - val_loss: 2.2157 - val_acc: 0.4907\n",
      "Epoch 15/50\n",
      "412/412 [==============================] - 0s 77us/step - loss: 1.9597 - acc: 0.4903 - val_loss: 2.1539 - val_acc: 0.5155\n",
      "Epoch 16/50\n",
      "412/412 [==============================] - 0s 77us/step - loss: 1.9066 - acc: 0.5024 - val_loss: 2.0936 - val_acc: 0.5280\n",
      "Epoch 17/50\n",
      "412/412 [==============================] - 0s 78us/step - loss: 1.8544 - acc: 0.5073 - val_loss: 2.0369 - val_acc: 0.5280\n",
      "Epoch 18/50\n",
      "412/412 [==============================] - 0s 89us/step - loss: 1.8046 - acc: 0.5073 - val_loss: 1.9854 - val_acc: 0.5217\n",
      "Epoch 19/50\n",
      "412/412 [==============================] - 0s 79us/step - loss: 1.7584 - acc: 0.5073 - val_loss: 1.9378 - val_acc: 0.5217\n",
      "Epoch 20/50\n",
      "412/412 [==============================] - 0s 82us/step - loss: 1.7172 - acc: 0.5073 - val_loss: 1.8952 - val_acc: 0.5217\n",
      "Epoch 21/50\n",
      "412/412 [==============================] - 0s 78us/step - loss: 1.6782 - acc: 0.5097 - val_loss: 1.8558 - val_acc: 0.5217\n",
      "Epoch 22/50\n",
      "412/412 [==============================] - 0s 80us/step - loss: 1.6433 - acc: 0.5097 - val_loss: 1.8172 - val_acc: 0.5217\n",
      "Epoch 23/50\n",
      "412/412 [==============================] - 0s 70us/step - loss: 1.6079 - acc: 0.5146 - val_loss: 1.7793 - val_acc: 0.5217\n",
      "Epoch 24/50\n",
      "412/412 [==============================] - 0s 72us/step - loss: 1.5768 - acc: 0.5097 - val_loss: 1.7410 - val_acc: 0.5217\n",
      "Epoch 25/50\n",
      "412/412 [==============================] - 0s 86us/step - loss: 1.5483 - acc: 0.5073 - val_loss: 1.7045 - val_acc: 0.5155\n",
      "Epoch 26/50\n",
      "412/412 [==============================] - 0s 81us/step - loss: 1.5209 - acc: 0.5073 - val_loss: 1.6725 - val_acc: 0.5217\n",
      "Epoch 27/50\n",
      "412/412 [==============================] - 0s 68us/step - loss: 1.4952 - acc: 0.5121 - val_loss: 1.6440 - val_acc: 0.5342\n",
      "Epoch 28/50\n",
      "412/412 [==============================] - ETA: 0s - loss: 1.5749 - acc: 0.445 - 0s 74us/step - loss: 1.4729 - acc: 0.5194 - val_loss: 1.6178 - val_acc: 0.5342\n",
      "Epoch 29/50\n",
      "412/412 [==============================] - 0s 79us/step - loss: 1.4505 - acc: 0.5194 - val_loss: 1.5955 - val_acc: 0.5342\n",
      "Epoch 30/50\n",
      "412/412 [==============================] - 0s 61us/step - loss: 1.4298 - acc: 0.5291 - val_loss: 1.5752 - val_acc: 0.5404\n",
      "Epoch 31/50\n",
      "412/412 [==============================] - 0s 66us/step - loss: 1.4102 - acc: 0.5437 - val_loss: 1.5547 - val_acc: 0.5342\n",
      "Epoch 32/50\n",
      "412/412 [==============================] - 0s 74us/step - loss: 1.3923 - acc: 0.5485 - val_loss: 1.5357 - val_acc: 0.5342\n",
      "Epoch 33/50\n",
      "412/412 [==============================] - ETA: 0s - loss: 1.3629 - acc: 0.562 - 0s 86us/step - loss: 1.3755 - acc: 0.5485 - val_loss: 1.5180 - val_acc: 0.5342\n",
      "Epoch 34/50\n",
      "412/412 [==============================] - 0s 86us/step - loss: 1.3596 - acc: 0.5510 - val_loss: 1.5021 - val_acc: 0.5404\n",
      "Epoch 35/50\n",
      "412/412 [==============================] - 0s 98us/step - loss: 1.3443 - acc: 0.5510 - val_loss: 1.4906 - val_acc: 0.5466\n",
      "Epoch 36/50\n",
      "412/412 [==============================] - 0s 101us/step - loss: 1.3296 - acc: 0.5485 - val_loss: 1.4796 - val_acc: 0.5466\n",
      "Epoch 37/50\n",
      "412/412 [==============================] - 0s 103us/step - loss: 1.3158 - acc: 0.5510 - val_loss: 1.4681 - val_acc: 0.5466\n",
      "Epoch 38/50\n",
      "412/412 [==============================] - 0s 71us/step - loss: 1.3035 - acc: 0.5510 - val_loss: 1.4570 - val_acc: 0.5466\n",
      "Epoch 39/50\n",
      "412/412 [==============================] - 0s 73us/step - loss: 1.2905 - acc: 0.5510 - val_loss: 1.4449 - val_acc: 0.5466\n",
      "Epoch 40/50\n",
      "412/412 [==============================] - 0s 72us/step - loss: 1.2798 - acc: 0.5558 - val_loss: 1.4313 - val_acc: 0.5528\n",
      "Epoch 41/50\n",
      "412/412 [==============================] - 0s 86us/step - loss: 1.2677 - acc: 0.5534 - val_loss: 1.4192 - val_acc: 0.5528\n",
      "Epoch 42/50\n",
      "412/412 [==============================] - 0s 72us/step - loss: 1.2577 - acc: 0.5510 - val_loss: 1.4072 - val_acc: 0.5528\n",
      "Epoch 43/50\n",
      "412/412 [==============================] - 0s 65us/step - loss: 1.2485 - acc: 0.5510 - val_loss: 1.3971 - val_acc: 0.5590\n",
      "Epoch 44/50\n",
      "412/412 [==============================] - 0s 70us/step - loss: 1.2398 - acc: 0.5534 - val_loss: 1.3875 - val_acc: 0.5590\n",
      "Epoch 45/50\n",
      "412/412 [==============================] - ETA: 0s - loss: 1.2515 - acc: 0.562 - 0s 69us/step - loss: 1.2310 - acc: 0.5534 - val_loss: 1.3796 - val_acc: 0.5590\n",
      "Epoch 46/50\n",
      "412/412 [==============================] - 0s 75us/step - loss: 1.2227 - acc: 0.5534 - val_loss: 1.3721 - val_acc: 0.5590\n",
      "Epoch 47/50\n",
      "412/412 [==============================] - 0s 74us/step - loss: 1.2139 - acc: 0.5510 - val_loss: 1.3649 - val_acc: 0.5590\n",
      "Epoch 48/50\n",
      "412/412 [==============================] - 0s 74us/step - loss: 1.2069 - acc: 0.5558 - val_loss: 1.3583 - val_acc: 0.5652\n",
      "Epoch 49/50\n",
      "412/412 [==============================] - 0s 64us/step - loss: 1.1991 - acc: 0.5437 - val_loss: 1.3524 - val_acc: 0.5590\n",
      "Epoch 50/50\n",
      "412/412 [==============================] - 0s 62us/step - loss: 1.1924 - acc: 0.5413 - val_loss: 1.3486 - val_acc: 0.5528\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a15f9c748>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newmodel_my.fit(X_train_my, Y_train_my,\n",
    "      epochs=50,\n",
    "      batch_size=128,\n",
    "      shuffle=True,\n",
    "      validation_data=(X_test_my, Y_test_my))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161/161 [==============================] - 0s 876us/step\n",
      "[[ 0.00654086  0.02832391  0.11880228  0.58943176  0.23722203  0.0196792 ]\n",
      " [ 0.00795613  0.02567776  0.08379525  0.52868009  0.32828906  0.02560172]\n",
      " [ 0.0062905   0.05577845  0.10373389  0.4854404   0.26302171  0.08573513]\n",
      " [ 0.00915197  0.10798932  0.21151154  0.43594623  0.15565801  0.07974298]\n",
      " [ 0.00605823  0.08195661  0.18711367  0.48864496  0.16372666  0.07249986]\n",
      " [ 0.00698486  0.09984285  0.20339143  0.46290284  0.15222274  0.07465526]\n",
      " [ 0.00875602  0.03355929  0.08341264  0.49444953  0.34814915  0.03167341]\n",
      " [ 0.00479839  0.07531507  0.17848738  0.51167589  0.16178779  0.06793547]\n",
      " [ 0.01665088  0.06502706  0.29672149  0.51027012  0.09332141  0.018009  ]\n",
      " [ 0.01526562  0.06700638  0.09772997  0.42100978  0.29100454  0.10798369]\n",
      " [ 0.00855785  0.03156222  0.12667738  0.56691575  0.24464618  0.02164062]\n",
      " [ 0.00501566  0.04521598  0.10070457  0.50451809  0.27947137  0.06507438]\n",
      " [ 0.00331637  0.07784025  0.06603109  0.6235528   0.13966446  0.089595  ]\n",
      " [ 0.00700896  0.02912793  0.12080291  0.58372742  0.23916343  0.02016926]\n",
      " [ 0.00439512  0.07136451  0.26966137  0.4844909   0.12686661  0.04322146]\n",
      " [ 0.00409775  0.06679213  0.08507368  0.59450155  0.16426316  0.08527179]\n",
      " [ 0.00813565  0.08809519  0.10885439  0.52154243  0.16834913  0.10502322]\n",
      " [ 0.02096335  0.13426667  0.19988592  0.37558451  0.16943654  0.09986299]\n",
      " [ 0.00461537  0.03602733  0.15268433  0.59302264  0.18591963  0.02773062]\n",
      " [ 0.01833675  0.06752981  0.29331002  0.51388454  0.08780681  0.01913206]\n",
      " [ 0.00584347  0.02273169  0.07809597  0.55256808  0.31759283  0.02316799]\n",
      " [ 0.0050817   0.03707644  0.17057654  0.59085739  0.17176487  0.02464311]\n",
      " [ 0.0038122   0.06800561  0.26497677  0.49803242  0.12345433  0.04171872]\n",
      " [ 0.00804675  0.11691336  0.17136674  0.4839032   0.13924088  0.08052913]\n",
      " [ 0.01109922  0.07281137  0.28100979  0.53558654  0.07266963  0.0268235 ]\n",
      " [ 0.00828377  0.03427485  0.16162074  0.54519063  0.23187613  0.01875393]\n",
      " [ 0.00916043  0.03243065  0.12870952  0.56107163  0.24646658  0.02216116]\n",
      " [ 0.00562593  0.07760931  0.11991423  0.55703217  0.16525197  0.07456636]\n",
      " [ 0.00631372  0.0234412   0.07950529  0.54665583  0.32032359  0.02376029]\n",
      " [ 0.0059365   0.02722885  0.11602694  0.5973326   0.23446675  0.01900835]\n",
      " [ 0.00526903  0.07424853  0.07455567  0.5787158   0.1757693   0.09144163]\n",
      " [ 0.00386949  0.08179747  0.09276151  0.59518993  0.14252749  0.08385406]\n",
      " [ 0.00653331  0.10794433  0.1970305   0.48308185  0.13495609  0.07045398]\n",
      " [ 0.01400764  0.05644649  0.25912341  0.56130487  0.09125473  0.01786294]\n",
      " [ 0.00950171  0.05581524  0.1072431   0.45759436  0.29187626  0.07796926]\n",
      " [ 0.00349483  0.0789668   0.06739739  0.61843818  0.14097039  0.09073238]\n",
      " [ 0.01689581  0.06256738  0.28049338  0.53073227  0.09045029  0.01886095]\n",
      " [ 0.00661201  0.09823579  0.20170753  0.46833047  0.15147525  0.07363896]\n",
      " [ 0.01262225  0.09892213  0.14010598  0.44295442  0.19202119  0.11337407]\n",
      " [ 0.00506685  0.03407283  0.11218686  0.60624319  0.21291818  0.029512  ]\n",
      " [ 0.01701353  0.06297411  0.28156683  0.52932972  0.09023105  0.01888483]\n",
      " [ 0.02216795  0.08056315  0.32427394  0.4721531   0.08118189  0.01965998]\n",
      " [ 0.01115224  0.08179439  0.10463335  0.47967443  0.22123702  0.10150854]\n",
      " [ 0.00499     0.02501845  0.07603933  0.53606379  0.33139136  0.02649706]\n",
      " [ 0.01702262  0.06429519  0.30159521  0.51122028  0.08989007  0.01597659]\n",
      " [ 0.00430739  0.03745056  0.10858773  0.60298407  0.20551573  0.04115444]\n",
      " [ 0.00474036  0.03000147  0.06019182  0.52154887  0.32885459  0.05466287]\n",
      " [ 0.01555599  0.11417582  0.24886695  0.42765841  0.11848485  0.07525798]\n",
      " [ 0.00425579  0.03559114  0.10819991  0.55404687  0.25628564  0.04162052]\n",
      " [ 0.00850045  0.0314774   0.1264772   0.56749058  0.24446467  0.02158966]\n",
      " [ 0.00840026  0.07359617  0.09667269  0.5071373   0.21916425  0.09502932]\n",
      " [ 0.01119643  0.08432422  0.12800145  0.46575519  0.2010828   0.1096399 ]\n",
      " [ 0.02464156  0.13387467  0.31034386  0.35373205  0.1080105   0.06939737]\n",
      " [ 0.00512925  0.07515039  0.27460539  0.46965891  0.13057484  0.04488118]\n",
      " [ 0.00842924  0.10403612  0.12153865  0.51913476  0.14996509  0.0968961 ]\n",
      " [ 0.0104408   0.0341573   0.13265523  0.54966998  0.2498872   0.02318948]\n",
      " [ 0.00781333  0.10823863  0.11827178  0.52585274  0.14524889  0.09457459]\n",
      " [ 0.01743986  0.11223993  0.18504803  0.38517368  0.19091094  0.1091876 ]\n",
      " [ 0.01160401  0.06246175  0.08921246  0.45064184  0.28383762  0.10224227]\n",
      " [ 0.00558642  0.08782702  0.08621642  0.5624373   0.1576988   0.10023402]\n",
      " [ 0.01103845  0.09332401  0.39421484  0.39009792  0.07827234  0.03305244]\n",
      " [ 0.00762667  0.1024515   0.20606683  0.45417207  0.1533868   0.07629619]\n",
      " [ 0.00341957  0.05719456  0.10021459  0.56176978  0.18910125  0.08830018]\n",
      " [ 0.00317057  0.02979665  0.08105779  0.5569362   0.28882015  0.04021862]\n",
      " [ 0.00441765  0.07438211  0.08742677  0.57779449  0.16193213  0.09404674]\n",
      " [ 0.01479916  0.11273185  0.24716343  0.43340501  0.1176269   0.07427362]\n",
      " [ 0.00790082  0.03056809  0.12431067  0.57370198  0.24247657  0.02104194]\n",
      " [ 0.00739529  0.05161066  0.11020271  0.47145212  0.28680578  0.07253352]\n",
      " [ 0.00523123  0.03766653  0.09686723  0.53737444  0.27544826  0.0474123 ]\n",
      " [ 0.00983646  0.04922085  0.23979266  0.59695816  0.08833469  0.01585712]\n",
      " [ 0.00740743  0.10948963  0.25184265  0.41485545  0.14868948  0.06771536]\n",
      " [ 0.00730987  0.02962653  0.12202812  0.5802294   0.2403339   0.02047213]\n",
      " [ 0.00131016  0.02658226  0.09839758  0.69260776  0.15128255  0.02981978]\n",
      " [ 0.003487    0.05628614  0.10371087  0.55339462  0.20062219  0.08249919]\n",
      " [ 0.01785811  0.12337214  0.30195218  0.3890318   0.10213754  0.0656482 ]\n",
      " [ 0.00765764  0.03219437  0.04337163  0.50174886  0.38154858  0.03347888]\n",
      " [ 0.00334514  0.05943192  0.20054008  0.5298121   0.15173839  0.05513237]\n",
      " [ 0.00654843  0.02833721  0.11883561  0.58933669  0.2372548   0.01968733]\n",
      " [ 0.00587712  0.06279084  0.12468019  0.48589405  0.21982467  0.10093319]\n",
      " [ 0.00600497  0.08837688  0.08518221  0.55234194  0.16364411  0.10444988]\n",
      " [ 0.00711125  0.02880074  0.08255258  0.50740153  0.34416345  0.02997037]\n",
      " [ 0.03389233  0.11214162  0.37854686  0.38571128  0.06965017  0.02005772]\n",
      " [ 0.00554405  0.06150357  0.12320846  0.49117133  0.21900135  0.09957124]\n",
      " [ 0.01489426  0.11291701  0.24738343  0.43266779  0.11773767  0.07439982]\n",
      " [ 0.01188814  0.05609871  0.18206269  0.57795262  0.14400938  0.02798851]\n",
      " [ 0.00705881  0.02921151  0.12100902  0.58313948  0.23936117  0.02022008]\n",
      " [ 0.00823753  0.03420053  0.16142248  0.54566818  0.23175043  0.01872082]\n",
      " [ 0.00763331  0.08556252  0.28650001  0.43097511  0.14006138  0.04926779]\n",
      " [ 0.00594511  0.08140367  0.18641575  0.4905262   0.16358449  0.07212485]\n",
      " [ 0.00611423  0.03043531  0.15101099  0.57061315  0.22480686  0.01701935]\n",
      " [ 0.00771623  0.04323285  0.14147791  0.5439232   0.22515523  0.03849464]\n",
      " [ 0.01081865  0.03463956  0.13373533  0.54653394  0.2507973   0.02347514]\n",
      " [ 0.00889034  0.04031816  0.04744569  0.47265926  0.40065432  0.03003228]\n",
      " [ 0.00825968  0.08774116  0.28869185  0.42323345  0.14192021  0.05015369]\n",
      " [ 0.01395184  0.03825668  0.14154616  0.52362001  0.25702828  0.02559696]\n",
      " [ 0.03552716  0.14529708  0.31544474  0.31484067  0.11535075  0.07353961]\n",
      " [ 0.019294    0.07080762  0.30145159  0.50305963  0.0860986   0.01928849]\n",
      " [ 0.01808704  0.12768014  0.2295799   0.36730155  0.16386527  0.09348607]\n",
      " [ 0.01144688  0.04678214  0.09263849  0.41706571  0.35000795  0.08205879]\n",
      " [ 0.00427885  0.08063491  0.07551196  0.58773255  0.15522093  0.09662078]\n",
      " [ 0.01547304  0.10270382  0.40344754  0.35966811  0.08326105  0.03544645]\n",
      " [ 0.03977181  0.14802483  0.31489462  0.3038004   0.1183126   0.07519571]\n",
      " [ 0.01074882  0.06969762  0.10868505  0.43946388  0.26053402  0.11087063]\n",
      " [ 0.01332626  0.03758245  0.14012811  0.52781552  0.25594351  0.0252042 ]\n",
      " [ 0.01472803  0.12857267  0.19473769  0.40777338  0.16414236  0.09004589]\n",
      " [ 0.02504629  0.11685143  0.41292799  0.31618243  0.09015319  0.03883866]\n",
      " [ 0.04272272  0.14958796  0.31445643  0.29692203  0.12006732  0.07624354]\n",
      " [ 0.01211719  0.06043545  0.11303391  0.43599853  0.29526395  0.08315091]\n",
      " [ 0.02045211  0.08449902  0.05055438  0.40147677  0.40334406  0.03967365]\n",
      " [ 0.01013718  0.09830899  0.10024241  0.50560462  0.17632708  0.10937977]\n",
      " [ 0.00950884  0.10424006  0.10025319  0.51056325  0.16352756  0.11190713]\n",
      " [ 0.01990649  0.07060083  0.12639937  0.38859162  0.3003439   0.09415784]\n",
      " [ 0.02098021  0.11155993  0.41001523  0.33217618  0.08766679  0.03760164]\n",
      " [ 0.01228422  0.0364095   0.13762039  0.53519362  0.25397435  0.02451797]\n",
      " [ 0.02303392  0.08347873  0.3307108   0.46327156  0.07975877  0.01974629]\n",
      " [ 0.01144392  0.09713386  0.29704189  0.39107287  0.14945725  0.05385022]\n",
      " [ 0.01138867  0.03886774  0.17319526  0.51704323  0.23873636  0.02076865]\n",
      " [ 0.01982237  0.07261068  0.3058252   0.49720311  0.08517125  0.01936741]\n",
      " [ 0.00903612  0.10759696  0.2111363   0.43722439  0.1555059   0.07950038]\n",
      " [ 0.0116482   0.03566     0.13599023  0.53996462  0.25265947  0.02407742]\n",
      " [ 0.0122003   0.03631214  0.13740988  0.53581083  0.25380602  0.02446082]\n",
      " [ 0.01073431  0.05836881  0.11961631  0.43871427  0.29241297  0.08015329]\n",
      " [ 0.0105171   0.03425558  0.1328761   0.54902893  0.25007448  0.02324775]\n",
      " [ 0.01203075  0.05688884  0.10097916  0.45267221  0.28232607  0.09510291]\n",
      " [ 0.01230395  0.11710161  0.21996219  0.40600759  0.15912966  0.08549502]\n",
      " [ 0.00791319  0.09009428  0.19697808  0.46165329  0.16544415  0.07791704]\n",
      " [ 0.00987247  0.03340934  0.13096121  0.55457467  0.24843724  0.0227451 ]\n",
      " [ 0.01425238  0.0909325   0.14908396  0.44074258  0.19117287  0.11381572]\n",
      " [ 0.01231016  0.03643952  0.13768527  0.53500342  0.25402606  0.02453556]\n",
      " [ 0.01522124  0.06467797  0.04961859  0.42955643  0.40501133  0.03591448]\n",
      " [ 0.00733516  0.10128993  0.20488434  0.45804772  0.152876    0.0755669 ]\n",
      " [ 0.00575602  0.08046208  0.1852188   0.49374446  0.16333447  0.0714842 ]\n",
      " [ 0.02005543  0.0712634   0.13561073  0.38211811  0.29743415  0.09351824]\n",
      " [ 0.01163815  0.03342108  0.08591771  0.50500244  0.3358269   0.02819366]\n",
      " [ 0.00777871  0.03037741  0.1238517   0.5750159   0.24204946  0.02092676]\n",
      " [ 0.02175449  0.0452906   0.15536726  0.48153731  0.2664279   0.02962241]\n",
      " [ 0.011364    0.03531608  0.13523491  0.5421688   0.2520414   0.02387477]\n",
      " [ 0.01099953  0.03486645  0.13424025  0.5450657   0.25121868  0.02360931]\n",
      " [ 0.01418029  0.03849772  0.14204891  0.52212816  0.25740781  0.02573708]\n",
      " [ 0.01421288  0.05676055  0.25992227  0.55979031  0.09136505  0.01794899]\n",
      " [ 0.02106935  0.13152385  0.23322667  0.35149547  0.16592902  0.09675569]\n",
      " [ 0.00855699  0.09258363  0.19984758  0.4536393   0.16583572  0.07953682]\n",
      " [ 0.00696005  0.02436229  0.08129942  0.53913623  0.3237187   0.02452328]\n",
      " [ 0.011364    0.03531608  0.13523491  0.5421688   0.2520414   0.02387477]\n",
      " [ 0.0210359   0.08667466  0.05062051  0.39865157  0.40297908  0.04003831]\n",
      " [ 0.04039178  0.15572722  0.05049268  0.32419047  0.38083693  0.04836101]\n",
      " [ 0.01128325  0.04063039  0.0840546   0.47818848  0.35206097  0.0337823 ]\n",
      " [ 0.01288226  0.11851844  0.2212217   0.40132856  0.15964505  0.08640397]\n",
      " [ 0.03071916  0.12303837  0.41532943  0.29776514  0.09291388  0.04023401]\n",
      " [ 0.01990647  0.07289724  0.30651355  0.49627852  0.08502466  0.01937951]\n",
      " [ 0.00905962  0.0482071   0.08790079  0.48825958  0.29452801  0.072045  ]\n",
      " [ 0.01217093  0.03627796  0.13733588  0.53602767  0.25374681  0.02444076]\n",
      " [ 0.00916743  0.03244051  0.12873238  0.56100577  0.2464868   0.02216706]\n",
      " [ 0.00857393  0.0887856   0.28970787  0.4195618   0.14279623  0.05057452]\n",
      " [ 0.01058316  0.07078105  0.18982932  0.43088666  0.23016946  0.06775031]\n",
      " [ 0.01253271  0.03669521  0.13823606  0.53338683  0.25446367  0.02468545]\n",
      " [ 0.00878799  0.09091071  0.31613353  0.38305837  0.15812346  0.04298593]\n",
      " [ 0.00728274  0.1010771   0.20466615  0.45875993  0.15278108  0.07543299]\n",
      " [ 0.00187259  0.02865976  0.08609956  0.68462396  0.15985137  0.03889275]\n",
      " [ 0.01392041  0.04425953  0.18256351  0.49268636  0.24333492  0.02323538]\n",
      " [ 0.01499123  0.13660783  0.25049376  0.37506706  0.14258227  0.08025783]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161/161 [==============================] - 0s 47us/step\n",
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 3 3 3 3 2 2 3 3 3 2 2 3 4 3 3\n",
      " 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 4 4 3 3\n",
      " 2 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "161/161 [==============================] - 0s 101us/step\n",
      "[ 3.  3.  3.  3.  4.  3.  3.  4.  2.  2.  3.  5.  2.  3.  3.  3.  2.  2.\n",
      "  3.  2.  5.  4.  4.  3.  4.  3.  3.  3.  5.  3.  3.  3.  3.  4.  2.  2.\n",
      "  3.  3.  3.  2.  4.  2.  2.  3.  3.  2.  4.  3.  2.  4.  3.  3.  3.  3.\n",
      "  3.  3.  2.  2.  2.  2.  3.  2.  2.  4.  3.  3.  2.  3.  3.  4.  4.  3.\n",
      "  3.  3.  5.  2.  3.  3.  2.  2.  2.  3.  3.  3.  4.  3.  3.  2.  3.  4.\n",
      "  3.  2.  2.  3.  4.  1.  2.  3.  3.  3.  2.  1.  2.  3.  3.  2.  2.  2.\n",
      "  2.  2.  2.  3.  2.  2.  3.  4.  3.  3.  3.  3.  3.  3.  2.  3.  3.  3.\n",
      "  1.  2.  3.  1.  4.  3.  3.  3.  2.  3.  3.  3.  3.  3.  3.  3.  5.  3.\n",
      "  3.  2.  2.  3.  3.  3.  2.  3.  3.  3.  3.  2.  2.  3.  2.  3.  3.]\n",
      "Accuracy:  0.552795031056\n",
      "161/161 [==============================] - 0s 95us/step\n",
      "[[ 0  2  2  0  0]\n",
      " [ 0  4 41  2  0]\n",
      " [ 0  2 85  1  0]\n",
      " [ 0  0 17  0  0]\n",
      " [ 0  0  5  0  0]]\n"
     ]
    }
   ],
   "source": [
    "print(newmodel_my.predict(X_test_my, verbose=1))\n",
    "print(np.argmax(newmodel_my.predict(X_test_my, verbose=1),axis=1))\n",
    "\n",
    "scores_my = newmodel_my.evaluate(X_test_my, Y_test_my, verbose=1) \n",
    "print(y_test_my)\n",
    "print(\"Accuracy: \", scores_my[1])\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cfm = confusion_matrix(np.array(y_test_my),np.array(np.argmax(newmodel_my.predict(X_test_my, verbose=1),axis=1)))\n",
    "print(cfm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
